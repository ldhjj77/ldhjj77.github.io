{"pages":[],"posts":[{"title":"1st","text":"HI~!","link":"/2021/06/23/1st/"},{"title":"A004_inheritance","text":"사용 버전 - Python 3.9.5 64-bit class Unit: def __init__(self, name, hp): self.name = name self.hp = hp class AttackUnit(Unit): def __init__(self, name, hp, damage): Unit.__init__(self, name, hp) self.damage = damage def attack(self, location): print('{0} : {1} 방향으로 적군을 공격합니다. [공격력 {2}]'.format(self.name, location, self.damage)) def damaged(self, damage): print('{0} : {1} 데미지를 입었습니다.'.format(self.name, self.damage)) self.hp -= damage print('{0} : 현재 체력은 {1} 입니다.'.format(self.name, self.hp)) if self.hp &lt;= 0: print(&quot;{0} : 파괴되었습니다. &quot;.format(self.name)) # 공중 유닛 class Flyable: def __init__(self, flying_speed): self.flying_speed = flying_speed def fly(self, name, location): self.name = name self.location = location print(&quot;{0} : {1} 방향으로 날아갑니다. [속도 {2}]&quot;.format(name, location, self.flying_speed)) # 다중 상속 class FlyableAttackUnit(AttackUnit, Flyable): def __init__(self, name, hp, damage, flying_speed): AttackUnit.__init__(self, name, hp, damage) Flyable.__init__(self, flying_speed) valkyrie = FlyableAttackUnit('발키리', 100, 10, 10) valkyrie1 = FlyableAttackUnit('발키리', 110, 15, 15) valkyrie1.fly(valkyrie.name, '3시') # 메소드 값 firebat1 = AttackUnit(&quot;파이어뱃&quot;, 50, 20) firebat1.attack(&quot;7시&quot;) firebat1.attack(&quot;7시&quot;) firebat1.attack(&quot;7시&quot;) # 오버라이딩 - 같은 함수가 존재 할 경우 구분해서 사용하는 방법 # 배틀크루저 : 공중 유닛, 최강 유닛 battle = FlyableAttackUnit(&quot;배틀크루저&quot;, 500, 25, 3) battle.fly(battle.name, &quot;9시&quot;) battle.attack('9시') marine = AttackUnit(&quot;마린&quot;, 40, 10) marine.attack(&quot;3시&quot;)","link":"/2021/06/23/A/A004-inheritance/"},{"title":"A005_starcraft","text":"사용 버전 - Python 3.9.5 64-bit 스타크레프트 게임을 할 수 있는 코드123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171from random import * # 일반 유닛class Unit: def __init__(self, name, hp, speed): self.name = name self.hp = hp self.speed = speed print(&quot;{0} 유닛이 생성되었습니다.&quot;.format(name)) # 출력문 추가 def move(self, location): print(&quot;{0} : {1} 방향으로 이동합니다. [속도 {2}]&quot;\\ .format(self.name, location, self.speed)) def damaged(self, damage): # AttackUnit 에서 Unit 으로 이동 print(&quot;{0} : {1} 데미지를 입었습니다.&quot;.format(self.name, damage)) self.hp -= damage print(&quot;{0} : 현재 체력은 {1} 입니다.&quot;.format(self.name, self.hp)) if self.hp &lt;= 0: print(&quot;{0} : 파괴되었습니다.&quot;.format(self.name))# 공격 유닛class AttackUnit(Unit): def __init__(self, name, hp, speed, damage): Unit.__init__(self, name, hp, speed) self.damage = damage def attack(self, location): print(&quot;{0} : {1} 방향으로 적군을 공격 합니다. [공격력 {2}]&quot; \\ .format(self.name, location, self.damage))# 마린class Marine(AttackUnit): def __init__(self): AttackUnit.__init__(self, &quot;마린&quot;, 40, 1, 5) # 이름, 체력, 이동속도, 공격력 # 스팀팩 : 일정 시간 동안 이동 및 공격 속도를 증가, 체력 10 감소 def stimpack(self): if self.hp &gt; 10: self.hp -= 10 print(&quot;{0} : 스팀팩을 사용합니다. (HP 10 감소)&quot;.format(self.name)) else: print(&quot;{0} : 체력이 부족하여 스팀팩을 사용하지 않습니다&quot;.format(self.name))# 탱크class Tank(AttackUnit): # 시즈모드 : 탱크를 지상에 고정시켜, 더 높은 파워로 공격 가능. 이동 불가. siege_developed = False # 시즈모드 개발여부 (클래스 변수) def __init__(self): AttackUnit.__init__(self, &quot;탱크&quot;, 150, 1, 35) # 이름, 체력, 이동속도, 공격력 self.siege_mode = False # 시즈모드 (해제 상태) # 시즈모드 def set_siege_mode(self): if Tank.siege_developed == False: # 시즈모드가 개발되지 않은 경우 메소드 탈출 return # 현재 시즈모드가 아닐 때 if self.siege_mode == False: print(&quot;{0} : 시즈모드로 전환합니다.&quot;.format(self.name)) self.damage *= 2 # 공격력 2배로 증가 self.siege_mode = True # 시즈 모드 설정 # 현재 시즈모드일 때 else: print(&quot;{0} : 시즈모드를 해제합니다.&quot;.format(self.name)) self.damage /= 2 # 공격력 절반으로 감소 self.siege_mode = False # 시즈 모드 해제# 날 수 있는 기능을 가진 클래스class Flyable: def __init__(self, flying_speed): self.flying_speed = flying_speed def fly(self, name, location): print(&quot;{0} : {1} 방향으로 날아갑니다. [속도 {2}]&quot;\\ .format(name, location, self.flying_speed))# 공중 공격 유닛class FlyableAttackUnit(AttackUnit, Flyable): def __init__(self, name, hp, damage, flying_speed): AttackUnit.__init__(self, name, hp, 0, damage) Flyable.__init__(self, flying_speed) def move(self, location): self.fly(self.name, location)# 레이스class Wraith(FlyableAttackUnit): def __init__(self): FlyableAttackUnit.__init__(self, &quot;레이스&quot;, 80, 20, 5) # 체력, 공격력, 공중 이동 속도 self.cloaked = False # 클로킹 모드 (해제 상태) # 클로킹 모드 def cloaking(self): # 현재 클로킹 모드일 때 if self.cloaked == True: print(&quot;{0} : 클로킹 모드 해제합니다.&quot;.format(self.name)) self.cloaked = False # 현재 클로킹 모드가 아닐 때 else: print(&quot;{0} : 클로킹 모드 설정합니다.&quot;.format(self.name)) self.cloaked = True def game_start(): print(&quot;[알림] 새로운 게임을 시작합니다.&quot;) def game_over(): print(&quot;Player : gg&quot;) print(&quot;[Player] 님이 게임에서 퇴장했습니다.&quot;)# 발키리 : 공중 공격 유닛, 한 번에 14발의 미사일 발사valkyrie1 = FlyableAttackUnit(&quot;발키리&quot;, 200, 6, 5)valkyrie1.fly(valkyrie1.name, &quot;3시&quot;)# 게임 시작game_start()# 마린 3명 생성m1 = Marine()m2 = Marine()m3 = Marine()# 탱크 2대 생성t1 = Tank()t2 = Tank()# 레이스 1대 생성w1 = Wraith()# 유닛 관리 - 리스트 생성attack_units = []attack_units.append(m1)attack_units.append(m2)attack_units.append(m3)attack_units.append(t1)attack_units.append(t2)attack_units.append(w1)print(attack_units)# 전군 이동for unit in attack_units: unit.move(&quot;1시&quot;)# 탱크 시즈모드 개발Tank.siege_developed = Trueprint(&quot;[알림] 탱크 시즈 모드 개발이 완료되었습니다.&quot;)# 공격 모드 준비( 스팀팩 / 시즈모드 / 클로킹)for unit in attack_units: if isinstance(unit, Marine): #unit 이 어떤 클레스인지 확인후 Marine이면 stimpack() 시전 unit.stimpack() elif isinstance(unit, Tank): unit.set_siege_mode() elif isinstance(unit, Wraith): unit.cloaking()# 전군 공격for unit in attack_units: unit.attack(&quot;1시&quot;)# 전군 피해for unit in attack_units: unit.damaged(randint(5, 60)) # 공격은 랜덤으로 5 ~ 20# 게임 종료game_over()","link":"/2021/06/23/A/A005-starcraft/"},{"title":"A006_Module","text":"사용 버전 - Python 3.9.5 64-bit 모듈12345678910111213141516171819202122232425262728import A007_movie # 만들어 놓은(다른 파일에 있는) movie 모듈 호출A007_movie.price(3)A007_movie.price_morning(4)A007_movie.price_soldier(5)import A007_movie as mv # movie 모듈이름을 mv로 변경해서 호출mv.price(3)mv.price_morning(4)mv.price_soldier(5)from A007_movie import * # 모듈이름을 안쓰고 함수만 바로 사용 가능price(3)price_morning(4)price_soldier(5)from A007_movie import price, price_morning # price와 price_morning만 호출price(3)price_morning(4)price_soldier(5) # 에러남from A007_movie import price_soldier as price # 이름 바꿔서 호출price(5) # 솔져 가격이 나옴 패키지1234567891011import travel.thailand # 경로가 다를경우 경로를 적어줘야함# travel폴더에 있는 thailand라는 파일trip_to = travel.thailand.ThailandPackage() # 변수에 클레스를 넣음trip_to.detail()from travel import vietnam # 베트남 호출trip_to = vietnam.VietnamPackage()trip_to.detail()from travel import * # 베트남과 타일렌드 모두 호출 모듈 위치 확인1234import inspect # 위치 확인하는 모듈import randomprint(inspect.getfile(random))#print(inspect.getabsfile(vietnam)) input - 사용자 입력을 받는 함수12language = input(&quot;어떤 언어를 좋아하나요 : &quot;)print('{0}은 아주 좋은 언어입니다.'.format(language))","link":"/2021/06/23/A/A006-Module/"},{"title":"A008_class-quiz","text":"사용 버전 - Python 3.9.5 64-bit 주어진 코드를 활용하여 부동산 프로그램을 작성하시오출력예제총 3곳의 매물이 있습니다동구 아파트 매매 5억 2020년달성군 오피스텔 전체 3억 2021년북구 빌라 월세 500/30 2019 코드123456789101112131415161718192021222324252627 class House: # 매물 초기화 def __init__(self, location, house_type, deal_type, price, year): self.location = location self.house_type = house_type self.deal_type = deal_type self.price = price self.year = year def show_detail(self): print(self.location, self.house_type, self.deal_type, self.price, self.year)houses = []h1 = House(&quot;동구&quot;, &quot;아파트&quot;, &quot;매매&quot;, &quot;5억&quot;, &quot;2020년&quot;)h2 = House(&quot;달성군&quot;, &quot;오피스텔&quot;, &quot;전세&quot;, &quot;3억&quot;, &quot;2021년&quot;)h3 = House(&quot;북구&quot;, &quot;빌라&quot;, &quot;월세&quot;, &quot;500/30&quot;, &quot;2019년&quot;)houses.append(h1)houses.append(h2)houses.append(h3)print(&quot;총 {0}곳의 매물이 있습니다&quot;.format(len(houses)))for house in houses: # houses의 내용을 한개씩house에 집어넣음 house.show_detail() # 순서대로 실행","link":"/2021/06/23/A/A008-class-quiz/"},{"title":"A007_movie","text":"사용 버전 - Python 3.9.5 64-bit A006과 연계된 파일 - 극장 요금 기능이 있는 모듈12345678910111213141516 # 일반 가격def price(people): print('{0}명 가격은 {1}원 입니다.'.format(people, people * 10000))# 조조 할인 가격def price_morning(people): print('{0}명 가격은 {1}원 입니다.'.format(people, people * 5000))# 군인 할인 가격def price_soldier(people): print('{0}명 가격은 {1}원 입니다.'.format(people, people * 2500))","link":"/2021/06/23/A/A007-movie/"},{"title":"A009_module-quiz","text":"사용 버전 - Python 3.9.5 64-bit 프로젝트 내에 나만의 시그니처를 남기는 모듈을 만드시오조건 : 모듈의 파일명은 본인 이름.py( 모듈 사용 예제 )import 본인이름본인이름.sign()123456 from A010_ldh_sign import ldhs1 = ldhldh.sign(s1)","link":"/2021/06/23/A/A009-module-quiz/"},{"title":"A010_ldh_sign","text":"사용 버전 - Python 3.9.5 64-bit A009 파일과 연계된 파일12345 class ldh: def sign(self): print('이 글은 ldh에 의해 작성되었습니다.')","link":"/2021/06/23/A/A010-ldh-sign/"},{"title":"A015-beautiful","text":"사용 버전 - Python 3.9.5 64-bit 웹툰 제목과 링크걸기1234567891011121314151617181920212223242526272829 &lt;!-- # 웹 크롤링 / 웹 스크레핑 --&gt;pip install --upgrade pippip install requestspip install beautifulsoup4pip install lxmlfrom os import nameimport requestsimport refrom bs4 import BeautifulSoup# 나노마신nano = 'https://comic.naver.com/webtoon/list.nhn?titleId=747271'res = requests.get(nano)res.raise_for_status()soup = BeautifulSoup(res.text, 'lxml')# 제목과 링크걸기cartoons = soup.find_all('td', attrs={&quot;class&quot;:&quot;title&quot;})title = cartoons[0].a.get_text()link = cartoons[0].a['href']print(title)print(link)for cartoon in cartoons: title = cartoon.a.get_text() link = 'https://comic.naver.com' + cartoon.a['href'] print(title, link) 평점 구하기12345678910 total_rates = 0cartoons = soup.find_all('div', attrs={'class':'rating_type'})for cartoon in cartoons: rate = cartoon.find('strong').get_text() # 평점 print(rate) total_rates += float(rate)print('전체 점수 : ', round(total_rates, 2))print('평균 점수 : ', round(total_rates / len(cartoons), 2)) URL 요청이 안될경우유저에이전트를 입력해주는 방법123456789 nano = 'https://www.coupang.com/np/search?component=&amp;q=%EB%85%B8%ED%8A%B8%EB%B6%81&amp;channel=user'headers = {'User=Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}# 유저 에이전트 추가res = requests.get(nano, headers=headers)res.raise_for_status()soup = BeautifulSoup(res.text, 'lxml')print(res.text) 이미지 가져오기1234567891011121314151617181920212223 res = requests.get('https://search.daum.net/search?w=tot&amp;q=2020%EB%85%84%EC%98%81%ED%99%94%EC%88%9C%EC%9C%84&amp;DA=MOR&amp;rtmaxcoll=MOR')res.raise_for_status()soup = BeautifulSoup(res.text, 'lxml')images = soup.find_all('img', attrs={'class':'thumb_img'})for idx, image in enumerate(images): # print(image['src']) image_url = image['src'] if image_url.startswith('//'): image_url = 'https:' + image_url image_res = requests.get(image_url) image_res.raise_for_status() print(image_url) with open('movie{0}.jpg'.format(idx+1), 'wb') as f: # 파일생성 f.write(image_res.content) if idx &gt;= 4: break","link":"/2021/06/23/A/A015-beautiful/"},{"title":"A014-re","text":"사용 버전 - Python 3.9.5 64-bit regix expression( 정규식 표현 ) 주민번호로 남녀 구분을 해야하는 등의 경우에 사용주민등록번호 123456-1234567이메일 주소 1234@1234.123차량 번호 12가 3456IP주소 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import re#abcd, book, desk#care, cafe, case, cave, cade, coffe, caaep = re.compile(&quot;ca.e&quot;) # ca.e 에서 . 은 정규표현식# . : 하나의 문자를 의미 care, cafe, case, cave, cade, caaep = re.compile(&quot;^de&quot;) # ^ : 시작하는 문자열 (de로 시작하는 문자 추출) deskp = re.compile(&quot;se$&quot;)# $ : 끝나는 문자열(se로 끝나는 문자 추출) casedef print_match(m): if m: print(m.group()) # group은 묶어주는 함수 print(m.string()) # 입력받은 문자열 print(m.start()) # 일치하는 문자열의 시작 인덱스값을 가져옴 print(m.end()) # 일치하는 문자열의 끝 인덱스값을 가져옴 print(m.span()) # 일치하는 문자열의 시작과 끝의 인덱스값을 가져옴 else: print(&quot;매칭되지 않음&quot;)lst = p.findall('good care cafe') # findall = 일치하는 모든것들을 리스트로 저장# lst = [care, cafe] 가 나옴lst = p.search('careless') # 문자열중에 일치하는것이 있는지 확인 carem = p.search('careless')# re.compile = 정규식(원하는 형태로)# .match - 문자열의 처음부터 일치하는가# .search - 문자열 중에 일치하는게 있는가# .findall - 일치하는 모든 문자열을 리스트로 반환# http Method# - request - GET / POSTimport requests # 터미널창에 pip install requests 입력해서 설치 필요from requests import status_codesurl = 'http://coupang.com'headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}res = requests.get(url, headers=headers)res.raise_for_status()# res = requests.get('http://naver.com')# res.raise_for_status()# print('응답 코드 : ', res.status_code)if res.status_code -- requests.codes.ok: print('정상입니다.')else: print('문제가 생겼습니다.[에러코드 ', res.status_code, ']')print(len(res.text))print(res.text)with open('mycoupang.html', 'w', encoding='utf8') as op: # 결과값을 파일로 생성 op.write(res.text)","link":"/2021/06/23/A/A014-re/"},{"title":"A011-exception","text":"사용 버전 - Python 3.9.5 64-bit 예외처리123456789101112131415161718192021222324252627class Bignumbererror(Exception): def __inint__(self, msg): self.msg = msg def __str__(self): return self.msgtry: # 예외처리 시작 print('한자리 숫자 나누기 전용 계산기') num1 = int(input('첫번째 숫자를 입력하세요')) num2 = int(input('첫번째 숫자를 입력하세요')) if num1 &gt;= 10 or num2 &gt;=10: # 예외가 될 조건 raise Bignumbererror('입력값 : {0}, {1}'.format(num1, num2)) # raise로 예외 지정 및 예외시 출력값 print('{0} / {1} = {2}'.format(num1, num2, float(num1 / num2))) # 정상일 경우 출력값 except ValueError: # 숫자외의 값이 입력될 경우 print('에러! 잘못된 값을 입력했습니다. 한자리 숫자만 입력하세요.')except Bignumbererror: # 사용자 정의 예외처리 1자리 숫자가 아닐경우 print('값이 정확하지 않음.')except ZeroDivisionError as err: # 0으로 나누기가 실행될 경우 print('0으로 나누기 금지! 0을 제외한 한자리 숫자를 입력하세요.')except Exception: # 그 외에 예외상황 발생시 print('알수 없는 에러가 발생하였습니다')finally: # 예외처리 끝 print(&quot;끝&quot;) 예제동네에 맛있는 치킨집이 있다.대기 손님을 위해 자동 주문 시스템을 제작하였다.시스템 코드를 확인하고 적절하게 예외처리 구문을 넣어보자. 조건 11보다 작거나 숫자가 아닌 입력값이 들어올 때는 ValueError 로 처리메세지 “잘못된 값을 입력하였습니다.”조건 2총 치킨량이 10마리로 한정치킨 소진 시 사용자 정의 에러[Soldouterror]를 발생시키고 프로그램 종료메시지 “재고가 소진되어 더 이상 주문을 받지 않습니다.” –&gt; 123456789101112131415161718192021222324252627282930class SoldOutError(Exception): passchicken = 10 # 남은 치킨의 수waiting = 1 # 매장 안에는 만석, 대기번호는 1번부터 시작while(True): try: print(&quot;[남은 치킨] : {0}&quot;.format(chicken)) order = int(input(&quot;치킨 몇마리 주문하시겠습니까? : &quot;)) if order &gt; chicken: # 남은 치킨보다 주문량이 많을 경우 print(&quot;재료가 부족합니다.&quot;) elif order &lt;=0: raise ValueError else: print(&quot;[대기번호 {0}] {1} 마리 주문이 완료되었습니다.&quot;.format(waiting, order)) waiting += 1 # 정상처리시 대기번호증가 chicken -= order # 정상처리시 치킨 감소 if chicken == 0: raise SoldOutError except ValueError: print(&quot;잘못된 값을 입력하였습니다.&quot;) except SoldOutError: print(&quot;재고가 소진되어 더 이상 주문을 받지 않습니다.&quot;) print(&quot;프로그램을 종료합니다.&quot;) break","link":"/2021/06/23/A/A011-exception/"},{"title":"A016-selenium","text":"사용 버전 - Python 3.9.5 64-bit 우선 본인이 사용중인 크롬의 버전을 확인한다크롬창을 열고 주소창에 chrome://version/ 를 입력하면 확인가능 다음으로 크롬웹드라이브를 아래 링크에서 크롬버전에 맞는 버전으로 받는다크롬웹드라이브 받은 파일을 파이썬 작업폴더에 넣고 진행 12345678910111213141516171819202122232425262728293031323334353637383940414243444546pip install selenium# 셀레니움 설치from selenium.webdriver.common.keys import Keysfrom selenium import webdriver# 크롬웹드라이브 다운받아서 작업폴더에 넣고 불러오기browser = webdriver.Chrome('./chromedriver.exe')# 크롬의 웹드라이브 위치 지정(chromedriver.exe 파일이 있는 위치)# 제어된 크롬창이 뜸browser.get('http://naver.com') # 제어창에 네이버 창이 뜸elem = browser.find_element_by_class_name('link_login') # 로그인 페이지의 정보를 elem 변수에 저장elem.click() # 로그인 버튼을 클릭함browser.back # 페이지가 뒤로 감 browser.forword# 페이지가 앞으로 감 elem = browser.find_element_by_id('query') # elem변수에 검색창 정보를 저장from selenium.webdriver.common.keys import Keys # 키 모듈 불러오기elem.send_keys('영화순위')# 검색창에 영화순위 라는 글자가 입력됨elem.send_keys(Keys.ENTER) # 검색시작됨# browser.close() # browser = webdriver.Chrome() 띄워놓은 창 닫기# browser.quit() 브라우저 종료","link":"/2021/06/28/A/A016-selenium/"},{"title":"A018-google-movie","text":"사용 버전 - Python 3.9.5 64-bit 구글 무비 정보 가져오기123456789101112131415161718import requestsfrom bs4 import BeautifulSoupurl = 'https://play.google.com/store/movies/top'res = requests.get(url)res.raise_for_status()soup = BeautifulSoup(res.text, 'lxml')movies = soup.find_all('div', attrs={'class': 'WsMG1c nnK0zc'})print(len(movies))with open('movie.html', 'w', encoding='utf8') as f: # f.write(res.text) # 일반적인 방식 f.write(soup.prettify()) # html 파일을 다듬어서 출력해줌","link":"/2021/06/29/A/A018_google-movie/"},{"title":"A021-translator","text":"사용 버전 - Python 3.9.5 64-bit 구글번역기 알파버전 설치 / 기본버전은 사용불가1pip install googletrans==4.0.0-rc1 번역기 코드1234567891011121314from googletrans import Translator text1 = input('번역할 영어를 입력 : ')text2 = input('번역할 한글을 입력 : ') translator = Translator() trans1 = translator.translate(text1, src='en', dest='ko')trans2 = translator.translate(text2, src='ko', dest='en') print(&quot;English to Korean: &quot;, trans1.text)print(&quot;Korean to English: &quot;, trans2.text) 번역할 영어를 입력 : egg번역할 한글을 입력 : 알English to Korean: 계란Korean to English: egg 사용 가능한 언어들‘af’: ‘afrikaans’, ‘am’: ‘amharic’, ‘ar’: ‘arabic’, ‘az’: ‘azerbaijani’, ‘be’: ‘belarusian’,‘bg’: ‘bulgarian’, ‘bn’: ‘bengali’, ‘bs’: ‘bosnian’, ‘ca’: ‘catalan’, ‘ceb’: ‘cebuano’,‘co’: ‘corsican’, ‘cs’: ‘czech’, ‘cy’: ‘welsh’, ‘da’: ‘danish’, ‘de’: ‘german’,‘el’: ‘greek’, ‘en’: ‘english’, ‘eo’: ‘esperanto’, ‘es’: ‘spanish’, ‘et’: ‘estonian’,‘eu’: ‘basque’, ‘fa’: ‘persian’, ‘fi’: ‘finnish’, ‘fil’: ‘Filipino’, ‘fr’: ‘french’,‘fy’: ‘frisian’, ‘ga’: ‘irish’, ‘gd’: ‘scots gaelic’, ‘gl’: ‘galician’, ‘gu’: ‘gujarati’,‘ha’: ‘hausa’, ‘haw’: ‘hawaiian’, ‘he’: ‘Hebrew’, ‘hi’: ‘hindi’, ‘hmn’: ‘hmong’,‘hr’: ‘croatian’, ‘ht’: ‘haitian creole’, ‘hu’: ‘hungarian’, ‘hy’: ‘armenian’,‘id’: ‘indonesian’, ‘ig’: ‘igbo’, ‘is’: ‘icelandic’, ‘it’: ‘italian’, ‘iw’: ‘hebrew’,‘ja’: ‘japanese’, ‘jw’: ‘javanese’, ‘ka’: ‘georgian’, ‘kk’: ‘kazakh’, ‘km’: ‘khmer’,‘kn’: ‘kannada’, ‘ko’: ‘korean’, ‘ku’: ‘kurdish (kurmanji)’, ‘ky’: ‘kyrgyz’, ‘la’: ‘latin’,‘lb’: ‘luxembourgish’, ‘lo’: ‘lao’, ‘lt’: ‘lithuanian’, ‘lv’: ‘latvian’, ‘mg’: ‘malagasy’,‘mi’: ‘maori’, ‘mk’: ‘macedonian’, ‘ml’: ‘malayalam’, ‘mn’: ‘mongolian’, ‘mr’: ‘marathi’,‘ms’: ‘malay’, ‘mt’: ‘maltese’, ‘my’: ‘myanmar (burmese)’, ‘ne’: ‘nepali’, ‘nl’: ‘dutch’,‘no’: ‘norwegian’, ‘ny’: ‘chichewa’, ‘pa’: ‘punjabi’, ‘pl’: ‘polish’, ‘ps’: ‘pashto’,‘pt’: ‘portuguese’, ‘ro’: ‘romanian’, ‘ru’: ‘russian’, ‘sd’: ‘sindhi’, ‘si’: ‘sinhala’,‘sk’: ‘slovak’, ‘sl’: ‘slovenian’, ‘sm’: ‘samoan’, ‘sn’: ‘shona’, ‘so’: ‘somali’,‘sq’: ‘albanian’, ‘sr’: ‘serbian’, ‘st’: ‘sesotho’, ‘su’: ‘sundanese’, ‘sv’: ‘swedish’,‘sw’: ‘swahili’, ‘ta’: ‘tamil’, ‘te’: ‘telugu’, ‘tg’: ‘tajik’, ‘th’: ‘thai’,‘tl’: ‘filipino’, ‘tr’: ‘turkish’, ‘uk’: ‘ukrainian’, ‘ur’: ‘urdu’, ‘uz’: ‘uzbek’,‘vi’: ‘vietnamese’, ‘xh’: ‘xhosa’, ‘yi’: ‘yiddish’, ‘yo’: ‘yoruba’,‘zh-cn’: ‘chinese (simplified)’, ‘zh-tw’: ‘chinese (traditional)’, ‘zu’: ‘zulu’","link":"/2021/06/30/A/A021_translator/"},{"title":"A017-naverair","text":"사용 버전 - Python 3.9.5 64-bit 네이버 에어 항공권 검색12345678910111213141516171819202122232425262728from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECbrowser = webdriver.Chrome()browser.maximize_window() # 제어창 크기를 최대로url = 'https://flight.naver.com/flights/'browser.get(url) # 제어창의 주소 입력# 가는날 선택 버튼이 클릭됨 element 에 s가 없는것 사용browser.find_element_by_link_text('가는날 선택').click() # 실제 가는날 선택 elements 에 s가 있는것 사용 [0]은 이번달 [1]은 다음달 등browser.find_elements_by_link_text('5')[0].click()# 오는날 선택browser.find_elements_by_link_text('7')[0].click()# 목적지 선택browser.find_element_by_xpath('//*[@id=&quot;recommendationList&quot;]/ul/li[1]').click()# 검색 시작browser.find_element_by_link_text('항공권 검색').click()","link":"/2021/06/29/A/A017-naverair/"},{"title":"A020_project","text":"사용 버전 - Python 3.9.5 64-bit 프로젝트 : 웹 스크레핑을 통해 나만의 비서를 만들어 보자.[조 건]1. 네이버에서 오늘 대구의 날씨를 가져온다.2. 네이버에서 헤드라인 뉴스 3건을 가져온다.3. IT 뉴스 3건을 가져온다.4. 오늘의 영어 회화 지문을 가져온다.(해커스 어학원)[출력 예시][ 오늘의 날씨]맑음, 어제보다 00도 높아요현재 00도 (최저 00도 / 최고 00도)오전 강수확률 00% / 오후 강수확률 00%미세먼지( ) 좋음[헤드라인 뉴스]1. 뉴스1제목( 링크 : https://…. )2. 뉴스2제목( 링크 : https://…. )3. 뉴스3제목( 링크 : https://…. )[IT 뉴스]1. 뉴스1제목( 링크 : https://…. )2. 뉴스2제목( 링크 : https://…. )3. 뉴스3제목( 링크 : https://…. )[오늘의 영어 회화](영어 지문)kim : How are you?lee : fine!(한글 지문)kim : 어때?lee : 좋아!파이썬 기초 : w3school, python.org, 위키독스(점프투파이썬)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import requestsimport refrom bs4 import BeautifulSoup# 반복되는 부분을 함수로 생성def create_soup(url, headers): # 필요한 매개변수를 넣어줘야함 res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') return soup# 날씨 wed / 온도 tem / 강수확률 rain / 미세먼지 fdust / 초미세먼지 ufdustdef scrape_weather(): print('[오늘의 날씨]') url = 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%82%A0%EC%94%A8&amp;tqi=h71ErlprvN8ss6l%2FnnCssssstk4-523554' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'} # res = requests.get(url, headers=headers) # res.raise_for_status() # soup = BeautifulSoup(res.text, 'lxml') soup = create_soup(url, headers) # 맑음, 어제보다 0도 높아요 cast = soup.find('p', attrs={'class':'cast_txt'}).get_text() curr_temp = soup.find('p', attrs={'class':'info_temperature'}).get_text().replace('도씨', '') # 현재온도 # 가져온 텍스트중에 '도씨' 를 공백처리 min_temp = soup.find('span', attrs={'class':'min'}).get_text() # 최저온도 max_temp = soup.find('span', attrs={'class':'max'}).get_text() # 최고온도 # 오전 오후 강수 확률 morning_rain_rate = soup.find('span', attrs={'class': 'point_time morning'}).get_text().strip() # 오전 강수확률 / 공백제거 afternoon_rain_rate = soup.find('span', attrs={'class': 'point_time afternoon'}).get_text().strip() # 오후 강수확률 / 공백제거 # 미세 먼지 dust = soup.find('dl', attrs={'class':'indicator'}) pm = dust.find('dd', attrs={'class':'lv1'}).get_text() # print(dust) print(cast) print('현재 {0} (최저 {1} / 최고 {2})'.format(curr_temp, min_temp, max_temp)) print('오전 {0} / 오후 {1}'.format(morning_rain_rate, afternoon_rain_rate)) print() print('미세먼지 {0}'.format(pm)) print() print()# 헤드라인 뉴스def scrape_headline_news(): print('[헤드라인 뉴스]') url = 'https://news.naver.com' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'} # res = requests.get(url, headers=headers) # res.raise_for_status() # soup = BeautifulSoup(res.text, 'lxml') soup = create_soup(url, headers) news_list = soup.find('ul', attrs={'class': 'hdline_article_list'}).find_all('li', limit=3) # ul 테그의 hdline_article_list 클레스에서 가져온 것들중에 li테그에 속한것을 3개만 가져옴 for index, news in enumerate(news_list): # 인덱스값 매기기 index는 번호를 매겨줌 title = news.find('a').get_text().strip() link = news.find('a')['href'] print('{0}. {1}'.format(index + 1, title)) print(' ( 링크 : {0} )'.format(url + link)) print() # IT뉴스def scrape_it_news(): print('[IT 뉴스]') url = 'https://news.naver.com/main/list.nhn?mode=LS2D&amp;mid=shm&amp;sid1=105&amp;sid2=230' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') news_list = soup.find('ul', attrs={'class': 'type06_headline'}).find_all('li', limit=3) for index, news in enumerate(news_list): a_idx = 0 # 임의의 변수 설정 img = news.find('img') # img 라는 테그를 찾아서 img변수에 넣음 if img: # img 테그가 나온다면 a_idx = 1 # a 태그가 있으면 1번인 a 태그의 정보를 가져옴( 두번째 a 태그를 가져옴) title = news.find_all('a')[a_idx].get_text().strip() # 인덱스 값이 여러개이기 때문에 all로 검색해야함 link = news.find_all('a')[a_idx]['href'] print('{0}. {1}'.format(index + 1, title)) print(' ( 링크 : {0} )'.format(link)) print() def scrape_english(): print('[오늘의 영어회화]') url = 'https://www.hackers.co.kr/?c=s_eng/eng_contents/I_others_english&amp;keywd=haceng_submain_lnb_eng_I_others_english&amp;logger_kw=haceng_submain_lnb_eng_I_others_english' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') print('[영어 지문]') sentences = soup.find_all('div', attrs={'id':re.compile('^conv_kor_t')}) # conv_kor_t2, conv_kor_t3 이런식으로 아이디의 이름이 다를경우 # 정규 표현식인 re를 사용해 테그를 검색해서 자료 검출 for sentence in sentences[len(sentences)//2:]: # len(sentences)//2: = 4 ~ 끝까지(7)이 됨 //는 앞에 정수만 가져오는거 print(sentence.get_text().strip()) print() print(&quot;[한글 지문]&quot;) for sentence in sentences[:len(sentences)//2]: # :len(sentences)//2 = 처음부터 3까지 print(sentence.get_text().strip()) print() if __name__ == '__main__': # scrape_weather()라는 함수가 같은파일(A020_project.py)안에 있다면 실행하도록 하는 코드 scrape_weather() # 오늘 날씨 정보 가져오기 scrape_headline_news() # 헤드라인 뉴스 scrape_it_news() # 아이티 뉴스 scrape_english() # 영어 회화 추가 내용123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121# 다음 뉴스에서 3개 뉴스의 제목과 링크 가져오기# 1. 제목...# 링크...def scrape_daum_news(): print('[다음 헤드라인 뉴스]') url = 'https://news.daum.net' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') news_list = soup.find('ul', attrs={'class': 'list_headline'}).find_all('li', limit=3) # ul 테그의 hdline_article_list 클레스에서 가져온 것들중에 li테그에 속한것을 3개만 가져옴 for index, news in enumerate(news_list): # 인덱스값 매기기 index는 번호를 매겨줌 title = news.find('a').get_text().strip() link = news.find('a')['href'] print('{0}. {1}'.format(index + 1, title)) print(' ( 링크 : {0} )'.format(link)) print()# # 다음 경제 뉴스에서 2개 뉴스의 제목과 링크 가져오기# 1. 제목...# 링크...def scrape_daum_economic_news(): print('[다음 경제 뉴스]') url = 'https://news.daum.net/economic#1' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') news_list = soup.find('ul', attrs={'class': 'list_mainnews'}).find_all('li', limit=2) # ul 테그의 hdline_article_list 클레스에서 가져온 것들중에 li테그에 속한것을 3개만 가져옴 for index, news in enumerate(news_list): a_idx = 0 # 임의의 변수 설정 img = news.find('img') # img 라는 테그를 찾아서 img변수에 넣음 if img: # img 테그가 나온다면 a_idx = 1 # a 태그가 있으면 1번인 a 태그의 정보를 가져옴( 두번째 a 태그를 가져옴) title = news.find_all('a')[a_idx].get_text().strip() # 인덱스 값이 여러개이기 때문에 all로 검색해야함 link = news.find_all('a')[a_idx]['href'] print('{0}. {1}'.format(index + 1, title)) print(' ( 링크 : {0} )'.format(link)) print() print()# # 다음 게임에서 추천 게임 4개의 제목과 이미지 가져오기# 1. 제목...# 이미지def scrape_daum_game(): print('[다음 게임]') url = 'http://game.daum.net' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') game_list_pc = soup.find('div', attrs={'class': 'area_pc'}).find_all('li', limit=3) # ul 테그의 hdline_article_list 클레스에서 가져온 것들중에 li테그에 속한것을 3개만 가져옴 for index, pc_game in enumerate(game_list_pc): # 인덱스값 매기기 index는 번호를 매겨줌 title = pc_game.find('a').get_text().strip() image_url = pc_game.find('img')['src'] print('{0}. {1}'.format(index + 1, title)) print(' ( 이미지 : {0} )'.format(image_url)) print() print() image_res = requests.get(image_url) image_res.raise_for_status() with open('./A/A020_pcgame{0}.jpg'.format(index+1), 'wb') as f: # 피씨게임 이미지 파일생성 f.write(image_res.content) game_list_m = soup.find('div', attrs={'class': 'area_m'}).find_all('li', limit=1) # ul 테그의 hdline_article_list 클레스에서 가져온 것들중에 li테그에 속한것을 3개만 가져옴 for index, m_game in enumerate(game_list_m): # 인덱스값 매기기 index는 번호를 매겨줌 title = m_game.find('a').get_text().strip() image_url = m_game.find('img')['src'] print('{0}. {1}'.format(index + 1, title)) print(' ( 이미지 : {0} )'.format(image_url)) print() print() image_res = requests.get(image_url) image_res.raise_for_status() with open('./A/A020_mgame{0}.jpg'.format(index+1), 'wb') as f: # 모바일 게임 이미지 파일생성 f.write(image_res.content) # 해커스에서 오늘의 한 줄 명언 가져오기# (영어명언)# (한글명언)def scrape_wisesay(): print('[오늘의 명언]') url = 'https://www.hackers.co.kr/?c=s_eng/eng_contents/B_others_wisesay&amp;keywd=haceng_submain_lnb_eng_B_others_wisesay&amp;logger_kw=haceng_submain_lnb_eng_B_others_wisesay' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') print('[영어 명언]') sentence_eng = soup.find('div', attrs={'class':'text_en'}).get_text() print(sentence_eng) print(&quot;[한글 명언]&quot;) sentence_ko = soup.find('div', attrs={'class':'text_ko'}).get_text() print(sentence_ko)if __name__ == '__main__': # scrape_weather()라는 함수가 같은파일(A020_project.py)안에 있다면 실행하도록 하는 코드 scrape_daum_news() # 다음 헤드라인 뉴스 scrape_daum_economic_news() # 다음 경제 뉴스 scrape_daum_game() # 다음 게임 scrape_wisesay() # 오늘의 명언 영어회화","link":"/2021/06/29/A/A020_project/"},{"title":"A023-data-analyze","text":"사용 버전 - Python 3.9.5 64-bit 123456789101112131415161718192021222324252627282930313233343536373839404142434445import pandas as pdfrom pandas.core.frame import DataFrame# pandas 자료구조 DataFrame(2차원배열) / Serise(1차원배열)s1 = pd.Series([3, 5, 8, 3, 2])print(s1)print(s1.values)print(s1.index)print(s1.dtypes)s2 = pd.Series([3, 5, 8, 3, 2], index=['b', 'p', 'a', 'r', 's']) # 인덱스와 값을 따로 입력print(s2)print(s2['b'])print(s2[0])print(s2.b)print(s2[:2])print(s2.reindex(['p', 'a', 's', 'r', 'b'])) # 기존 인덱스값을 넣지 않으면 값이 사라짐(순서만 조정하는경우 사용)s3 = pd.Series({'math':95, 'lang':90, 'code':95}) # 인덱스와 값을 같이 입력print(s3)s3.name = 'Scores's3.index.name = 'Subject'print(s3)x = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])print(x)print(x.values)data = {'subject': ['math', 'comp', 'phys', 'music'], 'score': [90, 80, 90, 100], 'student': [95, 85, 75, 90]}print(pd.DataFrame(data))y = pd.DataFrame(data)print(y.dtypes)print(len(y)) # 인덱스의 길이가 나옴print(y.shape) # 행과 열의 길이print(y.shape[0]) # 열의 길이print(y.shape[1]) # 행의 길이","link":"/2021/06/30/A/A023_data-analyze/"},{"title":"A027_news","text":"사용 버전 - Python 3.9.5 64-bit 텍스트 파일 생성후 뉴스 기록123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131from bs4 import BeautifulSoupimport urllib.requestimport timedef jtbc_news(): url = &quot;https://news.jtbc.joins.com/default.aspx&quot; res = urllib.request.urlopen(url) source = res.read() res.close() soup = BeautifulSoup(source, 'html.parser') soup = soup.find_all('div', attrs={'class':'feed_img'}, limit=10) # print(soup) now = time.strftime('%Y-%m-%d %H-%M', time.localtime(time.time())) # print(now) with open('./A/{0}_news.txt'.format(now), 'w') as f: # 피씨게임 이미지 파일생성 f.write('오늘의 주요 뉴스\\n\\n') for index, news in enumerate(soup): data = news.find('a').get_text() link = news.find('a')['href'] print('jtbc 뉴스 {0} : {1} '.format(index+1, data)) print(' 링크 : {0} '.format(link)) with open('./A/{0}_news.txt'.format(now), 'a') as f: # 피씨게임 이미지 파일생성 f.write('jtbc_news {0} : {1} \\n\\n'.format(index+1, data)) with open('./A/{0}_news.txt'.format(now), 'a') as f: # 피씨게임 이미지 파일생성 f.write('\\n\\n\\n') print() print()def psy_cardnews(): url = &quot;http://www.psychiatricnews.net/news/articleList.html?sc_section_code=S1N23&amp;view_type=sm&quot; res = urllib.request.urlopen(url) source = res.read() res.close() soup = BeautifulSoup(source, 'html.parser') soup = soup.find_all('h4', attrs={'class':'titles'}, limit=10) now = time.strftime('%Y-%m-%d %H-%M', time.localtime(time.time())) # print(soup) for index, news in enumerate(soup): data = news.find('a').get_text() link = news.find('a')['href'] print('카드뉴스 {0} : {1} '.format(index+1, data)) print(' 링크 : {0} '.format('http://www.psychiatricnews.net'+link)) with open('./A/{0}_news.txt'.format(now), 'a') as f: # 피씨게임 이미지 파일생성 f.write('psy_cardnews {0} : {1} \\n\\n'.format(index+1, data)) with open('./A/{0}_news.txt'.format(now), 'a') as f: # 피씨게임 이미지 파일생성 f.write('\\n\\n\\n') print() print()def sciencetimes(): url = &quot;https://www.sciencetimes.co.kr/category/sci-tech/&quot; res = urllib.request.urlopen(url) source = res.read() res.close() soup = BeautifulSoup(source, 'html.parser') soup = soup.find_all('div', attrs={'class':'board_cont'}, limit=9) now = time.strftime('%Y-%m-%d %H-%M', time.localtime(time.time())) # print(soup) for index, news in enumerate(soup): data = news.find('strong').get_text() link = news.find('a')['href'] print('과학 기술 {0} : {1} '.format(index+1, data)) print(' 링크 : {0} '.format(link)) with open('./A/{0}_news.txt'.format(now), 'a') as f: # 피씨게임 이미지 파일생성 f.write('sciencetimes {0} : {1} \\n\\n'.format(index+1, data)) with open('./A/{0}_news.txt'.format(now), 'a') as f: # 피씨게임 이미지 파일생성 f.write('\\n\\n\\n') print() print() def ilovepc(): url = &quot;http://www.ilovepc.co.kr/news/articleList.html?sc_section_code=S1N1&amp;view_type=sm&quot; res = urllib.request.urlopen(url) source = res.read() res.close() soup = BeautifulSoup(source, 'html.parser') soup = soup.find_all('div', attrs={'class':'list-titles'}, limit=8) now = time.strftime('%Y-%m-%d %H-%M', time.localtime(time.time())) # print(soup) for index, news in enumerate(soup): data = news.find('strong').get_text() link = news.find('a')['href'] print('피시 사랑 {0} : {1} '.format(index+1, data)) print(' 링크 : {0} '.format('http://www.ilovepc.co.kr'+link)) with open('./A/{0}_news.txt'.format(now), 'a') as f: # 피씨게임 이미지 파일생성 f.write('ilovepc {0} : {1} \\n\\n'.format(index+1, data)) with open('./A/{0}_news.txt'.format(now), 'a') as f: # 피씨게임 이미지 파일생성 f.write('\\n\\n\\n') print() print()def kormedi(): url = &quot;http://kormedi.com/healthnews/&quot; res = urllib.request.urlopen(url) source = res.read() res.close() soup = BeautifulSoup(source, 'html.parser') soup = soup.find_all('h2', attrs={'class':'title'}, limit=16) now = time.strftime('%Y-%m-%d %H-%M', time.localtime(time.time())) # print(soup) for index, news in enumerate(soup): data = news.find('a').get_text().strip() link = news.find('a')['href'] print('코메디 {0} : {1} '.format(index+1, data)) print(' 링크 : {0} '.format(link)) with open('./A/{0}_news.txt'.format(now), 'a') as f: # 피씨게임 이미지 파일생성 f.write('kormedi {0} : {1} \\n\\n'.format(index+1, data)) with open('./A/{0}_news.txt'.format(now), 'a') as f: # 피씨게임 이미지 파일생성 f.write('\\n\\n\\n') print() print()if __name__ == '__main__': # scrape_weather()라는 함수가 같은파일(A020_project.py)안에 있다면 실행하게 jtbc_news() # jtbc 뉴스 psy_cardnews() # 정신의학 카드뉴스 sciencetimes() # 과학 뉴스 ilovepc() # 피시사랑 뉴스 kormedi() # 코메디 뉴스","link":"/2021/07/01/A/A027_news/"},{"title":"A025-Stock","text":"사용 버전 - Python 3.9.5 64-bit 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102from bs4 import BeautifulSoupimport urllib.request as reqimport urllib.request###################### 환율def Exchange_Rate(): url = 'https://finance.naver.com/marketindex/' res = req.urlopen(url) soup = BeautifulSoup(res,'html.parser', from_encoding='euc-kr') name_nation = soup.select('h3.h_lst &gt; span.blind') name_price = soup.select('span.value') i = 0 for c_list in soup: try: print(i+1, name_nation[i].text, name_price[i].text) i = i + 1 except IndexError: pass##################### 코스피 지수def kospi(): url = &quot;https://finance.naver.com/sise/sise_index.nhn?code=KOSPI&quot; fp = urllib.request.urlopen(url) source = fp.read() fp.close() soup = BeautifulSoup(source, 'html.parser') soup = soup.find_all('div', attrs={'id':'quotient'}) kos = soup[0].get_text().strip() print() print('코스피 지수 : ' + kos)##################### 코스닥 지수def kosdaq(): url = &quot;https://finance.naver.com/sise/sise_index.nhn?code=KOSDAQ&quot; fp = urllib.request.urlopen(url) source = fp.read() fp.close() soup = BeautifulSoup(source, 'html.parser') soup = soup.find_all('div', attrs={'id':'quotient'}) kos = soup[0].get_text().strip() print() print('코스닥 지수 : ' + kos)#################### 다우지수 def dau(): url = &quot;https://finance.naver.com/world/sise.nhn?symbol=DJI@DJI&quot; fp = req.urlopen(url) source = fp.read() fp.close() soup = BeautifulSoup(source, 'html.parser') soup = soup.findAll(&quot;em&quot;) # print(soup) dau = soup[2].get_text().strip() dau1 = soup[3].get_text().strip() dau2 = soup[4].get_text().replace(&quot;\\n&quot;, &quot;&quot;).strip() print() print('다우지수 : '+ dau ) print('전일대비 : '+ dau1 + dau2) #################### 나스닥 지수 def nasdaq(): url = &quot;https://finance.naver.com/world/sise.nhn?symbol=NAS@IXIC&quot; fp = req.urlopen(url) source = fp.read() fp.close() soup = BeautifulSoup(source, 'html.parser') soup = soup.findAll(&quot;em&quot;) # print(soup) dau = soup[2].get_text().strip() dau1 = soup[3].get_text().strip() dau2 = soup[4].get_text().replace(&quot;\\n&quot;, &quot;&quot;).strip() print() print('나스닥지수 : '+ dau ) print('전일대비 : '+ dau1 + dau2) if __name__ == '__main__': # Exchange_Rate()라는 함수가 같은파일(A025_Stock.py)안에 있다면 실행하게 Exchange_Rate() kospi() kosdaq() dau() nasdaq()","link":"/2021/06/30/A/A025_Stock/"},{"title":"A029-assignment","text":"사용 버전 - Python 3.9.5 64-bit 네이버 IT/과학뉴스 3개의 제목과 링크 가져오기네이버 웹툰 배스트첼린지에 등록된 웹툰 제목과 평점 가져오기크롬웹드라이브를 활용해 네이버 로그인 하기123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778from bs4 import BeautifulSoupimport requestsfrom selenium import webdriverimport timedef IT_news(): print('[IT/과학 뉴스]') url = 'https://news.naver.com/' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') news_list = soup.find('div', attrs={'id':'section_it'}) news_list1 = news_list.find_all('li', limit=3) now = time.strftime('%Y-%m-%d %H-%M', time.localtime(time.time())) for index, news in enumerate(news_list1): title = news.find('a').get_text().strip() link = news.find('a')['href'] print('{0}. {1}'.format(index + 1, title)) print('( 링크 : {0} )'.format(link)) with open('./{0}_news.txt'.format(now), 'a') as f: f.write('[IT/과학 뉴스] {0} : {1} \\n\\n'.format(index+1, title)) with open('./{0}_news.txt'.format(now), 'a') as f: f.write('\\n\\n\\n') print() print() print()def bestChallenge(): print('[bestChallenge]') url = 'https://comic.naver.com/genre/bestChallenge.nhn' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') now = time.strftime('%Y-%m-%d %H-%M', time.localtime(time.time())) bast = soup.find_all('dl', attrs={'class':'mainTodayGrade'}) bast1 = soup.find('div', attrs={'class':'mainTodayBox'}) bast2 = bast1.find_all('h4') i = ['1', '2', '3'] for toon, toon1, i1 in zip(bast2, bast, i): title = toon.find('a').get_text().strip() star = toon1.find('strong').get_text() print(' {0} 제목 : {1} ( 평점 : {2} ) '.format(i1, title, star)) with open('./{0}_news.txt'.format(now), 'a') as f: f.write('[bestChallenge] {0} 제목 : {1} ( 평점 : {2} ) \\n\\n'.format(i1, title, star)) with open('./{0}_news.txt'.format(now), 'a') as f: f.write('') def naver_login(): naver = 'https://www.naver.com/' browser = webdriver.Chrome() browser.get(naver) browser.find_element_by_xpath('//*[@id=&quot;account&quot;]/a').click() time.sleep(2) browser.find_element_by_id('id').send_keys('naverid') time.sleep(1) browser.find_element_by_id('pw').send_keys('naverpw') time.sleep(1) browser.find_element_by_id('log.login').click() time.sleep(2) if __name__ == '__main__': IT_news() bestChallenge() naver_login()","link":"/2021/07/02/A/A029_assignment/"},{"title":"B001-Numpy_기본","text":"사용 버전 - Python 3.9.5 64-bit 1234567891011121314151617181920212223242526import numpy as np# 기본a = np.arange(15).reshape(3, 5)a# array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])print(a.shape)# (3, 5)print(a.ndim)# 2print(a.dtype.name)# 'int64'print(a.itemsize)# 8print(a.size)# 15type(a)# &lt;class 'numpy.ndarray'&gt;b = np.array([6, 7, 8])b = ([6, 7, 8])type(b)# &lt;class 'numpy.ndarray'&gt;","link":"/2021/07/05/B/B001_Numpy_%EA%B8%B0%EB%B3%B8/"},{"title":"B002_Numpy_배열 생성","text":"사용 버전 - Python 3.9.5 64-bit 12345678910111213141516171819202122232425import numpy as np# 배열 생성a = np.array([2, 3, 4])aprint(a)# [2, 3, 4]print(a.dtype)# int64b = np.array([(1.5, 2, 3), (4, 5, 6)])print(b.dtype)# float64print(b)# [[1.5 2. 3. ]# [4. 5. 6. ]]c = np.array([[1, 2], [3, 4]], dtype=complex)print(c)# [[1.+0.j 2.+0.j]# [3.+0.j 4.+0.j]]","link":"/2021/07/05/B/B002_Numpy_%EB%B0%B0%EC%97%B4%20%EC%83%9D%EC%84%B1/"},{"title":"B003_Numpy_배열 생성","text":"사용 버전 - Python 3.9.5 64-bit 123456789101112131415161718192021222324252627282930313233343536import numpy as np# 배열 생성a = np.zeros((3, 4))print(a)# [[0. 0. 0. 0.]# [0. 0. 0. 0.]# [0. 0. 0. 0.]]b = np.ones((2, 3, 4), dtype=np.int16)print(b)# [[[1 1 1 1]# [1 1 1 1]# [1 1 1 1]]# [[1 1 1 1]# [1 1 1 1]# [1 1 1 1]]]print(b.dtype)# int16c = np.empty((2, 3))print(c)# [[9.34577196e-307 9.34598246e-307 1.60218491e-306]# [1.69119873e-306 1.24611673e-306 1.05699581e-307]]d = np.arange(10, 30, 5)print(d)# [10 15 20 25]e = np.arange(0, 2, 0.3) print(e)# [0. 0.3 0.6 0.9 1.2 1.5 1.8]","link":"/2021/07/07/B/B003_Numpy_%EB%B0%B0%EC%97%B4%20%EC%83%9D%EC%84%B1/"},{"title":"B004_Numpy_배열 인쇄","text":"사용 버전 - Python 3.9.5 64-bit 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import numpy as npfrom numpy import pi# 배열 인쇄a = np.linspace(0, 2, 9) # 9 numbers from 0 to 2print(a)# [0. 0.25 0.5 0.75 1. 1.25 1.5 1.75 2. ]x = np.linspace(0, 2 * pi, 100) # useful to evaluate function at lots of pointsprint(x)# [0. 0.06346652 0.12693304 0.19039955 0.25386607 0.31733259# 0.38079911 0.44426563 0.50773215 0.57119866 0.63466518 0.6981317 # 0.76159822 0.82506474 0.88853126 0.95199777 1.01546429 1.07893081 # 1.14239733 1.20586385 1.26933037 1.33279688 1.3962634 1.45972992 # 1.52319644 1.58666296 1.65012947 1.71359599 1.77706251 1.84052903 # 1.90399555 1.96746207 2.03092858 2.0943951 2.15786162 2.22132814 # 2.28479466 2.34826118 2.41172769 2.47519421 2.53866073 2.60212725 # 2.66559377 2.72906028 2.7925268 2.85599332 2.91945984 2.98292636 # 3.04639288 3.10985939 3.17332591 3.23679243 3.30025895 3.36372547 # 3.42719199 3.4906585 3.55412502 3.61759154 3.68105806 3.74452458 # 3.8079911 3.87145761 3.93492413 3.99839065 4.06185717 4.12532369 # 4.1887902 4.25225672 4.31572324 4.37918976 4.44265628 4.5061228 # 4.56958931 4.63305583 4.69652235 4.75998887 4.82345539 4.88692191 # 4.95038842 5.01385494 5.07732146 5.14078798 5.2042545 5.26772102 # 5.33118753 5.39465405 5.45812057 5.52158709 5.58505361 5.64852012 # 5.71198664 5.77545316 5.83891968 5.9023862 5.96585272 6.02931923 # 6.09278575 6.15625227 6.21971879 6.28318531]f = np.sin(x)print(f)# [ 0.00000000e+00 6.34239197e-02 1.26592454e-01 1.89251244e-01# 2.51147987e-01 3.12033446e-01 3.71662456e-01 4.29794912e-01 # 4.86196736e-01 5.40640817e-01 5.92907929e-01 6.42787610e-01 # 6.90079011e-01 7.34591709e-01 7.76146464e-01 8.14575952e-01 # 8.49725430e-01 8.81453363e-01 9.09631995e-01 9.34147860e-01 # 9.54902241e-01 9.71811568e-01 9.84807753e-01 9.93838464e-01 # 9.98867339e-01 9.99874128e-01 9.96854776e-01 9.89821442e-01 # 9.78802446e-01 9.63842159e-01 9.45000819e-01 9.22354294e-01 # 8.95993774e-01 8.66025404e-01 8.32569855e-01 7.95761841e-01 # 7.55749574e-01 7.12694171e-01 6.66769001e-01 6.18158986e-01 # 5.67059864e-01 5.13677392e-01 4.58226522e-01 4.00930535e-01 # 3.42020143e-01 2.81732557e-01 2.20310533e-01 1.58001396e-01 # 9.50560433e-02 3.17279335e-02 -3.17279335e-02 -9.50560433e-02 # -1.58001396e-01 -2.20310533e-01 -2.81732557e-01 -3.42020143e-01 # -4.00930535e-01 -4.58226522e-01 -5.13677392e-01 -5.67059864e-01 # -6.18158986e-01 -6.66769001e-01 -7.12694171e-01 -7.55749574e-01 # -7.95761841e-01 -8.32569855e-01 -8.66025404e-01 -8.95993774e-01 # -9.22354294e-01 -9.45000819e-01 -9.63842159e-01 -9.78802446e-01 # -9.89821442e-01 -9.96854776e-01 -9.99874128e-01 -9.98867339e-01 # -9.93838464e-01 -9.84807753e-01 -9.71811568e-01 -9.54902241e-01 # -9.34147860e-01 -9.09631995e-01 -8.81453363e-01 -8.49725430e-01 # -8.14575952e-01 -7.76146464e-01 -7.34591709e-01 -6.90079011e-01 # -6.42787610e-01 -5.92907929e-01 -5.40640817e-01 -4.86196736e-01 # -4.29794912e-01 -3.71662456e-01 -3.12033446e-01 -2.51147987e-01 # -1.89251244e-01 -1.26592454e-01 -6.34239197e-02 -2.44929360e-16]","link":"/2021/07/09/B/B004_Numpy_%EB%B0%B0%EC%97%B4%20%EC%9D%B8%EC%87%84/"},{"title":"Plotly 그래프 HEXO 블로그에 올리기","text":"1 Anaconda 의 Jupyter Notebook을 실행한후 새 파일을 생성한다 2 plotly 회원가입하기공식사이트 3 API키 받기가입시 입력한 이메일로 접속해서 인증을 받음 공식사이트 우측상단에 세팅클릭 왼쪽에 API Keys 클릭 Regenerate Key 클릭 패스워드 입력 키확인 4 Jupyter Notebook에 다음 코드를 입력한다123456789 # 그래프 작성 코드import plotly.express as pximport chart_studiogapminder = px.data.gapminder()fig = px.scatter(gapminder.query(&quot;year==2007&quot;), x=&quot;gdpPercap&quot;, y=&quot;lifeExp&quot;, size=&quot;pop&quot;, color=&quot;continent&quot;, hover_name=&quot;country&quot;, log_x=True, size_max=60)fig.show() 출력값 유저정보 입력 코드12345678910111213 username = 'plotly 아이디입력' # your usernameapi_key = 'API 키입력' # your api key - go to profile &gt; settings &gt; regenerate keychart_studio.tools.set_credentials_file(username=username, api_key=api_key)import chart_studio.plotly as pypy.plot(fig, filename = 'gdp_per_cap', auto_open=True)# 결과 값 'https://plotly.com/~ldhjj77/1/' 12345678910 # iframe 변환 코드( url주소는 위쪽 코드 결과로 나온 본인주소로 변경해야함 )import chart_studio.tools as tlstls.get_embed('https://plotly.com/~ldhjj77/1.embed') #change to your url# 결과 값 '&lt;iframe id=&quot;igraph&quot; scrolling=&quot;no&quot; style=&quot;border:none;&quot; seamless=&quot;seamless&quot; src=&quot;https://plotly.com/~ldhjj77/1.embed&quot; height=&quot;525&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt;' 결과 값 중 링크값만 복사해서 다음 양식에 넣으면 끝{% iframe https://plotly.com/~ldhjj77/1.embed %} 위코드를 HEXO 마크다운파일에 입력최종 결과","link":"/2021/06/25/INFO/Plotly_iframe/"},{"title":"마크다운 기본 문법","text":"1. 글자 크기 조정123456# 가나다라## 가나다라### 가나다라#### 가나다라##### 가나다라###### 가나다라 가나다라가나다라가나다라가나다라가나다라가나다라 2. 글자 굵게1&lt;strong&gt;가나다라&lt;/strong&gt; 가나다라 3. 글자 기울기123&lt;em&gt; 가나다라&lt;/em&gt; 가나다라 4. 취소선123&lt;del&gt;가나다라&lt;/del&gt; 가나다라 5. 목록12345671. 가나다라2. 가나다라 - 가나다라 + 가나다라 * 가나다라 가나다라 가나다라 가나다라 가나다라 가나다라 6. 링크12&lt;a&gt;https://github.com/ldhjj77/ldhjj77.github.io&lt;/a&gt;[링크](https://github.com/ldhjj77/ldhjj77.github.io) https://github.com/ldhjj77/ldhjj77.github.io링크 7. 이미지 링크1![이미지업로드](http://www.gstatic.com/webp/gallery/5.jpg) 8. 테이블12345678910111213| 값 | 의미 | 기본값 ||---|:---:|---:|| `static` | 유형(기준) 없음 / 배치 불가능 | `static` || `relative` | 요소 자신을 기준으로 배치 | || `absolute` | 위치 상 부모(조상)요소를 기준으로 배치 | || `fixed` | 브라우저 창을 기준으로 배치 | |값 | 의미 | 기본값---|:---:|---:`static` | 유형(기준) 없음 / 배치 불가능 | `static``relative` | 요소 **자신**을 기준으로 배치 |`absolute` | 위치 상 **_부모_(조상)요소**를 기준으로 배치 |`fixed` | **브라우저 창**을 기준으로 배치 | 값 의미 기본값 static 유형(기준) 없음 / 배치 불가능 static relative 요소 자신을 기준으로 배치 absolute 위치 상 부모(조상)요소를 기준으로 배치 fixed 브라우저 창을 기준으로 배치 값 의미 기본값 static 유형(기준) 없음 / 배치 불가능 static relative 요소 자신을 기준으로 배치 absolute 위치 상 부모(조상)요소를 기준으로 배치 fixed 브라우저 창을 기준으로 배치 9. 줄바꿈12가나다라&lt;br&gt;&lt;br&gt;&lt;br&gt;가나다라 가나다라가나다라 10. 인용문123456789101112131415161718&lt;blockquote&gt;&gt; 가나다라&gt; 가나다라&gt;&gt; 가나다라&gt;&gt;&gt; 가나다라BREAK!&gt; 가나다라&gt;&gt; 가나다라&gt;&gt;&gt; 가나다라&gt;&gt;&gt;&gt; 가나다라&gt;&gt;&gt;&gt; 가나다라&gt;&gt;&gt;&gt; 가나다라&gt; 가나다라&gt;&gt; 가나다라&lt;/blockquote&gt; > 가나다라 > 가나다라 >> 가나다라 >>> 가나다라 BREAK! 가나다라 가나다라 가나다라 가나다라가나다라가나다라 가나다라 가나다라 인용문에 링크걸기 {% blockquote 사이트명 https://github.com/ldhjj77 링크 %} 가나다라 {% endblockquote %} 가나다라 사이트명링크 11. 코드의 표현방식 설정(md파일 내에서만 적용)1234567891011121314151617181920212223242526272829303132333435맨앞에 #은 지워야 함#```html&lt;a href=&quot;https://www.google.co.kr/&quot; target=&quot;_blank&quot;&gt;GOOGLE&lt;/a&gt;#```#```css.list &gt; li { position: absolute; top: 40px;}#```#```javascriptfunction func() { var a = 'AAA'; return a;}#```#```bash$ vim ./~zshrc#```#```pythons = &quot;Python syntax highlighting&quot;print s#```#```No language indicated, so no syntax highlighting. But let's throw in a tag.#``` 1&lt;a href=&quot;https://www.google.co.kr/&quot; target=&quot;_blank&quot;&gt;GOOGLE&lt;/a&gt; 1234.list &gt; li { position: absolute; top: 40px;} 1234function func() { var a = 'AAA'; return a;} 1$ vim ./~zshrc 12s = &quot;Python syntax highlighting&quot;print s 12No language indicated, so no syntax highlighting. But let's throw in a tag. 12. 코드 영역 설정(3가지 방식)12345 맨앞에 #은 제거#```python코드#``` 1코드 123&lt;pre&gt;&lt;code&gt;코드&lt;/code&gt;&lt;/pre&gt; 1코드 {% codeblock %} 코드 {% endcodeblock %} 1코드 코드에 제목과 링크걸기 {% codeblock 제목 https://github.com/ldhjj77 &nbsp링크 %} 코드 {% endcodeblock %} 제목&nbsp링크1코드 13. 주석처리하기( 컨트롤 + / 를 하면 주석처리됨)1234&lt;pre&gt;&lt;code&gt;&lt;!-- 주석처리할 내용 --&gt;주석처리할 내용&lt;/code&gt;&lt;/pre&gt; 주석처리할 내용 14. 테그와 카테고리 설정(보통 글의 최상단에 작성)123456789---title: A011-exceptiondate: 2021-06-23 15:45:49tags: - exception - 예외처리 - Acategories: Python--- 15. 유튜브 링크걸기유튜브 아이디 찾는법 코드 {% youtube video_id %}","link":"/2021/06/25/INFO/markdown_info/"},{"title":"E001_pycaret install error","text":"1 cmd창을 관리자모드로 실행 2 설치할 경로를 잡아주고 conda create –name 폴더명 python=3.8 # 입력 3 conda activate 폴더명 # 입력 4 pip install pycaret # 설치 시작 설치도중 다음과 같은 에러 발생 ERROR: Command errored out with exit status 1: 이하 생략 에러 가장 하단에 해답이 있었음 원인은 C++이 설치가 안되있어서 그런것 error: Microsoft Visual C++ 14.0 or greater is required. Get it with “Microsoft C++ Build Tools”: https://visualstudio.microsoft.com/visual-cpp-build-tools/ 설명에 있는 주소로 가서 build tools를 받아서 설치하고실행행후 왼쪽 위에 있는 C++ 을 체크해서 설치를 해주고 다시 설치시도 5 pip install pycaret pycaret 설치 완료 주피터 노트북 설정 그림과 같이 pycaret을 선택해준후 JupiterLap을 설치해주면 됨","link":"/2021/06/24/Error/E001_pycaret-install-error/"},{"title":"HEXO_기능 및 추가기능","text":"1. PDF 업로드하기설치하기( 쉘창에 입력 )1npm install --save hexo-pdf MD파일에 입력1{% pdf ./파일경로 %}","link":"/2021/07/12/INFO/HEXO_info/"},{"title":"B005_Numpy_배열 인쇄","text":"사용 버전 - Python 3.9.5 64-bit 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as np# 배열 인쇄a = np.arange(6) # 1d arrayprint(a)# [0 1 2 3 4 5]b = np.arange(12).reshape(4, 3) # 2d arrayprint(b)# [[ 0 1 2]# [ 3 4 5]# [ 6 7 8]# [ 9 10 11]]c = np.arange(24).reshape(2, 3, 4) # 3d arrayprint(c)# [[[ 0 1 2 3]# [ 4 5 6 7]# [ 8 9 10 11]]# [[12 13 14 15]# [16 17 18 19]# [20 21 22 23]]]print(np.arange(10000))# [ 0 1 2 ... 9997 9998 9999]print(np.arange(10000).reshape(100, 100))# [[ 0 1 2 ... 97 98 99]# [ 100 101 102 ... 197 198 199]# [ 200 201 202 ... 297 298 299]# ...# [9700 9701 9702 ... 9797 9798 9799]# [9800 9801 9802 ... 9897 9898 9899]# [9900 9901 9902 ... 9997 9998 9999]]import sysnp.set_printoptions(threshold=sys.maxsize) # 출력량이 많아서 중간을 생략한 부분을 강제로 모두 출력하도록 하는 명령어","link":"/2021/07/13/B/B005_Numpy_%EB%B0%B0%EC%97%B4%20%EC%9D%B8%EC%87%84/"},{"title":"B006_Numpy_기본 작동","text":"사용 버전 - Python 3.9.5 64-bit 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import numpy as np# 기본 작동a = np.array([20, 30, 40, 50])b = np.arange(4)print(b)# [0 1 2 3]c = a - bprint(c)# [20 29 38 47]print(b**2)# [0 1 4 9]print(10 * np.sin(a))# [ 9.12945251 -9.88031624 7.4511316 -2.62374854]print(a &lt; 35)# [ True True False False]A = np.array([[1, 1], [0, 1]])B = np.array([[2, 0], [3, 4]])print(A * B) # [[2 0]# [0 4]]print(A @ B) # [[5 4]# [3 4]]print(A.dot(B)) # [[5 4]# [3 4]]rg = np.random.default_rng(1) # create instance of default random number generatora = np.ones((2, 3), dtype=int)b = rg.random((2, 3))a *= 3print(a)# [[3 3 3]# [3 3 3]]b += aprint(b)# [[3.51182162 3.9504637 3.14415961]# [3.94864945 3.31183145 3.42332645]]# a += b # print(a) 해당 코드는 b는 정수형으로 자동 변환되지 않기때문에 에러가남","link":"/2021/07/13/B/B006_Numpy_%EA%B8%B0%EB%B3%B8%20%EC%9E%91%EB%8F%99/"},{"title":"B009_Numpy_범용 함수","text":"사용 버전 - Python 3.9.5 64-bit 12345678910111213141516import numpy as np# 범용 함수B = np.arange(3)print(B)# [0 1 2]print(np.exp(B))# [1. 2.71828183 7.3890561 ]print(np.sqrt(B))# [0. 1. 1.41421356]C = np.array([2., -1., 4.])print(np.add(B, C))# [2. 0. 6.]","link":"/2021/07/13/B/B009_Numpy_%EB%B2%94%EC%9A%A9%20%ED%95%A8%EC%88%98/"},{"title":"B011_Numpy_인덱싱, 슬라이싱 및 반복문","text":"사용 버전 - Python 3.9.5 64-bit 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import numpy as np# 인덱싱, 슬라이싱 및 반복def f(x, y): return 10 * x + yb = np.fromfunction(f, (5, 4), dtype=int)print(b)# [[ 0 1 2 3]# [10 11 12 13]# [20 21 22 23]# [30 31 32 33]# [40 41 42 43]]print(b[2, 3])# 23print(b[0:5, 1]) # each row in the second column of b# [ 1 11 21 31 41]print(b[:, 1]) # equivalent to the previous example# [ 1 11 21 31 41]print(b[1:3, :]) # each column in the second and third row of b# [[10 11 12 13]# [20 21 22 23]]print(b[-1])# [40 41 42 43]for row in b: print(row)# [0 1 2 3]# [10 11 12 13]# [20 21 22 23]# [30 31 32 33]# [40 41 42 43]for element in b.flat: print(element)# 0# 1# 2# 3# 10# 11# 12# 13# 20# 21# 22# 23# 30# 31# 32# 33# 40# 41# 42# 43","link":"/2021/07/15/B/B011_Numpy_%EC%9D%B8%EB%8D%B1%EC%8B%B1,%20%EC%8A%AC%EB%9D%BC%EC%9D%B4%EC%8B%B1%20%EB%B0%8F%20%EB%B0%98%EB%B3%B5/"},{"title":"B010_Numpy_인덱싱, 슬라이싱 및 반복문","text":"사용 버전 - Python 3.9.5 64-bit 1234567891011121314151617181920212223242526272829303132333435363738import numpy as np# 인덱싱, 슬라이싱 및 반복a = np.arange(10)**3print(a)# # [ 0 1 8 27 64 125 216 343 512 729]print(a[2])# # 8print(a[2:5])# # [ 8 27 64]# # equivalent to a[0:6:2] = 1000;# # from start to position 6, exclusive, set every 2nd element to 1000a[:6:2] = 1000print(a)# [1000 1 1000 27 1000 125 216 343 512 729]print(a[::-1]) # reversed a# [ 729 512 343 216 125 1000 27 1000 1 1000]for i in a: print(i**(1 / 3.))# 9.999999999999998# 1.0# 9.999999999999998# 3.0# 9.999999999999998# 5.0# 5.999999999999999# 6.999999999999999# 7.999999999999999# 8.999999999999998","link":"/2021/07/15/B/B010_Numpy_%EC%9D%B8%EB%8D%B1%EC%8B%B1,%20%EC%8A%AC%EB%9D%BC%EC%9D%B4%EC%8B%B1%20%EB%B0%8F%20%EB%B0%98%EB%B3%B5/"},{"title":"B012_Numpy_인덱싱, 슬라이싱 및 반복문","text":"사용 버전 - Python 3.9.5 64-bit 12345678910111213141516171819import numpy as np# 인덱싱, 슬라이싱 및 반복c = np.array([[[ 0, 1, 2], # a 3D array (two stacked 2D arrays) [ 10, 12, 13]], [[100, 101, 102], [110, 112, 113]]])print(c.shape)# (2, 2, 3)print(c[1, ...]) # same as c[1, :, :] or c[1]# [[100 101 102]# [110 112 113]]print(c[..., 2]) # same as c[:, :, 2]# [[ 2 13]# [102 113]]","link":"/2021/07/19/B/B012_Numpy_%EC%9D%B8%EB%8D%B1%EC%8B%B1,%20%EC%8A%AC%EB%9D%BC%EC%9D%B4%EC%8B%B1%20%EB%B0%8F%20%EB%B0%98%EB%B3%B5/"},{"title":"Tableau_맵핑001(위도와 경도 설정)","text":"1. 데이터로 쓸 엑셀파일을 하나 만들고 위도와 경도 컬럼을 만든다. 2. 구글맵에서 우클릭해서 나오는 위치정보를 가져온다. 3. 엑셀에 위도와 경도를 입력한다. 4. 엑셀에 도시명 컬럼을 만들고 서울을 입력한다. 그리고 저장하기 5. Tableau에서 방금만든 엑셀파일을 불러온다 6. 시트로 이동해서 위도와 경도를 설정해준다. 7. 위도와 경도 더블클릭. 8. 지도가 나타난걸 확인 할 수 있음. 9. 상단 매뉴에서 기본적인 맵옵션 변경가능.","link":"/2021/07/23/INFO/Tableau_%EB%A7%B5%ED%95%91_001/"},{"title":"코로나전국현황 크롤링 PPT &#x2F; PDF 링크","text":"클릭 –&gt; 코로나 전국현황 크롤링 PPT 클릭 –&gt; 코로나 전국현황 크롤링 PDF","link":"/2021/08/02/project_md/PDF/P005_%EC%BD%94%EB%A1%9C%EB%82%98%EC%A0%84%EA%B5%AD%ED%98%84%ED%99%A9_%ED%81%AC%EB%A1%A4%EB%A7%81_PDF/"},{"title":"타이타닉 PPT &#x2F; PDF 링크","text":"클릭 –&gt; 타이타닉 PPT 클릭 –&gt; 타이타닉 PDF","link":"/2021/07/12/project_md/PDF/P002_%EC%BC%80%EA%B8%80_%ED%83%80%EC%9D%B4%ED%83%80%EB%8B%89_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_PDF/"},{"title":"케글 7월의 놀이터 PPT &#x2F; PDF 링크","text":"클릭 –&gt; 케글 7월의 놀이터 PPT 클릭 –&gt; 케글 7월의 놀이터 PDF","link":"/2021/07/12/project_md/PDF/P003_%EC%BC%80%EA%B8%80_Tabular_Playground_202107_PDF/"},{"title":"정보보안 프로젝트 PPT &#x2F; PDF 링크","text":"클릭 –&gt; 정보보안 PPT 클릭 –&gt; 정보보안 PDF","link":"/2021/07/12/project_md/PDF/P001_%EC%A0%95%EB%B3%B4%EB%B3%B4%EC%95%88_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_PDF/"},{"title":"네이버 날씨 크롤링 PPT &#x2F; PDF 링크","text":"클릭 –&gt; 네이버 날씨 크롤링 PPT 클릭 –&gt; 네이버 날씨 크롤링 PDF","link":"/2021/07/23/project_md/PDF/P004_%EB%82%A0%EC%94%A8_%ED%81%AC%EB%A1%A4%EB%A7%81_PDF/"},{"title":"날씨_크롤링(대구)(수정)","text":"사용 버전 - Python 3.9.5 64-bit 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import requestsfrom bs4 import BeautifulSoupimport pandas as pdimport globimport os# 타입이 bs4.element.ResultSet 이면 get_text가 사용불가# bs4.element.Tag 일경우엔 사용가능# to2 = today.select('li:nth-of-type(1)') # 속성의 순서대로 검색# 딕셔너리로 데이터프레임 만들면 같은내용 복사가 가능함(좌표)def 대구날씨(): urllist = ['https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EC%A4%91%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%82%A8%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnhJ%2BdprvN8ssvrmBmZssssssQw-143351', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%82%A8%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%82%A8%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnhJHwprvmZss76xMXdssssstMC-076095', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EC%88%98%EC%84%B1%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EC%A4%91%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnh8ulprvxsssn0AvC0ssssst2d-020805', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%8F%99%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EC%88%98%EC%84%B1%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hniPmsp0Jy0sstlROU0ssssstkK-093405', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%B6%81%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%8F%99%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hniPEsp0JXossOIhBnVssssssQo-391917', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EC%84%9C%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%B6%81%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnilldprvxZsseozVpsssssssXG-244070', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%8B%AC%EC%84%9C%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%B6%81%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hniPWsp0JXVssmVHitlssssst5K-512391' ] for u in urllist: url = u headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') area = soup.find('span', attrs={'class':'btn_select'}).get_text() # 지역명 # 주간 날씨 tomorrow = soup.find('ul', attrs={'class':'list_area _pageList'}) # 전체데이터 t1 = tomorrow.find_all('li') # 리스트 데이터 weekly = pd.DataFrame() for i in t1: day = i.find('span', attrs={'class': 'day_info'}).get_text() # 날짜 정보 num = i.find_all('span', attrs={'class': 'num'}) # 강수확률 수치 전체 num1 = num[0].get_text() # 강수확률 첫번째 자료 불러오기 num2 = num[1].get_text() # 강수확률 두번째 자료 불러오기 dayp = day # 날자 nummin = num1 # 오전 강수확률 nummax = num2 # 오휴 강수확률 point = i.find('dd').get_text() # 전체 기온 pointp = point.replace(&quot;°&quot; ,&quot;&quot;) # ° 삭제 pointp = pointp.split(&quot;/&quot;) # / 기준으로 문자열 분리 point1 = pointp[0] # 최저기온 point2 = pointp[1] # 최고기온 wtarray = pd.DataFrame({'지역' : [area], '날짜' : [dayp], '오전 강수확률' : [nummin], '오후 강수확률' : [nummax], '최저기온' : [point1], '최고기온' : [point2]}) weekly = weekly.append(wtarray) weekly.to_excel('{0}.xlsx'.format(area)) # weekly 시트 내용 # 파일 합치기 all_data = pd.DataFrame() for f in glob.glob('*.xlsx'): # w로 시작하는 모든파일을 반복하며 추가 df = pd.read_excel(f) # f파일의 내용을 df에 넣기 all_data = all_data.append(df, ignore_index=True) # df의 내용을 all_data에 추가 all_data.drop('Unnamed: 0', axis=1, inplace=True) # Unnamed: 0 삭제 좌표 = pd.DataFrame({'위도' : ['35.8357'] * 5 + ['35.8275'] * 5 + ['35.8855'] * 5 + ['35.9282'] * 5 + ['35.8750'] * 5 + ['35.8353'] * 5 + ['35.8656'] * 5, '경도' : ['128.5860'] * 5 + ['128.5290'] * 5 + ['128.6318'] * 5 + ['128.5775'] * 5 + ['128.5497'] * 5 + ['128.6632'] * 5 + ['128.5933'] * 5}) all_data['위도'] = 좌표['위도'] # 위도 추가 all_data['경도'] = 좌표['경도'] # 경도 추가 all_data.to_excel('./source/xlsx/005_네이버날씨_대구.xlsx') # 대구날씨 파일 # 필요없어진 파일 삭제 리스트 remove = ['대구광역시 남구 이천동', '대구광역시 달서구 성당동', '대구광역시 동구 신암동', '대구광역시 북구 칠성동1가', '대구광역시 서구 내당동', '대구광역시 수성구 범어동', '대구광역시 중구 동인동1가'] for i in remove: os.remove('{0}.xlsx'.format(i)) if __name__ == '__main__': 대구날씨()","link":"/2021/07/27/project_md/%EC%BD%94%EB%93%9C/P005_%EB%82%A0%EC%94%A8_%ED%81%AC%EB%A1%A4%EB%A7%81(%EB%8C%80%EA%B5%AC)(%EC%88%98%EC%A0%95)_%EC%BD%94%EB%93%9C/"},{"title":"날씨_크롤링(전국)","text":"사용 버전 - Python 3.9.5 64-bit 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from pandas.core.frame import DataFrameimport requestsfrom bs4 import BeautifulSoupimport pandas as pd# 타입이 bs4.element.ResultSet 이면 get_text가 사용불가# bs4.element.Tag 일경우엔 사용가능# to2 = today.select('li:nth-of-type(1)') # 속성의 순서대로 검색def 전국날씨(): url = 'https://search.naver.com/search.naver?sm=tab_sug.top&amp;where=nexearch&amp;query=%EC%A0%84%EA%B5%AD%EB%82%A0%EC%94%A8&amp;oquery=%EB%82%A0%EC%94%A8&amp;tqi=hniG7lp0J14ssiOjmSCssssss7G-015724&amp;acq=%EC%A0%84%EA%B5%AD%EB%82%A0%EC%94%A8&amp;acr=10&amp;qdt=0' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') # 날씨상태 서울 = soup.find('a', attrs={'class':'w_box ct001013'}).get_text() 춘천 = soup.find('a', attrs={'class':'w_box ct003007'}).get_text() 강릉 = soup.find('a', attrs={'class':'w_box ct003007'}).get_text() 대전 = soup.find('a', attrs={'class':'w_box ct004001'}).get_text() 청주 = soup.find('a', attrs={'class':'w_box ct006005'}).get_text() 대구 = soup.find('a', attrs={'class':'w_box ct007007'}).get_text() 광주 = soup.find('a', attrs={'class':'w_box ct011005'}).get_text() 전주 = soup.find('a', attrs={'class':'w_box ct010011'}).get_text() 부산 = soup.find('a', attrs={'class':'w_box ct008008'}).get_text() 백령도 = soup.find('a', attrs={'class':'w_box ct002001'}).get_text() 울릉도 = soup.find('a', attrs={'class':'w_box ct009002'}).get_text() 제주도 = soup.find('a', attrs={'class':'w_box ct012005'}).get_text() # 지역 = pd.DataFrame(['서울'] + ['춘천'] + ['강릉'] + ['대전'] + ['청주'] + ['대구'] + ['광주'] + ['전주'] + ['부산'] + ['백령도'] + ['울릉도'] + ['제주도']) 날씨 = [서울, 춘천, 강릉, 대전, 청주, 대구, 광주, 전주, 부산, 백령도, 울릉도, 제주도] 날씨1 = pd.DataFrame() for i in 날씨: a = i[:-3] # 날씨상태 b = i[-3] + i[-2] # 현재기온 c = pd.DataFrame([[a]+ [b]]) 날씨1 = 날씨1.append(c) 위도 = DataFrame() 경도 = DataFrame() 날씨1['지역'] = pd.Series(['서울','춘천','강릉','대전','청주','대구','광주','전주','부산','백령','울릉','제주'], index=날씨1.index) 날씨1['위도'] = pd.Series(['37.55277148225529','37.884604551086255', '37.7188738133609', '36.298443669486655', '36.7535834068629', '36.47429186120351', '35.0852405939778', '35.6823644990858', '35.43589255808068', '37.95109593665316', '37.52884596038456', '33.37508939020389'], index=날씨1.index) 날씨1['경도'] = pd.Series(['126.80515370195922', '127.74316185221805', '128.80744728995975', '126.89837578409714', '128.105309906976', '129.0721448593317', '126.797385195114', '127.72762483852838', '128.81521579680458', '124.6729994007028', '130.87662844275005', '126.55323507614226'], index=날씨1.index) 날씨1.columns = ['기상상황', '현재기온', '지역', '위도', '경도'] 날씨1.to_excel('./source/xlsx/004_네이버날씨_전국.xlsx', sheet_name='전국')if __name__ == '__main__': 전국날씨()","link":"/2021/07/22/project_md/%EC%BD%94%EB%93%9C/P004_%EB%82%A0%EC%94%A8_%ED%81%AC%EB%A1%A4%EB%A7%81(%EC%A0%84%EA%B5%AD)_%EC%BD%94%EB%93%9C/"},{"title":"코로나전국현황_크롤링_오라클연동_코드","text":"사용 버전 - Python 3.9.5 64-bit / 오라클 SQL 21.2.0.187 빌드 187.1842 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697from numpy.lib.function_base import appendimport requestsfrom bs4 import BeautifulSoupimport pandas as pdimport cx_Oraclefrom sqlalchemy import create_enginedef 코로나(): url = 'https://search.naver.com/search.naver?where=nexearch&amp;sm=top_hty&amp;fbm=1&amp;ie=utf8&amp;query=%EA%B5%AD%EB%82%B4+%EC%BD%94%EB%A1%9C%EB%82%98+%ED%99%95%EC%A7%84%EC%9E%90+%ED%98%84%ED%99%A9' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') 전체 = soup.find('div', attrs={'class':'api_subject_bx'}) 확진환자 = soup.find('li', attrs={'class':'info_01'}).get_text() 사망자 = soup.find('li', attrs={'class':'info_04'}).get_text() 지역명추출 = soup.find_all('td', attrs={'class':'align_center'}) 지역 = [] for i in 지역명추출: a = i.get_text().strip() 지역.append(a) if a == '세종' : break 지역 = pd.DataFrame(지역) 누적확진자추출 = soup.find_all('td', attrs={'class':'align_right'}) 확진자 = [] for i in 누적확진자추출: b = i.get_text().strip() c = b.replace(',','') # , 로 인해 숫자로 변환이 안되기때문에 제거 확진자.append(c) 누적확진자 = pd.DataFrame(확진자[0:35:2]) 전일확진자 = pd.DataFrame(확진자[1:37:2]) 코로나현황 = pd.concat([지역, 누적확진자 , 전일확진자], axis=1) 코로나현황.columns = ['지역', '누적확진자', '전일확진자'] 코로나현황.drop(코로나현황.index[7], inplace=True) # 불필요한 '검역'행 삭제 코로나현황2 = pd.DataFrame(코로나현황) 코로나현황2 = 코로나현황2.reset_index(drop=True) # 인덱스 재설정 return 코로나현황2 def 위도_경도(): 좌표리스트 = {'위도' : ['37.5666']+ ['37.4376'] + ['35.8722'] + ['37.4563'] + ['35.1794'] + ['35.4677'] + ['36.3346'] + ['36.6990'] + ['37.8710'] + ['36.3511'] + ['37.0243'] + ['35.1597'] + ['35.5388'] + ['35.7459'] + ['34.8824'] + ['33.5000'] + [' 36.4875'], '경도' : ['127.0780'] + ['127.5172'] + ['128.6025'] + ['126.7052'] + ['129.0755'] + ['128.2147'] + ['128.8789'] + ['126.8004'] + ['128.1657'] + ['127.6850'] + ['127.7010'] + ['126.8530'] + ['129.3166'] + ['127.1530'] + ['126.9929'] + ['126.5166'] + ['127.2816']} # 합계용 좌표 36.3750 / 128.0230 좌표 = pd.DataFrame(좌표리스트) return(좌표)코로나현황 = 코로나()코로나현황 = 코로나현황.apply(pd.to_numeric, errors = 'ignore')# 합계 추가# 코로나현황합계 = [{'지역' : '합계' , '누적확진자' : 코로나현황['누적확진자'].sum() ,# '전일확진자' : 코로나현황['전일확진자'].sum()}]# 코로나현황합계 = pd.DataFrame(코로나현황합계)# 코로나현황1 = 코로나현황.append(코로나현황합계)# 코로나현황2 = 코로나현황1.reset_index(drop=True, inplace=False) # 인덱스 재설정좌표 = 위도_경도()전국코로나현황 = pd.concat([코로나현황, 좌표 ], axis=1)전국코로나현황.to_excel('./source/xlsx/P007_전국코로나현황.xlsx') if __name__ == '__main__': 코로나() 위도_경도() # 데이터 프레임을 오라클 데이터베이스에 넣기# PROTOCOL=TCP / HOST = 서버주소 / PORT = 서버포트 / SERVICE_NAME = 데이터베이스이름dsn_tns = &quot;(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=localhost)(PORT=1521))\\ (CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=orcl)))&quot;pwd = 'tiger' # 사용자 비밀번호 / 아래 c##scott은 사용자명engine = create_engine('oracle+cx_oracle://c##scott:' + pwd + '@%s' % dsn_tns)전국코로나현황.to_sql('P007_전국코로나현황', engine.connect(), if_exists='replace', index=False)","link":"/2021/07/29/project_md/%EC%BD%94%EB%93%9C/P007_%EC%BD%94%EB%A1%9C%EB%82%98%EC%A0%84%EA%B5%AD%ED%98%84%ED%99%A9_%ED%81%AC%EB%A1%A4%EB%A7%81_%EC%BD%94%EB%93%9C/"},{"title":"날씨_크롤링(합본)_오라클연동","text":"사용 버전 - Python 3.9.5 64-bit / 오라클 SQL 21.2.0.187 빌드 187.1842 오라클 연동 추가 - 210729123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151import requestsfrom bs4 import BeautifulSoupimport pandas as pdimport globimport osfrom pandas.core.frame import DataFrameimport cx_Oraclefrom sqlalchemy import create_engine# 타입이 bs4.element.ResultSet 이면 get_text가 사용불가# bs4.element.Tag 일경우엔 사용가능# to2 = today.select('li:nth-of-type(1)') # 속성의 순서대로 검색# 딕셔너리로 데이터프레임 만들면 같은내용 복사가 가능함(좌표)def 대구날씨(): urllist = ['https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EC%A4%91%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%82%A8%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnhJ%2BdprvN8ssvrmBmZssssssQw-143351', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%82%A8%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%82%A8%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnhJHwprvmZss76xMXdssssstMC-076095', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EC%88%98%EC%84%B1%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EC%A4%91%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnh8ulprvxsssn0AvC0ssssst2d-020805', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%8F%99%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EC%88%98%EC%84%B1%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hniPmsp0Jy0sstlROU0ssssstkK-093405', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%B6%81%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%8F%99%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hniPEsp0JXossOIhBnVssssssQo-391917', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EC%84%9C%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%B6%81%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnilldprvxZsseozVpsssssssXG-244070', 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%8B%AC%EC%84%9C%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%B6%81%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hniPWsp0JXVssmVHitlssssst5K-512391' ] for u in urllist: url = u headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') area = soup.find('span', attrs={'class':'btn_select'}).get_text() # 지역명 # 주간 날씨 tomorrow = soup.find('ul', attrs={'class':'list_area _pageList'}) # 전체데이터 t1 = tomorrow.find_all('li') # 리스트 데이터 weekly = pd.DataFrame() for i in t1: day = i.find('span', attrs={'class': 'day_info'}).get_text() # 날짜 정보 num = i.find_all('span', attrs={'class': 'num'}) # 강수확률 수치 전체 num1 = num[0].get_text() # 강수확률 첫번째 자료 불러오기 num2 = num[1].get_text() # 강수확률 두번째 자료 불러오기 dayp = day # 날자 nummin = num1 # 오전 강수확률 nummax = num2 # 오휴 강수확률 point = i.find('dd').get_text() # 전체 기온 pointp = point.replace(&quot;°&quot; ,&quot;&quot;) # ° 삭제 pointp = pointp.split(&quot;/&quot;) # / 기준으로 문자열 분리 point1 = pointp[0] # 최저기온 point2 = pointp[1] # 최고기온 wtarray = pd.DataFrame({'지역' : [area], '날짜' : [dayp], '오전 강수확률' : [nummin], '오후 강수확률' : [nummax], '최저기온' : [point1], '최고기온' : [point2]}) weekly = weekly.append(wtarray) weekly.to_excel('{0}.xlsx'.format(area)) # weekly 시트 내용 # 파일 합치기 all_data = pd.DataFrame() for f in glob.glob('*.xlsx'): # w로 시작하는 모든파일을 반복하며 추가 df = pd.read_excel(f) # f파일의 내용을 df에 넣기 all_data = all_data.append(df, ignore_index=True) # df의 내용을 all_data에 추가 all_data.drop('Unnamed: 0', axis=1, inplace=True) # Unnamed: 0 삭제 좌표 = pd.DataFrame({'위도' : ['35.8357'] * 5 + ['35.8275'] * 5 + ['35.8855'] * 5 + ['35.9282'] * 5 + ['35.8750'] * 5 + ['35.8353'] * 5 + ['35.8656'] * 5, '경도' : ['128.5860'] * 5 + ['128.5290'] * 5 + ['128.6318'] * 5 + ['128.5775'] * 5 + ['128.5497'] * 5 + ['128.6632'] * 5 + ['128.5933'] * 5}) all_data['위도'] = 좌표['위도'] # 위도 추가 all_data['경도'] = 좌표['경도'] # 경도 추가 all_data.to_excel('./source/xlsx/P006_네이버날씨_대구.xlsx') # 대구날씨 파일 # 필요없어진 파일 삭제 리스트 remove = ['대구광역시 남구 이천동', '대구광역시 달서구 성당동', '대구광역시 동구 신암동', '대구광역시 북구 칠성동1가', '대구광역시 서구 내당동', '대구광역시 수성구 범어동', '대구광역시 중구 동인동1가'] for i in remove: os.remove('{0}.xlsx'.format(i)) # 데이터 프레임을 데이터베이스에 넣기 # PROTOCOL=TCP / HOST = 서버주소 / PORT = 서버포트 / SERVICE_NAME = 데이터베이스이름 dsn_tns = &quot;(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=localhost)(PORT=1521))\\ (CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=orcl)))&quot; pwd = 'tiger' # 사용자 비밀번호 / 아래 c##scott은 사용자명 engine = create_engine('oracle+cx_oracle://c##scott:' + pwd + '@%s' % dsn_tns) all_data.to_sql('P006_네이버날씨_대구', engine.connect(), if_exists='replace', index=False)def 전국날씨(): url = 'https://search.naver.com/search.naver?sm=tab_sug.top&amp;where=nexearch&amp;query=%EC%A0%84%EA%B5%AD%EB%82%A0%EC%94%A8&amp;oquery=%EB%82%A0%EC%94%A8&amp;tqi=hniG7lp0J14ssiOjmSCssssss7G-015724&amp;acq=%EC%A0%84%EA%B5%AD%EB%82%A0%EC%94%A8&amp;acr=10&amp;qdt=0' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') # 날씨상태 서울 = soup.find('a', attrs={'class':'w_box ct001013'}).get_text() 춘천 = soup.find('a', attrs={'class':'w_box ct003007'}).get_text() 강릉 = soup.find('a', attrs={'class':'w_box ct003007'}).get_text() 대전 = soup.find('a', attrs={'class':'w_box ct004001'}).get_text() 청주 = soup.find('a', attrs={'class':'w_box ct006005'}).get_text() 대구 = soup.find('a', attrs={'class':'w_box ct007007'}).get_text() 광주 = soup.find('a', attrs={'class':'w_box ct011005'}).get_text() 전주 = soup.find('a', attrs={'class':'w_box ct010011'}).get_text() 부산 = soup.find('a', attrs={'class':'w_box ct008008'}).get_text() 백령도 = soup.find('a', attrs={'class':'w_box ct002001'}).get_text() 울릉도 = soup.find('a', attrs={'class':'w_box ct009002'}).get_text() 제주도 = soup.find('a', attrs={'class':'w_box ct012005'}).get_text() # 지역 = pd.DataFrame(['서울'] + ['춘천'] + ['강릉'] + ['대전'] + ['청주'] + ['대구'] + ['광주'] + ['전주'] + ['부산'] + ['백령도'] + ['울릉도'] + ['제주도']) 날씨 = [서울, 춘천, 강릉, 대전, 청주, 대구, 광주, 전주, 부산, 백령도, 울릉도, 제주도] 날씨1 = pd.DataFrame() for i in 날씨: a = i[:-3] # 날씨상태 b = i[-3] + i[-2] # 현재기온 c = pd.DataFrame([[a]+ [b]]) 날씨1 = 날씨1.append(c) 위도 = DataFrame() 경도 = DataFrame() 날씨1['지역'] = pd.Series(['서울','춘천','강릉','대전','청주','대구','광주','전주','부산','백령','울릉','제주'], index=날씨1.index) 날씨1['위도'] = pd.Series(['37.55277148225529','37.884604551086255', '37.7188738133609', '36.298443669486655', '36.7535834068629', '36.47429186120351', '35.0852405939778', '35.6823644990858', '35.43589255808068', '37.95109593665316', '37.52884596038456', '33.37508939020389'], index=날씨1.index) 날씨1['경도'] = pd.Series(['126.80515370195922', '127.74316185221805', '128.80744728995975', '126.89837578409714', '128.105309906976', '129.0721448593317', '126.797385195114', '127.72762483852838', '128.81521579680458', '124.6729994007028', '130.87662844275005', '126.55323507614226'], index=날씨1.index) 날씨1.columns = ['기상상황', '현재기온', '지역', '위도', '경도'] 날씨1.to_excel('./source/xlsx/P006_네이버날씨_전국.xlsx', sheet_name='전국') # 데이터 프레임을 데이터베이스에 넣기 # PROTOCOL=TCP / HOST = 서버주소 / PORT = 서버포트 / SERVICE_NAME = 데이터베이스이름 dsn_tns = &quot;(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=localhost)(PORT=1521))\\ (CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=orcl)))&quot; pwd = 'tiger' # 사용자 비밀번호 / 아래 c##scott은 사용자명 engine = create_engine('oracle+cx_oracle://c##scott:' + pwd + '@%s' % dsn_tns) 날씨1.to_sql('P006_네이버날씨_전국', engine.connect(), if_exists='replace', index=False)if __name__ == '__main__': 대구날씨() 전국날씨()","link":"/2021/07/27/project_md/%EC%BD%94%EB%93%9C/P006_%EB%82%A0%EC%94%A8_%ED%81%AC%EB%A1%A4%EB%A7%81(%ED%95%A9%EB%B3%B8)/"},{"title":"케글_Tabular_Playground_202107","text":"1123456789101112import pandas as pdimport numpy as npimport seaborn as sns # 통계 데이터 시각화 패키지import matplotlib.pyplot as plt # 대화 형 플롯과 간단한 프로그래밍 플롯 생성import time # 시간데이터 처리용 모듈import warnings # 경고 메시지를 출력하고 걸러내는 모듈# simplefilter -&gt; 간단한 항목으로 이뤄진 제어문# 'ignore -&gt; 일치하는 경고를 인쇄하지 않음# FutureWarning -&gt; 폐지된 기능에 대한 경고의 베이스 범주warnings.simplefilter(action='ignore', category=FutureWarning) 21234train = pd.read_csv('../input/tabular-playground-series-jul-2021/train.csv')train # 7111열의 데이터가 있음 31234test = pd.read_csv('../input/tabular-playground-series-jul-2021/test.csv')test # 2247열의 데이터가 있음 4123456# concat -&gt; 인자로 주어진 배열이나 값들을 기존 배열에 합쳐서 새 배열을 반환# 기존배열을 변경하지 않고 추가된 새로운 배열을 반환all_data = pd.concat([train, test])all_data # 9358열의 데이터가 있음 5123all_data.info() # 합친 데이터 정보보기 612345678910111213141516171819202122232425262728293031323334# to_datetime -&gt; 데이터프레임을 datetime 으로 변환# 'date_time' 컬럼의 내용을 datetime으로 변환후 추가all_data['date_time'] = pd.to_datetime(all_data['date_time'])# 'year' 컬럼을 생성하고 'date_time' 데이터의 년도 데이터를 추가all_data['year'] = all_data['date_time'].dt.year# 'month' 컬럼을 생성하고 'date_time' 데이터의 월 데이터를 추가all_data['month'] = all_data['date_time'].dt.month# 'week' 컬럼을 생성하고 'date_time' 데이터의 주 데이터를 추가all_data['week'] = all_data['date_time'].dt.week# 'day' 컬럼을 생성하고 'date_time' 데이터의 일 데이터를 추가all_data['day'] = all_data['date_time'].dt.day# 'dayofweek' 컬럼을 생성하고 'date_time' 데이터에서 요일 구분을 위한 데이터 추가all_data['dayofweek'] = all_data['date_time'].dt.dayofweek# 'time' 컬럼을 생성하고 'date_time' 데이터에서 몇일차인지의 데이터를 추가# 현재날짜 - 데이터첫번째 날짜all_data['time'] = all_data['date_time'].dt.date - all_data['date_time'].dt.date.min()# 'hour' 컬럼을 생성하고 'date_time' 데이터의 시간 데이터를 추가all_data['hour'] = all_data['date_time'].dt.hour# 'time' 컬럼의 내용중 일 데이터만 추가(몇일차데이터 삽입)all_data['time'] = all_data['time'].apply(lambda x : x.days)# 'date_time' 컬럼 삭제all_data.drop(columns = 'date_time', inplace = True)all_data 712345678# 'dayofweek'의 데이터타입을 object로 변환all_data['dayofweek'] = all_data['dayofweek'].astype(object)# get_dummies -&gt; 가변수화 # 문자형을 수치형으로 바꾸기만 했을경우 관계성으로 인한 학습에러를 방지하기위해 가변수화의 과정이 필요함all_data = pd.get_dummies(all_data) 8123456# all_data['SMC'] = (all_data['absolute_humidity'] * 100) / all_data['relative_humidity']# all_data['Dew_Point'] = 243.12*(np.log(all_data['relative_humidity'] * 0.01) + (17.62 * all_data['deg_C'])/(243.12+all_data['deg_C']))/(17.62-(np.log(all_data['relative_humidity'] * 0.01)+17.62*all_data['deg_C']/(243.12+all_data['deg_C'])))# all_data['relative_humidity'] = all_data['relative_humidity']/100# all_data['deg_F'] = all_data['deg_C'] * 1.8 + 32 9123456789# 데이터 분리# train2 변수에 train의 열의 수만큼 all_data의 데이터를 추가train2 = all_data[:len(train)]# test2 변수에 train의 열의 수만큼 건너뛰고 all_data의 데이터를 추가test2 = all_data[len(train):]# train['SMC'] = train2['SMC'] 10123456# 편차값을 줄이기위한 로그 스케일링 함수 생성def log_scaling(col): # log1p -&gt; 0의 로그값을 구하려고 하면 에러가 나기때문에 원본에1을 더한다음 로그스케일링을 진행하도록 하는함수 col = np.log1p(col) return col 11123456# cols 차트 리스트 생성cols = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']for col in cols: # train2에 col 칼럼에 col의 내용을 로그스케일링 한 데이터를 추가 train2[col] = log_scaling(train2[col]) 1212345678910111213141516171819# Figure 객체를 생성하고 len(cols)=3 *2 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴fig, ax = plt.subplots(len(cols), 2, figsize=(12,12))n = 0for i in cols: # histplot -&gt; 변수에 대한 히스토그램 # 하나 혹은 두 개의 변수 분포를 나타내는 전형적인 시각화 도구로 범위에 포함화는 관측수를 세어 표시 # train[cols] 값에대한 단순그래프 # ax -&gt; 그래프 생성위치 설정 sns.histplot(train[i], ax=ax[n, 0]); # train2[cols] 값에대한 단순그래프 sns.histplot(train2[i], ax = ax[n, 1]); n += 1# tight_layout -&gt; 여백설정과 레이아웃 설정을 해줌fig.tight_layout()plt.show() 1312345678910111213141516171819202122232425# train_3에다가 train2의 데이타중 다음 3개의 칼럼'target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides' 을 삭제후 추가train_3 = train2.drop(columns = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'])# test_3에다가 test2의 데이타중 다음 3개의 칼럼'target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides' 을 삭제후 추가test_3 = test2.drop(columns = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'])# train_co에다가 train2의 데이타중 다음 2개의 칼럼 'target_benzene', 'target_nitrogen_oxides' 을 삭제후 추가train_co = train2.drop(columns = ['target_benzene', 'target_nitrogen_oxides'])# train_be에다가 train2의 데이타중 다음 2개의 칼럼 'target_carbon_monoxide', 'target_nitrogen_oxides' 을 삭제후 추가train_be = train2.drop(columns = ['target_carbon_monoxide', 'target_nitrogen_oxides'])# train_no에다가 train2의 데이타중 다음 2개의 칼럼 'target_carbon_monoxide', 'target_benzene' 을 삭제후 추가train_no = train2.drop(columns = ['target_carbon_monoxide', 'target_benzene'])# test_co에다가 test2의 데이타중 다음 2개의 칼럼'target_benzene', 'target_nitrogen_oxides' 을 삭제후 추가test_co = test2.drop(columns = ['target_benzene', 'target_nitrogen_oxides'])# test_be에다가 test2의 데이타중 다음 2개의 칼럼'target_carbon_monoxide', 'target_nitrogen_oxides' 을 삭제후 추가test_be = test2.drop(columns = ['target_carbon_monoxide', 'target_nitrogen_oxides'])# test_no에다가 test2의 데이타중 다음 2개의 칼럼'target_carbon_monoxide', 'target_benzene' 을 삭제후 추가test_no = test2.drop(columns = ['target_carbon_monoxide', 'target_benzene']) 141234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# Figure 객체를 생성하고 4*3 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴fig, ax = plt.subplots(4, 3, figsize = (12,10))# ax -&gt; 그래프 생성위치 설정# x축은 'year'(년도단위) y축은 'target_carbon_monoxide' 의 평균값을 'r' (red) 빨간색으로 표시ax[0,0].plot(train2.groupby(train2['year'])['target_carbon_monoxide'].mean(), 'r');# 'target_benzene' 의 평균값ax[0,1].plot(train2.groupby(train2['year'])['target_benzene'].mean(), 'r');# 'target_nitrogen_oxides' 의 평균값ax[0,2].plot(train2.groupby(train2['year'])['target_nitrogen_oxides'].mean(), 'r');# x축은 'month'(월단위) y축은 'target_carbon_monoxide' 의 평균값을 'b' (blue) 파란색으로 표시ax[1,0].plot(train2.groupby(train2['month'])['target_carbon_monoxide'].mean(), 'b');# 'target_benzene' 의 평균값ax[1,1].plot(train2.groupby(train2['month'])['target_benzene'].mean(), 'b');# 'target_nitrogen_oxides' 의 평균값ax[1,2].plot(train2.groupby(train2['month'])['target_nitrogen_oxides'].mean(), 'b');# x축은 'time'(몇일차) y축은 'target_carbon_monoxide' 의 평균값을 'y' (yellow) 노란색으로 표시ax[2,0].plot(train2.groupby(train2['time'])['target_carbon_monoxide'].mean(), 'y');# 'target_benzene' 의 평균값ax[2,1].plot(train2.groupby(train2['time'])['target_benzene'].mean(), 'y');# 'target_nitrogen_oxides' 의 평균값ax[2,2].plot(train2.groupby(train2['time'])['target_nitrogen_oxides'].mean(), 'y');# x축은 'hour'(시간) y축은 'target_carbon_monoxide' 의 평균값을 'black' 검은색으로 표시ax[3,0].plot(train2.groupby(train2['hour'])['target_carbon_monoxide'].mean(), 'black');# 'target_benzene' 의 평균값ax[3,1].plot(train2.groupby(train2['hour'])['target_benzene'].mean(), 'black');# 'target_nitrogen_oxides' 의 평균값ax[3,2].plot(train2.groupby(train2['hour'])['target_nitrogen_oxides'].mean(), 'black');# 그래프 제목설정ax[0,0].set_title('Year-CO')ax[0,1].set_title('Year-Benzene')ax[0,2].set_title('Year-NOx')ax[1,0].set_title('month-CO')ax[1,1].set_title('month-Benzene')ax[1,2].set_title('month-NOx')ax[2,0].set_title('time-CO')ax[2,1].set_title('time-Benzene')ax[2,2].set_title('time-NOx')ax[3,0].set_title('hour-CO')ax[3,1].set_title('hour-Benzene')ax[3,2].set_title('hour-NOx')# tight_layout -&gt; 여백설정과 레이아웃 설정을 해줌fig.tight_layout()plt.show() 1512345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# Figure 객체를 생성하고 3*3 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴fig, ax = plt.subplots(3, 3, figsize = (10,10))# ax -&gt; 그래프 생성위치 설정# x축은 'deg_C' y축은 'target_carbon_monoxide' 의 평균값을 'r' (red) 빨간색으로 표시ax[0,0].plot(train2.groupby(train2['deg_C'])['target_carbon_monoxide'].mean(), 'r');# 'target_benzene' 의 평균값ax[0,1].plot(train2.groupby(train2['deg_C'])['target_benzene'].mean(), 'r');# 'target_nitrogen_oxides' 의 평균값ax[0,2].plot(train2.groupby(train2['deg_C'])['target_nitrogen_oxides'].mean(), 'r');# x축은 'relative_humidity' y축은 'target_carbon_monoxide' 의 평균값을 'b' (blue) 파란색으로 표시ax[1,0].plot(train2.groupby(train2['relative_humidity'])['target_carbon_monoxide'].mean(), 'b');# 'target_benzene' 의 평균값ax[1,1].plot(train2.groupby(train2['relative_humidity'])['target_benzene'].mean(), 'b');# 'target_nitrogen_oxides' 의 평균값ax[1,2].plot(train2.groupby(train2['relative_humidity'])['target_nitrogen_oxides'].mean(), 'b');# x축은 'absolute_humidity' y축은 'target_carbon_monoxide' 의 평균값을 'y' (yellow) 노란색으로 표시ax[2,0].plot(train2.groupby(train2['absolute_humidity'])['target_carbon_monoxide'].mean(), 'y');# 'target_benzene' 의 평균값ax[2,1].plot(train2.groupby(train2['absolute_humidity'])['target_benzene'].mean(), 'y');# 'target_nitrogen_oxides' 의 평균값ax[2,2].plot(train2.groupby(train2['absolute_humidity'])['target_nitrogen_oxides'].mean(), 'y');# 그래프 제목설정ax[0,0].set_title('deg-CO')ax[0,1].set_title('deg-Benzene')ax[0,2].set_title('deg-NOx')ax[1,0].set_title('rel_humid-CO')ax[1,1].set_title('rel_humid-Benzene')ax[1,2].set_title('rel_humid-NOx')ax[2,0].set_title('ab_humid-CO')ax[2,1].set_title('ab_humid-Benzene')ax[2,2].set_title('ab_humid-NOx')# tight_layout -&gt; 여백설정과 레이아웃 설정을 해줌fig.tight_layout()plt.show() 16123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# Figure 객체를 생성하고 3*3 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴fig, ax = plt.subplots(5, 3, figsize = (10,13))# ax -&gt; 그래프 생성위치 설정# x축은 'sensor_1' y축은 'target_carbon_monoxide' 의 평균값을 'r' (red) 빨간색으로 표시ax[0,0].plot(train2.groupby(train2['sensor_1'])['target_carbon_monoxide'].mean(), 'r');# 'target_benzene' 의 평균값ax[0,1].plot(train2.groupby(train2['sensor_1'])['target_benzene'].mean(), 'r');# 'target_nitrogen_oxides' 의 평균값ax[0,2].plot(train2.groupby(train2['sensor_1'])['target_nitrogen_oxides'].mean(), 'r');# x축은 'sensor_2' y축은 'target_carbon_monoxide' 의 평균값을 'b' (blue) 파란색으로 표시ax[1,0].plot(train2.groupby(train2['sensor_2'])['target_carbon_monoxide'].mean(), 'b');# 'target_benzene' 의 평균값ax[1,1].plot(train2.groupby(train2['sensor_2'])['target_benzene'].mean(), 'b');# 'target_nitrogen_oxides' 의 평균값ax[1,2].plot(train2.groupby(train2['sensor_2'])['target_nitrogen_oxides'].mean(), 'b');# x축은 'sensor_3' y축은 'target_carbon_monoxide' 의 평균값을 'y' (yellow) 노란색으로 표시ax[2,0].plot(train2.groupby(train2['sensor_3'])['target_carbon_monoxide'].mean(), 'y');# 'target_benzene' 의 평균값ax[2,1].plot(train2.groupby(train2['sensor_3'])['target_benzene'].mean(), 'y');# 'target_nitrogen_oxides' 의 평균값ax[2,2].plot(train2.groupby(train2['sensor_3'])['target_nitrogen_oxides'].mean(), 'y');# x축은 'sensor_4' y축은 'target_carbon_monoxide' 의 평균값을 'black' 검은색으로 표시ax[3,0].plot(train2.groupby(train2['sensor_4'])['target_carbon_monoxide'].mean(), 'black');# 'target_benzene' 의 평균값ax[3,1].plot(train2.groupby(train2['sensor_4'])['target_benzene'].mean(), 'black');# 'target_nitrogen_oxides' 의 평균값ax[3,2].plot(train2.groupby(train2['sensor_4'])['target_nitrogen_oxides'].mean(), 'black');# x축은 'sensor_5' y축은 'target_carbon_monoxide' 의 평균값을 'violet' 보라색으로 표시ax[4,0].plot(train2.groupby(train2['sensor_5'])['target_carbon_monoxide'].mean(), 'violet');# 'target_benzene' 의 평균값ax[4,1].plot(train2.groupby(train2['sensor_5'])['target_benzene'].mean(), 'violet');# 'target_nitrogen_oxides' 의 평균값ax[4,2].plot(train2.groupby(train2['sensor_5'])['target_nitrogen_oxides'].mean(), 'violet');# 그래프 제목설정ax[0,0].set_title('sensor_1-CO')ax[0,1].set_title('sensor_1-Benzene')ax[0,2].set_title('sensor_1-NOx')ax[1,0].set_title('sensor_2-CO')ax[1,1].set_title('sensor_2-Benzene')ax[1,2].set_title('sensor_2-NOx')ax[2,0].set_title('sensor_3-CO')ax[2,1].set_title('sensor_3-Benzene')ax[2,2].set_title('sensor_3-NOx')ax[3,0].set_title('sensor_4-CO')ax[3,1].set_title('sensor_4-Benzene')ax[3,2].set_title('sensor_4-NOx')ax[4,0].set_title('sensor_5-CO')ax[4,1].set_title('sensor_5-Benzene')ax[4,2].set_title('sensor_5-NOx')# tight_layout -&gt; 여백설정과 레이아웃 설정을 해줌fig.tight_layout()plt.show() 17123456plt.figure(figsize=(12,12))# heatmap -&gt; X축과 Y축에 2개의 범주형 자료의 계급(class)별로 연속형 자료를 집계한 자료를 사용하여, 집계한 값에 비례하여 색깔을 다르게 해서 2차원으로 자료를 시각화# corr -&gt; correlation(상관관계)sns.heatmap(train2.corr()); 1812345678910111213141516171819202122232425262728293031323334353637383940# Figure 객체를 생성하고 3*3 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴fig, ax = plt.subplots(3, 3, figsize = (20,15))# boxplot -&gt; 상자 그림 그래프# 자료로부터 얻어낸 통계량인 5가지 요약 수치로 그림# ax -&gt; 그래프 생성위치 설정# x축은 'year'(년단위) y축은 'target_carbon_monoxide' 을 사용sns.boxplot(train2['year'], train2['target_carbon_monoxide'], ax = ax[0, 0]);# y축을 'target_benzene' 을 사용sns.boxplot(train2['year'], train2['target_benzene'], ax= ax[0, 1]);# y축을 'target_nitrogen_oxides' 을 사용sns.boxplot(train2['year'], train2['target_nitrogen_oxides'], ax = ax[0, 2]);# x축은 'month'(월단위) y축은 'target_carbon_monoxide' 을 사용sns.boxplot(train2['month'], train2['target_carbon_monoxide'], ax = ax[1, 0]);# y축을 'target_benzene' 을 사용sns.boxplot(train2['month'], train2['target_benzene'], ax= ax[1, 1]);# y축을 'target_nitrogen_oxides' 을 사용sns.boxplot(train2['month'], train2['target_nitrogen_oxides'], ax = ax[1, 2]);# x축은 'hour'(시간대별단위) y축은 'target_carbon_monoxide' 을 사용sns.boxplot(train2['hour'], train2['target_carbon_monoxide'], ax = ax[2,0]);# y축을 'target_benzene' 을 사용sns.boxplot(train2['hour'], train2['target_benzene'], ax= ax[2,1]);# y축을 'target_nitrogen_oxides' 을 사용sns.boxplot(train2['hour'], train2['target_nitrogen_oxides'], ax = ax[2,2]);plt.show(); 19모델링1234# 파이케럿설치 / 코드를 처음 실행하는 경우라면 주석을지우고 한번 설치해야함# !pip install pycaret 2012345678910# pycaret.regression -&gt; 파이케럿 초기화# setup -&gt; 환경설정# compare_models -&gt; 모델성능비교# blend_models -&gt; 블렌드 모델(혼합모델)# finalize_model -&gt; 모델 완성# predict_model -&gt; 예측 모델# plot_model -&gt; 모델의 성능을 분석from pycaret.regression import setup, compare_models, blend_models, finalize_model, predict_model, plot_model 211234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 모델함수 생성# train -&gt;데이터셋, target -&gt; 컬럼, test -&gt; 테스트용 데이터셋# n_select -&gt; 성능평가후 상위 몇개의 모델을 반환 할것인가(5개 5개 4개를 사용)# opt -&gt; 정렬옵션 설정 ( 'RMSLE' 사용 )def pycaret_model(train, target, test, n_select, fold, opt): print('Setup Your Data....') # 환경 설정값 setup(data=train, # 데이터로 쓸 데이터 설정 target=target, # 데이터내의 어떤값을 쓸것인가 numeric_imputation = 'mean', # 결측값(nan등)에 평균값 입력 silent= True) # 에러메시지 침묵시킴 print('Comparing Models....') # compare_models -&gt; 모델을 비교해서 최고성능의 모델을 찾음 # sort=opt -&gt; 정렬 옵션(매개변수로 정의해놓은 상태 / 'RMSLE' 사용됨) # n_select -&gt; 최상위 모델 한개만 반환 # fold -&gt; 데이터셋의 숫자(매개변수로 정의해놓은 상태 / 3개 사용됨) # exclude -&gt; 블렉리스트 / include -&gt; 해당모델만 비교 best = compare_models(sort=opt, n_select=n_select, fold = fold, exclude = ['xgboost']) print('Here is Best Model Feature Importances!') # plot_model -&gt; 모델의 성능분석 # estimator -&gt; 측정자 / 'feature' -&gt; Feature Importance(기능 중요도) # time.sleep(5) -&gt; 5초간 대기 plot_model(estimator = best[0], plot = 'feature') time.sleep(5) print('Blending Models....') # blend_models -&gt; 블렌드 모델(혼합모델-추정자 간의 합의를 사용하여 최종 예측을 생성) # estimator_list -&gt; 추정자 리스트 # fold -&gt; 데이터셋의 숫자(매개변수로 정의해놓은 상태 / 3개 사용됨) # optimize -&gt; 정렬옵션('RMSLE'가 사용됨) blended = blend_models(estimator_list= best, fold=fold, optimize=opt) # predict_model -&gt; 정확도 예측(blend_models 사용) pred_holdout = predict_model(blended) print('Finallizing Models....') # finalize_model -&gt; 모델 완성(blend_models 사용) final_model = finalize_model(blended) print('Done...!!!') # 최종 예측 pred_esb = predict_model(final_model, test) re = pred_esb['Label'] return re 221234567# 셈플데이터 불러오기sub = pd.read_csv('../input/tabular-playground-series-jul-2021/sample_submission.csv')# sub의 'target_carbon_monoxide' 칼럼에 train_co의 'target_carbon_monoxide' 칼럼을 test_co 를 사용해서 최종예측값을 내고 그값의 지수값을 추가sub['target_carbon_monoxide'] = np.exp(pycaret_model(train_co, 'target_carbon_monoxide', test_co, 5, 3, 'RMSLE'))-1 231234# sub의 'target_benzene' 칼럼에 train_be의 'target_benzene' 칼럼을 test_co 를 사용해서 최종예측값을 내고 그값의 지수값을 추가sub['target_benzene'] = np.exp(pycaret_model(train_be, 'target_benzene', test_be, 5, 3, 'RMSLE'))-1 231234# sub의 'target_nitrogen_oxides' 칼럼에 train_no의 'target_nitrogen_oxides' 칼럼을 test_co 를 사용해서 최종예측값을 내고 그값의 지수값을 추가sub['target_nitrogen_oxides'] = np.exp(pycaret_model(train_no, 'target_nitrogen_oxides', test_no, 4, 3, 'RMSLE')) - 1 25123sub 26123sub.to_csv('sub.csv', index=False)","link":"/2021/07/13/project_md/%EC%BD%94%EB%93%9C/P002_%EC%BC%80%EA%B8%80_Tabular_Playground_202107_%EC%BD%94%EB%93%9C/"},{"title":"날씨_크롤링(대구)","text":"사용 버전 - Python 3.9.5 64-bit 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467import requestsimport refrom bs4 import BeautifulSoupimport pandas as pdimport numpy as npimport globimport os# 타입이 bs4.element.ResultSet 이면 get_text가 사용불가# bs4.element.Tag 일경우엔 사용가능# to2 = today.select('li:nth-of-type(1)') # 속성의 순서대로 검색def 대구_중구(): url = 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EC%A4%91%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%82%A8%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnhJ%2BdprvN8ssvrmBmZssssssQw-143351' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') area = soup.find('span', attrs={'class':'btn_select'}).get_text() # 지역명 cast = soup.find('p', attrs={'class':'cast_txt'}).get_text() # 맑음 흐림 min_temp = soup.find('span', attrs={'class':'min'}).get_text() # 최저온도 min = &quot;최저온도 : &quot; + min_temp max_temp = soup.find('span', attrs={'class':'max'}).get_text() # 최고온도 max = &quot;최고온도 : &quot; + max_temp # 오전 오후 강수 확률 morning_rain_rate = soup.find('span', attrs={'class': 'point_time morning'}).get_text().strip() # 오전 강수확률 / 공백제거 morning = &quot;오전 &quot; + morning_rain_rate afternoon_rain_rate = soup.find('span', attrs={'class': 'point_time afternoon'}).get_text().strip() # 오후 강수확률 / 공백제거 afternoon = &quot;오후 &quot; + afternoon_rain_rate # 미세 먼지 dust = soup.find('dl', attrs={'class':'indicator'}) pm = dust.find('dd', attrs={'class':'lv1'}).get_text() pmp = &quot;미세먼지 : &quot; + pm # 오늘날씨 today = pd.DataFrame([cast + min + max + morning + afternoon + pmp]) today.columns = ['예보'] today.loc[:, &quot;지역&quot;] = pd.Series([area], index=today.index) today.index = [area] # 주간 날씨 tomorrow = soup.find('ul', attrs={'class':'list_area _pageList'}) # 전체데이터 t1 = tomorrow.find_all('li') # 리스트 데이터 weekly = pd.DataFrame() for i in t1: day = i.find('span', attrs={'class': 'day_info'}).get_text() # 날짜 정보 num = i.find_all('span', attrs={'class': 'num'}) # 강수확률 수치 전체 num1 = num[0].get_text() # 강수확률 첫번째 자료 불러오기 num2 = num[1].get_text() # 강수확률 두번째 자료 불러오기 dayp = day # 날자 nummin = num1 # 오전 강수확률 nummax = num2 # 오휴 강수확률 point = i.find('dd').get_text() # 전체 기온 pointp = point.replace(&quot;°&quot; ,&quot;&quot;) # ° 삭제 pointp = pointp.split(&quot;/&quot;) # / 기준으로 문자열 분리 point1 = pointp[0] # 최저기온 point2 = pointp[1] # 최고기온 wtarray = pd.DataFrame([[area] + [dayp] + [nummin] + [nummax] + [point1] + [point2] + ['35.865676774945136'] + ['128.59339991922084']]) weekly = weekly.append(wtarray) weekly.columns = ['지역','날짜', '오전 강수확률', '오후 강수확률', '최저기온', '최고기온', '위도', '경도'] # weekly.index = [area +&quot; 당일&quot;,area +&quot; 1일후&quot;,area +&quot; 2일후&quot;,area +&quot; 3일후&quot;,area +&quot; 4일후&quot; ] with pd.ExcelWriter('w1.xlsx') as writer: # 엑셀파일 생성 weekly.to_excel(writer, sheet_name=area+'weekly') today.to_excel(writer, sheet_name=area+'today')def 대구_남구(): url = 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%82%A8%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%82%A8%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnhJHwprvmZss76xMXdssssstMC-076095' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') area = soup.find('span', attrs={'class':'btn_select'}).get_text() # 지역명 cast = soup.find('p', attrs={'class':'cast_txt'}).get_text() # 맑음 흐림 min_temp = soup.find('span', attrs={'class':'min'}).get_text() # 최저온도 min = &quot;최저온도 : &quot; + min_temp max_temp = soup.find('span', attrs={'class':'max'}).get_text() # 최고온도 max = &quot;최고온도 : &quot; + max_temp # 오전 오후 강수 확률 morning_rain_rate = soup.find('span', attrs={'class': 'point_time morning'}).get_text().strip() # 오전 강수확률 / 공백제거 morning = &quot;오전 &quot; + morning_rain_rate afternoon_rain_rate = soup.find('span', attrs={'class': 'point_time afternoon'}).get_text().strip() # 오후 강수확률 / 공백제거 afternoon = &quot;오후 &quot; + afternoon_rain_rate # 미세 먼지 dust = soup.find('dl', attrs={'class':'indicator'}) pm = dust.find('dd', attrs={'class':'lv1'}).get_text() pmp = &quot;미세먼지 : &quot; + pm # 오늘날씨 today = pd.DataFrame([cast + min + max + morning + afternoon + pmp]) today.columns = ['예보'] today.loc[:, &quot;지역&quot;] = pd.Series([area], index=today.index) today.index = [area] # 주간 날씨 tomorrow = soup.find('ul', attrs={'class':'list_area _pageList'}) # 전체데이터 t1 = tomorrow.find_all('li') # 리스트 데이터 weekly = pd.DataFrame() for i in t1: day = i.find('span', attrs={'class': 'day_info'}).get_text() # 날짜 정보 num = i.find_all('span', attrs={'class': 'num'}) # 강수확률 수치 전체 num1 = num[0].get_text() # 강수확률 첫번째 자료 불러오기 num2 = num[1].get_text() # 강수확률 두번째 자료 불러오기 dayp = day # 날자 nummin = num1 # 오전 강수확률 nummax = num2 # 오휴 강수확률 point = i.find('dd').get_text() # 전체 기온 pointp = point.replace(&quot;°&quot; ,&quot;&quot;) # ° 삭제 pointp = pointp.split(&quot;/&quot;) # / 기준으로 문자열 분리 point1 = pointp[0] # 최저기온 point2 = pointp[1] # 최고기온 wtarray = pd.DataFrame([[area] + [dayp] + [nummin] + [nummax] + [point1] + [point2] + ['35.835786350731176'] + ['128.58601516279393']]) weekly = weekly.append(wtarray) weekly.columns = ['지역','날짜', '오전 강수확률', '오후 강수확률', '최저기온', '최고기온', '위도', '경도'] # weekly.index = [area +&quot; 당일&quot;,area +&quot; 1일후&quot;,area +&quot; 2일후&quot;,area +&quot; 3일후&quot;,area +&quot; 4일후&quot; ] with pd.ExcelWriter('w2.xlsx') as writer: # 엑셀파일 생성 weekly.to_excel(writer, sheet_name='weekly') today.to_excel(writer, sheet_name='today')def 대구_수성구(): url = 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EC%88%98%EC%84%B1%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EC%A4%91%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnh8ulprvxsssn0AvC0ssssst2d-020805' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') area = soup.find('span', attrs={'class':'btn_select'}).get_text() # 지역명 cast = soup.find('p', attrs={'class':'cast_txt'}).get_text() # 맑음 흐림 min_temp = soup.find('span', attrs={'class':'min'}).get_text() # 최저온도 min = &quot;최저온도 : &quot; + min_temp max_temp = soup.find('span', attrs={'class':'max'}).get_text() # 최고온도 max = &quot;최고온도 : &quot; + max_temp # 오전 오후 강수 확률 morning_rain_rate = soup.find('span', attrs={'class': 'point_time morning'}).get_text().strip() # 오전 강수확률 / 공백제거 morning = &quot;오전 &quot; + morning_rain_rate afternoon_rain_rate = soup.find('span', attrs={'class': 'point_time afternoon'}).get_text().strip() # 오후 강수확률 / 공백제거 afternoon = &quot;오후 &quot; + afternoon_rain_rate # 미세 먼지 dust = soup.find('dl', attrs={'class':'indicator'}) pm = dust.find('dd', attrs={'class':'lv1'}).get_text() pmp = &quot;미세먼지 : &quot; + pm # 오늘날씨 today = pd.DataFrame([cast + min + max + morning + afternoon + pmp]) today.columns = ['예보'] today.loc[:, &quot;지역&quot;] = pd.Series([area], index=today.index) today.index = [area] # 주간 날씨 tomorrow = soup.find('ul', attrs={'class':'list_area _pageList'}) # 전체데이터 t1 = tomorrow.find_all('li') # 리스트 데이터 weekly = pd.DataFrame() for i in t1: day = i.find('span', attrs={'class': 'day_info'}).get_text() # 날짜 정보 num = i.find_all('span', attrs={'class': 'num'}) # 강수확률 수치 전체 num1 = num[0].get_text() # 강수확률 첫번째 자료 불러오기 num2 = num[1].get_text() # 강수확률 두번째 자료 불러오기 dayp = day # 날자 nummin = num1 # 오전 강수확률 nummax = num2 # 오휴 강수확률 point = i.find('dd').get_text() # 전체 기온 pointp = point.replace(&quot;°&quot; ,&quot;&quot;) # ° 삭제 pointp = pointp.split(&quot;/&quot;) # / 기준으로 문자열 분리 point1 = pointp[0] # 최저기온 point2 = pointp[1] # 최고기온 wtarray = pd.DataFrame([[area] + [dayp] + [nummin] + [nummax] + [point1] + [point2] + ['35.83534521914002'] + ['128.66325476750768']]) weekly = weekly.append(wtarray) weekly.columns = ['지역','날짜', '오전 강수확률', '오후 강수확률', '최저기온', '최고기온', '위도', '경도'] # weekly.index = [area +&quot; 당일&quot;,area +&quot; 1일후&quot;,area +&quot; 2일후&quot;,area +&quot; 3일후&quot;,area +&quot; 4일후&quot; ] with pd.ExcelWriter('w3.xlsx') as writer: # 엑셀파일 생성 weekly.to_excel(writer, sheet_name='weekly') today.to_excel(writer, sheet_name='today')def 대구_동구(): url = 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%8F%99%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EC%88%98%EC%84%B1%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hniPmsp0Jy0sstlROU0ssssstkK-093405' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') area = soup.find('span', attrs={'class':'btn_select'}).get_text() # 지역명 cast = soup.find('p', attrs={'class':'cast_txt'}).get_text() # 맑음 흐림 min_temp = soup.find('span', attrs={'class':'min'}).get_text() # 최저온도 min = &quot;최저온도 : &quot; + min_temp max_temp = soup.find('span', attrs={'class':'max'}).get_text() # 최고온도 max = &quot;최고온도 : &quot; + max_temp # 오전 오후 강수 확률 morning_rain_rate = soup.find('span', attrs={'class': 'point_time morning'}).get_text().strip() # 오전 강수확률 / 공백제거 morning = &quot;오전 &quot; + morning_rain_rate afternoon_rain_rate = soup.find('span', attrs={'class': 'point_time afternoon'}).get_text().strip() # 오후 강수확률 / 공백제거 afternoon = &quot;오후 &quot; + afternoon_rain_rate # 미세 먼지 dust = soup.find('dl', attrs={'class':'indicator'}) pm = dust.find('dd', attrs={'class':'lv1'}).get_text() pmp = &quot;미세먼지 : &quot; + pm # 오늘날씨 today = pd.DataFrame([cast + min + max + morning + afternoon + pmp]) today.columns = ['예보'] today.loc[:, &quot;지역&quot;] = pd.Series([area], index=today.index) today.index = [area] # 주간 날씨 tomorrow = soup.find('ul', attrs={'class':'list_area _pageList'}) # 전체데이터 t1 = tomorrow.find_all('li') # 리스트 데이터 weekly = pd.DataFrame() for i in t1: day = i.find('span', attrs={'class': 'day_info'}).get_text() # 날짜 정보 num = i.find_all('span', attrs={'class': 'num'}) # 강수확률 수치 전체 num1 = num[0].get_text() # 강수확률 첫번째 자료 불러오기 num2 = num[1].get_text() # 강수확률 두번째 자료 불러오기 dayp = day # 날자 nummin = num1 # 오전 강수확률 nummax = num2 # 오휴 강수확률 point = i.find('dd').get_text() # 전체 기온 pointp = point.replace(&quot;°&quot; ,&quot;&quot;) # ° 삭제 pointp = pointp.split(&quot;/&quot;) # / 기준으로 문자열 분리 point1 = pointp[0] # 최저기온 point2 = pointp[1] # 최고기온 wtarray = pd.DataFrame([[area] + [dayp] + [nummin] + [nummax] + [point1] + [point2] + ['35.885556427869986'] + ['128.6318876186057']]) weekly = weekly.append(wtarray) weekly.columns = ['지역','날짜', '오전 강수확률', '오후 강수확률', '최저기온', '최고기온', '위도', '경도'] # weekly.index = [area +&quot; 당일&quot;,area +&quot; 1일후&quot;,area +&quot; 2일후&quot;,area +&quot; 3일후&quot;,area +&quot; 4일후&quot; ] with pd.ExcelWriter('w4.xlsx') as writer: # 엑셀파일 생성 weekly.to_excel(writer, sheet_name='weekly') today.to_excel(writer, sheet_name='today')def 대구_북구(): url = 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%B6%81%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%8F%99%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hniPEsp0JXossOIhBnVssssssQo-391917' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') area = soup.find('span', attrs={'class':'btn_select'}).get_text() # 지역명 cast = soup.find('p', attrs={'class':'cast_txt'}).get_text() # 맑음 흐림 min_temp = soup.find('span', attrs={'class':'min'}).get_text() # 최저온도 min = &quot;최저온도 : &quot; + min_temp max_temp = soup.find('span', attrs={'class':'max'}).get_text() # 최고온도 max = &quot;최고온도 : &quot; + max_temp # 오전 오후 강수 확률 morning_rain_rate = soup.find('span', attrs={'class': 'point_time morning'}).get_text().strip() # 오전 강수확률 / 공백제거 morning = &quot;오전 &quot; + morning_rain_rate afternoon_rain_rate = soup.find('span', attrs={'class': 'point_time afternoon'}).get_text().strip() # 오후 강수확률 / 공백제거 afternoon = &quot;오후 &quot; + afternoon_rain_rate # 미세 먼지 dust = soup.find('dl', attrs={'class':'indicator'}) pm = dust.find('dd', attrs={'class':'lv1'}).get_text() pmp = &quot;미세먼지 : &quot; + pm # 오늘날씨 today = pd.DataFrame([cast + min + max + morning + afternoon + pmp]) today.columns = ['예보'] today.loc[:, &quot;지역&quot;] = pd.Series([area], index=today.index) today.index = [area] # 주간 날씨 tomorrow = soup.find('ul', attrs={'class':'list_area _pageList'}) # 전체데이터 t1 = tomorrow.find_all('li') # 리스트 데이터 weekly = pd.DataFrame() for i in t1: day = i.find('span', attrs={'class': 'day_info'}).get_text() # 날짜 정보 num = i.find_all('span', attrs={'class': 'num'}) # 강수확률 수치 전체 num1 = num[0].get_text() # 강수확률 첫번째 자료 불러오기 num2 = num[1].get_text() # 강수확률 두번째 자료 불러오기 dayp = day # 날자 nummin = num1 # 오전 강수확률 nummax = num2 # 오휴 강수확률 point = i.find('dd').get_text() # 전체 기온 pointp = point.replace(&quot;°&quot; ,&quot;&quot;) # ° 삭제 pointp = pointp.split(&quot;/&quot;) # / 기준으로 문자열 분리 point1 = pointp[0] # 최저기온 point2 = pointp[1] # 최고기온 wtarray = pd.DataFrame([[area] + [dayp] + [nummin] + [nummax] + [point1] + [point2] + ['35.92827446221116'] + ['128.5775049653885']]) weekly = weekly.append(wtarray) weekly.columns = ['지역','날짜', '오전 강수확률', '오후 강수확률', '최저기온', '최고기온', '위도', '경도'] # weekly.index = [area +&quot; 당일&quot;,area +&quot; 1일후&quot;,area +&quot; 2일후&quot;,area +&quot; 3일후&quot;,area +&quot; 4일후&quot; ] with pd.ExcelWriter('w5.xlsx') as writer: # 엑셀파일 생성 weekly.to_excel(writer, sheet_name='weekly') today.to_excel(writer, sheet_name='today') def 대구_서구(): url = 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EC%84%9C%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%B6%81%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hnilldprvxZsseozVpsssssssXG-244070' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') area = soup.find('span', attrs={'class':'btn_select'}).get_text() # 지역명 cast = soup.find('p', attrs={'class':'cast_txt'}).get_text() # 맑음 흐림 min_temp = soup.find('span', attrs={'class':'min'}).get_text() # 최저온도 min = &quot;최저온도 : &quot; + min_temp max_temp = soup.find('span', attrs={'class':'max'}).get_text() # 최고온도 max = &quot;최고온도 : &quot; + max_temp # 오전 오후 강수 확률 morning_rain_rate = soup.find('span', attrs={'class': 'point_time morning'}).get_text().strip() # 오전 강수확률 / 공백제거 morning = &quot;오전 &quot; + morning_rain_rate afternoon_rain_rate = soup.find('span', attrs={'class': 'point_time afternoon'}).get_text().strip() # 오후 강수확률 / 공백제거 afternoon = &quot;오후 &quot; + afternoon_rain_rate # 미세 먼지 dust = soup.find('dl', attrs={'class':'indicator'}) pm = dust.find('dd', attrs={'class':'lv1'}).get_text() pmp = &quot;미세먼지 : &quot; + pm # 오늘날씨 today = pd.DataFrame([cast + min + max + morning + afternoon + pmp]) today.columns = ['예보'] today.loc[:, &quot;지역&quot;] = pd.Series([area], index=today.index) today.index = [area] # 주간 날씨 tomorrow = soup.find('ul', attrs={'class':'list_area _pageList'}) # 전체데이터 t1 = tomorrow.find_all('li') # 리스트 데이터 weekly = pd.DataFrame() for i in t1: day = i.find('span', attrs={'class': 'day_info'}).get_text() # 날짜 정보 num = i.find_all('span', attrs={'class': 'num'}) # 강수확률 수치 전체 num1 = num[0].get_text() # 강수확률 첫번째 자료 불러오기 num2 = num[1].get_text() # 강수확률 두번째 자료 불러오기 dayp = day # 날자 nummin = num1 # 오전 강수확률 nummax = num2 # 오휴 강수확률 point = i.find('dd').get_text() # 전체 기온 pointp = point.replace(&quot;°&quot; ,&quot;&quot;) # ° 삭제 pointp = pointp.split(&quot;/&quot;) # / 기준으로 문자열 분리 point1 = pointp[0] # 최저기온 point2 = pointp[1] # 최고기온 wtarray = pd.DataFrame([[area] + [dayp] + [nummin] + [nummax] + [point1] + [point2] + ['35.87505470483516'] + ['128.54975242049758']]) weekly = weekly.append(wtarray) weekly.columns = ['지역','날짜', '오전 강수확률', '오후 강수확률', '최저기온', '최고기온', '위도', '경도'] # weekly.index = [area +&quot; 당일&quot;,area +&quot; 1일후&quot;,area +&quot; 2일후&quot;,area +&quot; 3일후&quot;,area +&quot; 4일후&quot; ] with pd.ExcelWriter('w6.xlsx') as writer: # 엑셀파일 생성 weekly.to_excel(writer, sheet_name='weekly') today.to_excel(writer, sheet_name='today')def 대구_달서구(): url = 'https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=nexearch&amp;query=%EB%8C%80%EA%B5%AC%EB%8B%AC%EC%84%9C%EA%B5%AC%EB%82%A0%EC%94%A8&amp;oquery=%EB%8C%80%EA%B5%AC%EB%B6%81%EA%B5%AC%EB%82%A0%EC%94%A8&amp;tqi=hniPWsp0JXVssmVHitlssssst5K-512391' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') area = soup.find('span', attrs={'class':'btn_select'}).get_text() # 지역명 cast = soup.find('p', attrs={'class':'cast_txt'}).get_text() # 맑음 흐림 min_temp = soup.find('span', attrs={'class':'min'}).get_text() # 최저온도 min = &quot;최저온도 : &quot; + min_temp max_temp = soup.find('span', attrs={'class':'max'}).get_text() # 최고온도 max = &quot;최고온도 : &quot; + max_temp # 오전 오후 강수 확률 morning_rain_rate = soup.find('span', attrs={'class': 'point_time morning'}).get_text().strip() # 오전 강수확률 / 공백제거 morning = &quot;오전 &quot; + morning_rain_rate afternoon_rain_rate = soup.find('span', attrs={'class': 'point_time afternoon'}).get_text().strip() # 오후 강수확률 / 공백제거 afternoon = &quot;오후 &quot; + afternoon_rain_rate # 미세 먼지 dust = soup.find('dl', attrs={'class':'indicator'}) pm = dust.find('dd', attrs={'class':'lv1'}).get_text() pmp = &quot;미세먼지 : &quot; + pm # 오늘날씨 today = pd.DataFrame([cast + min + max + morning + afternoon + pmp]) today.columns = ['예보'] today.loc[:, &quot;지역&quot;] = pd.Series([area], index=today.index) today.index = [area] # 주간 날씨 tomorrow = soup.find('ul', attrs={'class':'list_area _pageList'}) # 전체데이터 t1 = tomorrow.find_all('li') # 리스트 데이터 weekly = pd.DataFrame() for i in t1: day = i.find('span', attrs={'class': 'day_info'}).get_text() # 날짜 정보 num = i.find_all('span', attrs={'class': 'num'}) # 강수확률 수치 전체 num1 = num[0].get_text() # 강수확률 첫번째 자료 불러오기 num2 = num[1].get_text() # 강수확률 두번째 자료 불러오기 dayp = day # 날자 nummin = num1 # 오전 강수확률 nummax = num2 # 오휴 강수확률 point = i.find('dd').get_text() # 전체 기온 pointp = point.replace(&quot;°&quot; ,&quot;&quot;) # ° 삭제 pointp = pointp.split(&quot;/&quot;) # / 기준으로 문자열 분리 point1 = pointp[0] # 최저기온 point2 = pointp[1] # 최고기온 wtarray = pd.DataFrame([[area] + [dayp] + [nummin] + [nummax] + [point1] + [point2] + ['35.82758789651996'] + ['128.52908959379286']]) weekly = weekly.append(wtarray) weekly.columns = ['지역','날짜', '오전 강수확률', '오후 강수확률', '최저기온', '최고기온', '위도', '경도'] # weekly.index = [area +&quot; 당일&quot;,area +&quot; 1일후&quot;,area +&quot; 2일후&quot;,area +&quot; 3일후&quot;,area +&quot; 4일후&quot; ] with pd.ExcelWriter('w7.xlsx') as writer: # 엑셀파일 생성 weekly.to_excel(writer, sheet_name='weekly') today.to_excel(writer, sheet_name='today')if __name__ == '__main__': 대구_중구() 대구_남구() 대구_수성구() 대구_동구() 대구_북구() 대구_서구() 대구_달서구()all_data = pd.DataFrame() for f in glob.glob('w*.xlsx'): # w로 시작하는 모든파일을 반복하며 추가 df = pd.read_excel(f) # 내용을 df에 넣기 all_data = all_data.append(df, ignore_index=True) # df의 내용을 all_data에 추가 all_data.drop('Unnamed: 0', axis=1, inplace=True) # Unnamed: 0 삭제 all_data.to_excel('./source/xlsx/003_네이버날씨_대구.xlsx')os.remove('w1.xlsx')os.remove('w2.xlsx')os.remove('w3.xlsx')os.remove('w4.xlsx')os.remove('w5.xlsx')os.remove('w6.xlsx')os.remove('w7.xlsx')","link":"/2021/07/21/project_md/%EC%BD%94%EB%93%9C/P003_%EB%82%A0%EC%94%A8_%ED%81%AC%EB%A1%A4%EB%A7%81(%EB%8C%80%EA%B5%AC)_%EC%BD%94%EB%93%9C/"},{"title":"kaggle | 타이타닉으로 알아보는 Keras vs LightGBM vs CatBoost vs XGBoost 차이점 및 코드해석","text":"인용글 - https://www.kaggle.com/ajalnine/titanic-keras-vs-lightgbm-vs-catboost-vs-xgboost위 사이트에 가입후 우측상단에 Edit My Copy를 누르면 코드가 복사되고필요한 부분에 대한 결과값을 출력해 볼수 있음 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from math import * # C 표준에서 정의된 수학 함수import pandas as pd # 오픈소스 데이터 분석 및 조작 도구import numpy as np # 행렬이나 일반적으로 대규모 다차원 배열을 쉽게 처리 할 수 있도록 지원하는 파이썬의 라이브러리import tensorflow as tf # ML 모델을 개발하고 학습시키는 데 도움이 되는 핵심 오픈소스 라이브러리import warnings # 경고 메시지를 출력하고 걸러내는 모듈from sklearn.metrics import accuracy_score # 하위 집합의 정확도를 계산from sklearn.model_selection import train_test_split, StratifiedKFold# train_test_split - 배열 또는 행렬을 임의 학습 및 테스트 하위 집합으로 분할# StratifiedKFold - 계층화 된 K- 폴드 교차 검증 자from sklearn.preprocessing import Imputer, StandardScaler# Imputer - 결 측값 완성을위한 대치 변환기# StandardScaler - 평균을 제거하고 단위 분산에 맞게 조정하여 기능 표준화import matplotlib.pyplot as plt # 대화 형 플롯과 간단한 프로그래밍 플롯 생성 from matplotlib import gridspec # 영역을 내마음대로 나누고 싶을 때 사용import keras # 딥러닝 프레임워크from keras.models import Sequential# 각 레이어에 정확히 하나의 입력 텐서와 하나의 출력 텐서가 있는 일반 레이어 스택에 적합한 모델from keras.layers import Dense, Dropout, BatchNormalization, Activation# Dense - 입력과 출력을 모두 연결해주며, 입력과 출력을 각각 연결해주는 가중치를 포함하고 있음# Dropout - Dropout은 딥러닝 학습에 있어서의 문제중 하나인 Overfitting을 해소하기 위해 사용# 네트워크의 유닛의 일부만 동작하고 일부는 동작하지 않도록 하는 방법# BatchNormalization - vanishing/exploding gradient 문제를 해결하기위해 사용# Activation - 활성화 함수 tf.nn.relu또는 &quot;relu&quot;와 같은 내장 활성화 함수의 문자열 이름from keras.optimizers import Adam # 1 차 및 2 차 모멘트의 적응 적 추정을 기반으로하는 확률 적 경사 하강 법from keras import backend as K # 텐서 곱셈, 합성 곱 등의 저수준의 연산을 제공from tensorflow.python.client import device_lib # Tensorflow가 내 GPU를 활용하고 있는지 확인import lightgbm as lgb # 트리 기반 학습 알고리즘을 사용하는 그라디언트 부스팅 프레임 워크import catboost as cb # 데이터에 범주형 변수 많을 때 유용한 모델import xgboost as xgb # 분산 그라디언트 부스팅 라이브러리import seaborn as sns # 통계 데이터 시각화 패키지sns.set_style(&quot;whitegrid&quot;) # whitegrid 사용pd.set_option('display.max_columns', None) # 최대 열수 설정pd.set_option('display.max_rows', 200) # 출력 행수 설정warnings.filterwarnings('ignore') # 일치하는 경고를 인쇄하지 않습니다print(device_lib.list_local_devices())config = tf.ConfigProto(device_count={&quot;CPU&quot;: 1, &quot;GPU&quot; : 1}) # tensorflow에 사용할 장비 설정session = tf.Session(config=config) # tensorflow의 연산결과를 확인하기위한 변수K.set_session(session) # keras가 tensorflow의 세션을 사용할수 있도록 123456789101112131415161718192021222324252627282930### 2# 데이터 합치기#test = pd.read_csv(r&quot;c:\\work\\dataset\\titanic\\test.csv&quot;, &quot;,&quot;)#train = pd.read_csv(r&quot;c:\\work\\dataset\\titanic\\train.csv&quot;, &quot;,&quot;)test = pd.read_csv(&quot;../input/test.csv&quot;, &quot;,&quot;) # test파일 불러오기 (&quot;경로&quot;, &quot;문자열&quot;)train = pd.read_csv(&quot;../input/train.csv&quot;, &quot;,&quot;) # train파일 불러오기 (&quot;경로&quot;, &quot;문자열&quot;)test[&quot;is_test&quot;] = True # test함수에 &quot;is_test&quot; 컬럼을 생성하고 내용은 전부 True 입력train[&quot;is_test&quot;] = False # train함수에 &quot;is_test&quot; 컬럼을 생성하고 내용은 전부 Fales 입력 # 13개의 컬럼을 가진 common 함수 생성common = pd.concat([test, train],axis=0).loc[:,[&quot;PassengerId&quot;, &quot;Survived&quot;, &quot;is_test&quot;, &quot;Age&quot;, &quot;Cabin&quot;, &quot;Embarked&quot;, &quot;Fare&quot;, &quot;Name&quot;, &quot;Parch&quot;, &quot;Pclass&quot;, &quot;Sex&quot;, &quot;SibSp&quot;, &quot;Ticket&quot;]]# PassengerId : 순번 # survived : 생존=1, 죽음=0# pclass : 승객 등급. 1등급=1, 2등급=2, 3등급=3# Name : 이름# Sex : 성별# Age : 나이# sibsp : 함께 탑승한 형제 또는 배우자 수# parch : 함께 탑승한 부모 또는 자녀 수# ticket : 티켓 번호# Fare : 티켓가격# cabin : 선실 번호# embarked : 탑승장소 S=Southhampton, C=Cherbourg, Q=Queenstown 123456### 3# 중복 티켓 수 확인common[&quot;Ticket&quot;].count() - len(common[&quot;Ticket&quot;].unique()) 123456789101112131415161718192021222324252627### 4# train 데이터를 이용해 그룹화 / by=&quot;Ticket&quot; -&gt; &quot;Ticket&quot;의 값이 추가되고 / as_index=False -&gt; 인덱스 사용안함# .agg({&quot;PassengerId&quot; : 'count', -&gt; Cabin의 값이 같은 &quot;PassengerId&quot; 의 수를 'count'해서(합) &quot;PassengerId&quot; 에 추가 # &quot;Sex&quot; : lambda x : x[x==&quot;female&quot;].count()}) -&gt; &quot;female&quot; 일 경우 count()(합)의 값을 &quot;Sex&quot; 칼럼에 추가t = train.groupby(by=&quot;Ticket&quot;, as_index=False).agg({&quot;PassengerId&quot; : 'count', &quot;Sex&quot; : lambda x : x[x==&quot;female&quot;].count()})# t 의컬럼을 &quot;Ticket&quot;, &quot;SameTicket&quot;, &quot;FemalesOnTicket&quot; 으로 변경t.columns = [&quot;Ticket&quot;, &quot;SameTicket&quot;, &quot;FemalesOnTicket&quot;]# common 에다가 t 를 병합함(how=&quot;left&quot; -&gt; 왼쪽 프레임의 키만 사용 / on=&quot;Ticket&quot; -&gt; 조인 할 열 또는 인덱스 수준 이름)common = pd.merge(common, t, how=&quot;left&quot;, on=&quot;Ticket&quot;)# common데이터에 &quot;TicketDigits&quot; 컬럼을 생성하고 Ticket의 내용을 숫자로 변환해서 입력# str.split(&quot; &quot;) -&gt; 공백으로 문자열을 나눔 / errors=&quot;coerce&quot; -&gt; 잘못된 구문은 NaN으로 설정# .astype(np.str) -&gt; 문자형으로 변환 / .str.len() -&gt; 길이구하기# 결과는 문자의 길이가 나옴common[&quot;TicketDigits&quot;] = pd.to_numeric(common[&quot;Ticket&quot;].str.split(&quot; &quot;).str[-1], errors=&quot;coerce&quot;).astype(np.str).str.len()# common 데이터에 &quot;TicketIsNumber&quot; 칼럼 추가후 &quot;Ticket&quot; 의 내용을 추가# .str.contains(&quot;[A-Za-z]&quot;, regex = True) -&gt; [A-Za-z] 포함여부 확인후 포함되어있다면 True값 반환common[&quot;TicketIsNumber&quot;] = ~common[&quot;Ticket&quot;].str.contains(&quot;[A-Za-z]&quot;, regex = True)# common 데이터에 &quot;FemalesPerTicketPart&quot; 컬럼 추가후 common[&quot;FemalesOnTicket&quot;]/common[&quot;SameTicket&quot;] 값을 삽입common[&quot;FemalesPerTicketPart&quot;] = common[&quot;FemalesOnTicket&quot;]/common[&quot;SameTicket&quot;] 123456789101112### 5# Figure 객체를 생성하고 1*3 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴fig, ax = plt.subplots(1, 3, figsize=(20, 5))# 막대그래프 생성sns.barplot(x=&quot;TicketDigits&quot;, y=&quot;Survived&quot;, data=common, ax=ax[0]) # &quot;TicketDigits&quot; 의 생존률sns.barplot(x=&quot;TicketIsNumber&quot;, y=&quot;Survived&quot;, data=common, ax = ax[1]) # &quot;TicketIsNumber&quot; 의 생존률# 산점도 그래프 생성sns.regplot(x=&quot;FemalesPerTicketPart&quot;, y=&quot;Survived&quot;, data=common, ax = ax[2]) # &quot;FemalesPerTicketPart&quot; 의 생존률plt.show(); 123456789### 6# str.contains - 지정한 문자열이 포함되어있는지 확인(있으면 True값을 반환)# 중북이름 여부 컬럼 생성common[&quot;DoubleName&quot;] = common[&quot;Name&quot;].str.contains(&quot;\\(&quot;)# 이름길이 측정 컬럼 생성common[&quot;NameLen&quot;] = common[&quot;Name&quot;].str.len() 1234567891011### 7# Figure 객체를 생성하고 1*2 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴fig, ax = plt.subplots(1, 2, figsize=(20, 5))# &quot;DoubleName&quot; 의 생존률 막대그래프 생성sns.barplot(x=&quot;DoubleName&quot;, y=&quot;Survived&quot;, data=common, ax=ax[0])# &quot;NameLen&quot; 의 산점도 그래프 생성sns.regplot(x=&quot;NameLen&quot;, y=&quot;Survived&quot;, data=common, ax = ax[1])plt.show(); 12345678910111213141516171819202122### 8# title 컬럼 생성( Miss, Mr등 )common[&quot;Title&quot;] = common[&quot;Name&quot;].str.split(&quot;, &quot;).str[1].str.split(&quot; &quot;).str[0]common.loc[common[&quot;Title&quot;].str[-1]!=&quot;.&quot;, &quot;Title&quot;]=&quot;Bad&quot;# 특이한 타이틀(Col, Major 등) 추출하기 rare_title = common[&quot;Title&quot;].value_counts()[common[&quot;Title&quot;].value_counts() &lt; 5].index# 특이한 타이틀들을 Rare 로 변환하기common[&quot;Title&quot;] = common[&quot;Title&quot;].apply(lambda x: 'Rare' if x in rare_title else x)# 타이틀별 생존률titletarget = common.groupby(by=&quot;Title&quot;, as_index=False).agg({&quot;Survived&quot; : 'mean'})# titletarget의 컬럼을 &quot;Title&quot;, &quot;TargetByTitle&quot; 로 변환titletarget.columns = [&quot;Title&quot;, &quot;TargetByTitle&quot;]# common 에다가 titletarget 를 병합함(how=&quot;left&quot; -&gt; 왼쪽 프레임의 키만 사용 / on=&quot;Title&quot; -&gt; 조인 할 열 또는 인덱스 수준 이름)common = pd.merge(common, titletarget, how=&quot;left&quot;, on=&quot;Title&quot;) 1234567### 9# title 별 생존률 막대그래프sns.barplot(x=&quot;Title&quot;, y=&quot;TargetByTitle&quot;, data=common)plt.show() 12345678### 10# Family 컬럼을 생성하고 Parch + SibSp + 1 의값을 넣음common[&quot;Family&quot;] = common[&quot;Parch&quot;] + common[&quot;SibSp&quot;] + 1# Alone 컬럼을 생성하고 Family 가 1인 경우에 True 값을 넣고 그외엔 False를 넣음common[&quot;Alone&quot;] = common[&quot;Family&quot;] == 1 123456789101112131415161718192021222324252627282930313233343536373839### 11# train 데이터를 이용해 그룹화 / by=&quot;Cabin&quot; -&gt; Cabin의 값이 추가되고 / as_index=False -&gt; 인덱스 사용안함# .agg({&quot;PassengerId&quot; : 'count'}) -&gt; Cabin의 값이 같은 &quot;PassengerId&quot; 의 수를 'count'해서(합) &quot;PassengerId&quot; 에 추가 # 선실별로 인원수 구하기cap = train.groupby(by=&quot;Cabin&quot;, as_index=False).agg({&quot;PassengerId&quot; : 'count'})# cap의 컬럼을 &quot;Cabin&quot;, &quot;SameCabin&quot; 로 변경cap.columns = [&quot;Cabin&quot;, &quot;SameCabin&quot;]# common 데이터에 cap 데이터를 병합함(how=&quot;left&quot; -&gt; 왼쪽 프레임의 키만 사용 / on=&quot;Title&quot; -&gt; 조인 할 열 또는 인덱스 수준 이름)common = pd.merge(common, cap, how=&quot;left&quot;, on=&quot;Cabin&quot;)# common데이터에 &quot;CabinNumber&quot; 컬럼을 생성하고 Cabin의 내용을 숫자로 변환해서 입력# errors=&quot;coerce&quot; -&gt; 잘못된 구문은 NaN으로 설정common[&quot;CabinNumber&quot;] = pd.to_numeric(common[&quot;Cabin&quot;].str[1:], errors = &quot;coerce&quot;)# common 데이터에 &quot;CabinEven&quot; 컬럼을 생성후 &quot;CabinNumber&quot;값을 2로 나눈후 나머지값 입력(선실이 좌측인지 우측인지 구분)common[&quot;CabinEven&quot;] = common[&quot;CabinNumber&quot;] %2# &quot;CabinsPerMan&quot; 컬럼을 생성후 &quot;Cabin&quot; 의내용을 가져와서 공백으로 문자열을 분리하고 문자열의 갯수를 입력common[&quot;CabinsPerMan&quot;] = common[&quot;Cabin&quot;].str.split(&quot; &quot;).str.len()# &quot;Deck&quot; 컬럼을 생성후 &quot;Cabin&quot; 의내용을 가져와서 첫글자에따라 순위를 매기고 nan값에는 -1을 입력common[&quot;Deck&quot;] = common[&quot;Cabin&quot;].str[0].rank().fillna(-1)# common 데이터를 이용해 그룹화 / by=&quot;Deck&quot; -&gt; &quot;Deck&quot;의 값이 추가되고 / as_index=False -&gt; 인덱스 사용안함# .agg({&quot;Survived&quot; : 'mean'}) -&gt; &quot;Deck&quot;의 값이 같은 &quot;Survived&quot; 의 수를 'mean'해서(평균) &quot;Survived&quot; 에 추가 # 선실 앞자리 글자별 평균생존률 구하기decktarget = common.groupby(by=&quot;Deck&quot;, as_index=False).agg({&quot;Survived&quot; : 'mean'})# decktarget의 컬럼을 &quot;Deck&quot;, &quot;TargetByDeck&quot; 로 변환decktarget.columns = [&quot;Deck&quot;, &quot;TargetByDeck&quot;]# common 데이터에 decktarget 데이터를 병합함(how=&quot;left&quot; -&gt; 왼쪽 프레임의 키만 사용 / on=&quot;Deck&quot; -&gt; 조인 할 열 또는 인덱스 수준 이름)common = pd.merge(common, decktarget, how=&quot;left&quot;, on=&quot;Deck&quot;) 12345678910111213141516### 12# Figure 객체를 생성하고 1*3 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴fig, ax = plt.subplots(1, 3, figsize=(20, 5))# &quot;Deck&quot;의 생존률 막대그래프sns.barplot(x=&quot;Deck&quot;, y=&quot;TargetByDeck&quot;, data=common.sort_values(&quot;Deck&quot;), ax=ax[0])# &quot;CabinEven&quot;의 생존률 막대그래프sns.barplot(x=&quot;CabinEven&quot;, y=&quot;Survived&quot;, data=common, ax=ax[1])# &quot;CabinNumber&quot;의 생존률 산점도그래프sns.regplot(x=&quot;CabinNumber&quot;, y=&quot;Survived&quot;, data=common, ax=ax[2])plt.show() 123456### 13# 연령별 성별별 생존률의 점이 겹치지 않는 범주 형 산점도 그래프sns.swarmplot(x=&quot;Survived&quot;, y=&quot;Age&quot;, hue=&quot;Sex&quot;, palette=sns.color_palette([&quot;#20AFCF&quot;,&quot;#cf4040&quot;]), data=common)plt.show() 12345678910111213141516171819202122232425262728### 14# pd.qcut(common['Age'] -&gt; 동일한 갯수로 나눔 / fillna(common['Age'] -&gt; nan에 'Age'의 평균값을 정수형으로 삽입# 'AgeGroup' 컬럼을 생성하고 'Age'의 내용을 6개의 연령대를 만들어서 연령대로 구분(-0.001 ~ 19 / 19 ~ 25 / 25 ~ 29 / 29 ~ 30 / 30 ~ 41 / 41 ~ 80 )해서 삽입하고 # nan에는 평균값인 25 ~ 29가 들어감common['AgeGroup'] = pd.qcut(common['Age'].fillna(common['Age'].mean()).astype(int), 6)# common 데이터를 이용해 그룹화 / by=&quot;AgeGroup&quot; -&gt; &quot;AgeGroup&quot;의 값이 추가되고 / as_index=False -&gt; 인덱스 사용안함# .agg({&quot;Survived&quot; : 'mean'}) -&gt; &quot;AgeGroup&quot;의 값이 같은 &quot;Survived&quot; 의 수를 'mean'해서(평균) &quot;Survived&quot; 에 추가 # 연령 그룹별 평균생존률agetarget = common.groupby(by=&quot;AgeGroup&quot;, as_index=False).agg({&quot;Survived&quot; : 'mean'})# agetarget의 컬럼을 &quot;AgeGroup&quot;, &quot;TargetByAgeGroup&quot;로 변환agetarget.columns = [&quot;AgeGroup&quot;, &quot;TargetByAgeGroup&quot;]# common 데이터에 agetarget 데이터를 병합함(how=&quot;left&quot; -&gt; 왼쪽 프레임의 키만 사용 / on=&quot;AgeGroup&quot; -&gt; 조인 할 열 또는 인덱스 수준 이름)common = pd.merge(common, agetarget, how=&quot;left&quot;, on=&quot;AgeGroup&quot;)# &quot;IsTinyChild&quot; 컬럼을 생성후 &quot;Age&quot; &lt;1 값을 삽입common[&quot;IsTinyChild&quot;] = common[&quot;Age&quot;]&lt;1# &quot;IsChild&quot; 컬럼을 생성후 &quot;Age&quot; &lt;10 값을 삽입common[&quot;IsChild&quot;] = common[&quot;Age&quot;]&lt;10# &quot;AverageAge&quot; 컬럼을 생성후 &quot;Age&quot; / &quot;Family&quot; 값을 삽입common[&quot;AverageAge&quot;] = common[&quot;Age&quot;] / common[&quot;Family&quot;] 12345678910111213141516### 15# Figure 객체를 생성하고 1*3 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴fig, ax = plt.subplots(1, 3, figsize=(20, 5))# &quot;AgeGroup&quot;의 생존률 막대그래프sns.barplot(x=&quot;AgeGroup&quot;, y=&quot;TargetByAgeGroup&quot;, data=common, ax = ax[0])# &quot;IsChild&quot;의 생존률 막대그래프sns.barplot(x=&quot;IsChild&quot;, y=&quot;Survived&quot;, data=common, ax = ax[1])# &quot;AverageAge&quot;의 생존률 산점도그래프sns.regplot(x=&quot;AverageAge&quot;, y=&quot;Survived&quot;, data=common, ax = ax[2])plt.show(); 123456789101112131415161718192021222324252627### 16# 'FareGroup' 컬럼을 생성하고 'Fare'의 값을 6개의 그룹으로 나눠서 삽입(-0.001 ~ 7.0 / 7.0 ~ 8.0 / 8.0 ~ 14.0 / 14.0 ~ 26.0 / 26.0 ~ 53.0 / 53.0 ~ 512.0)# nan에는 'Fare' 값의 평균값을 정수로 삽입common['FareGroup'] = pd.qcut(common['Fare'].fillna(common['Fare'].mean()).astype(int), 6)# common 데이터를 이용해 그룹화 / by=&quot;FareGroup&quot; -&gt; &quot;FareGroup&quot;의 값이 추가되고 / as_index=False -&gt; 인덱스 사용안함# .agg({&quot;Survived&quot; : 'mean'}) -&gt; &quot;FareGroup&quot;의 값이 같은 &quot;Survived&quot; 의 수를 'mean'해서(평균) &quot;Survived&quot; 에 추가 # 페어그룹별 평균 생존률faretarget = common.groupby(by=&quot;FareGroup&quot;, as_index=False).agg({&quot;Survived&quot; : 'mean'})# faretarget의 컬럼을 &quot;FareGroup&quot;, &quot;TargetByFareGroup&quot;로 변경faretarget.columns = [&quot;FareGroup&quot;, &quot;TargetByFareGroup&quot;]# common 데이터에 faretarget 데이터를 병합함(how=&quot;left&quot; -&gt; 왼쪽 프레임의 키만 사용 / on=&quot;FareGroup&quot; -&gt; 조인 할 열 또는 인덱스 수준 이름)common = pd.merge(common, faretarget, how=&quot;left&quot;, on=&quot;FareGroup&quot;)# &quot;AverageFareByFamily&quot; 컬럼을 생성후 &quot;Fare&quot; / &quot;Family&quot; 값을 삽입common[&quot;AverageFareByFamily&quot;] = common[&quot;Fare&quot;] / common[&quot;Family&quot;]# &quot;AverageFareByTicket&quot; 컬럼을 생성후 &quot;Fare&quot; / &quot;SameTicket&quot; 값을 삽입common[&quot;AverageFareByTicket&quot;] = common[&quot;Fare&quot;] / common[&quot;SameTicket&quot;]# &quot;FareLog&quot; 컬럼을 생성후 &quot;Fare&quot; 의 로그값을 삽입common[&quot;FareLog&quot;] = np.log(common[&quot;Fare&quot;]) 1234567### 17# &quot;FareGroup&quot;의 생존률 막대그래프sns.barplot(x=&quot;FareGroup&quot;, y=&quot;TargetByFareGroup&quot;, data=common)plt.show() 123456789101112131415161718192021222324252627282930313233343536### 18# common 데이터를 이용해 그룹화 / by=&quot;Pclass&quot; -&gt;&quot;Pclass&quot;의 값이 추가되고 / as_index=False -&gt; 인덱스 사용안함# .agg({&quot;Survived&quot; : 'mean'}) -&gt; &quot;Pclass&quot;의 값이 같은 &quot;Survived&quot; 의 수를 'mean'해서(평균) &quot;Survived&quot; 에 추가 # 승객등급별 생존률 구하기pclasstarget = common.groupby(by=&quot;Pclass&quot;, as_index=False).agg({&quot;Survived&quot; : 'mean'})# pclasstarget의 컬럼을 &quot;Pclass&quot;, &quot;TargetByPclass&quot;로 변환pclasstarget.columns = [&quot;Pclass&quot;, &quot;TargetByPclass&quot;]# common 데이터에 pclasstarget 데이터를 병합함(how=&quot;left&quot; -&gt; 왼쪽 프레임의 키만 사용 / on=&quot;Pclass&quot; -&gt; 조인 할 열 또는 인덱스 수준 이름)common = pd.merge(common, pclasstarget, how=&quot;left&quot;, on=&quot;Pclass&quot;)# common 데이터를 이용해 그룹화 / by=&quot;Embarked&quot; -&gt; &quot;Embarked&quot;의 값이 추가되고 / as_index=False -&gt; 인덱스 사용안함# .agg({&quot;Survived&quot; : 'mean'}) -&gt; &quot;Embarked&quot;의 값이 같은 &quot;Survived&quot; 의 수를 'mean'해서(평균) &quot;Survived&quot; 에 추가 # 탑승장소별로 생존률 구하기Embarkedtarget = common.groupby(by=&quot;Embarked&quot;, as_index=False).agg({&quot;Survived&quot; : 'mean'})# Embarkedtarget의 컬럼을 &quot;Embarked&quot;, &quot;TargetByEmbarked&quot;로 변환Embarkedtarget.columns = [&quot;Embarked&quot;, &quot;TargetByEmbarked&quot;]# common 데이터에 Embarkedtarget 데이터를 병합함(how=&quot;left&quot; -&gt; 왼쪽 프레임의 키만 사용 / on=&quot;Embarked&quot; -&gt; 조인 할 열 또는 인덱스 수준 이름)common = pd.merge(common, Embarkedtarget, how=&quot;left&quot;, on=&quot;Embarked&quot;)# common 데이터를 이용해 그룹화 / by=&quot;Sex&quot; -&gt; &quot;Sex&quot;의 값이 추가되고 / as_index=False -&gt; 인덱스 사용안함# .agg({&quot;Survived&quot; : 'mean'}) -&gt; &quot;Sex&quot;의 값이 같은 &quot;Survived&quot; 의 수를 'mean'해서(평균) &quot;Survived&quot; 에 추가 # 성별별 생존률 구하기Sextarget = common.groupby(by=&quot;Sex&quot;, as_index=False).agg({&quot;Survived&quot; : 'mean'})# Sextarget의 컬럼을 &quot;Sex&quot;, &quot;TargetBySex&quot;로 변환Sextarget.columns = [&quot;Sex&quot;, &quot;TargetBySex&quot;]# common 데이터에 Sextarget 데이터를 병합함(how=&quot;left&quot; -&gt; 왼쪽 프레임의 키만 사용 / on=&quot;Sex&quot; -&gt; 조인 할 열 또는 인덱스 수준 이름)common = pd.merge(common, Sextarget, how=&quot;left&quot;, on=&quot;Sex&quot;) 12345678910111213141516### 19# Figure 객체를 생성하고 1*3 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴fig, ax = plt.subplots(1, 3, figsize=(20, 5))# &quot;Pclass&quot;의 생존률 막대그래프sns.barplot(x=&quot;Pclass&quot;, y=&quot;TargetByPclass&quot;, data=common, ax=ax[0])# &quot;Embarked&quot;의 생존률 막대그래프sns.barplot(x=&quot;Embarked&quot;, y=&quot;TargetByEmbarked&quot;, data=common, ax = ax[1])# &quot;Sex&quot;의 생존률 산점도그래프sns.barplot(x=&quot;Sex&quot;, y=&quot;TargetBySex&quot;, data=common, ax = ax[2])plt.show(); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657### 20# 사용할 모든 컬럼을 가진 allfeatures 변수 생성allfeatures = [ &quot;PassengerId&quot;, &quot;is_test&quot;, &quot;Survived&quot;, &quot;Age&quot;, &quot;Fare&quot;, &quot;Parch&quot;, &quot;Pclass&quot;, &quot;SibSp&quot;, &quot;Sex&quot;, &quot;Embarked&quot;, &quot;SameTicket&quot;, &quot;FemalesOnTicket&quot;, &quot;SameCabin&quot;, &quot;Deck&quot;, &quot;TargetByDeck&quot;, &quot;TargetByTitle&quot;, &quot;TargetByAgeGroup&quot;, &quot;TargetByFareGroup&quot;, &quot;TargetByPclass&quot;, &quot;TargetByEmbarked&quot;, &quot;TargetBySex&quot;, &quot;Title&quot;, &quot;CabinNumber&quot;, &quot;CabinEven&quot;, &quot;CabinsPerMan&quot;, &quot;DoubleName&quot;, &quot;NameLen&quot;, &quot;TicketDigits&quot;, &quot;TicketIsNumber&quot;, &quot;IsTinyChild&quot;, &quot;IsChild&quot;, &quot;Alone&quot;, &quot;Family&quot;, &quot;AverageAge&quot;, &quot;AverageFareByFamily&quot;, &quot;AverageFareByTicket&quot;, &quot;FemalesPerTicketPart&quot;]# c변수를 생성하고 common데이터의 모든행과 allfeatures에 들어있는 열의 정보를 가져오도록 함c = common.loc[:, allfeatures]# 문자형 데이터인 &quot;Title&quot;, &quot;Embarked&quot;, &quot;Pclass&quot;, &quot;Sex&quot;를 수치형 데이터로 변환후 가변수화함# 문자형 컬럼을 데이터 종류별로 컬럼을 만듬(title 을 Title_Dr. Title_Master. 등으로) # 문자형을 수치형으로 바꾸기만 했을경우 관계성으로 인한 학습에러를 방지하기위해 가변수화의 과정이 필요함c = pd.get_dummies(c, columns=[ &quot;Title&quot;, &quot;Embarked&quot;, &quot;Pclass&quot;, &quot;Sex&quot;]) 123456### 21# .describe() -&gt; 데이터 요약 / .sort_values(&quot;count&quot;) -&gt; &quot;count&quot; 컬럼을 기준으로 정렬c.describe().T.sort_values(&quot;count&quot;) 123456789101112131415161718192021222324252627282930313233### 22# .iloc -&gt; 정수기반 인덱싱 / Imputer(strategy=&quot;most_frequent&quot;) -&gt; 결측치를 &quot;most_frequent&quot;(최빈값)으로 대체# .fit_transform(c.iloc[:,3:]) -&gt; c.iloc[:,3:] 데이터의 계수추정과 자료변환 실행c.iloc[:,3:] = Imputer(strategy=&quot;most_frequent&quot;).fit_transform(c.iloc[:,3:])# dep변수에 &quot;is_test&quot;가 False인 &quot;Survived&quot;값 추가dep = c[c[&quot;is_test&quot;] == False].loc[:, [&quot;Survived&quot;]]# indep변수에 &quot;is_test&quot;가 False인 자료들의 3열이후의(4열부터) 값들 추가indep = c[c[&quot;is_test&quot;] == False].iloc[:, 3:]# res변수에 &quot;is_test&quot;가 True인 자료들의 3열이후의(4열부터) 값들 추가res = c[c[&quot;is_test&quot;] == True].iloc[:, 3:]# res_index변수에 &quot;is_test&quot;가 True인 자료들의 &quot;PassengerId&quot; 값을 추가res_index = c[c[&quot;is_test&quot;] == True].loc[:, &quot;PassengerId&quot;]# .iloc -&gt; 정수기반 인덱싱 # StandardScaler() -&gt; 평균을 제거하고 단위 분산으로 스케일링하여 기능 표준화# .fit_transform(indep.iloc[:,3:]) -&gt; indep.iloc[:,3:] 데이터의 계수추정과 자료변환 실행indep.iloc[:,3:] = StandardScaler().fit_transform(indep.iloc[:,3:])# StandardScaler() -&gt; 평균을 제거하고 단위 분산으로 스케일링하여 기능 표준화# .fit_transform(res.iloc[:,3:]) -&gt; res.iloc[:,3:] 데이터의 계수추정과 자료변환 실행res.iloc[:,3:] = StandardScaler().fit_transform(res.iloc[:,3:])# train_test_split -&gt; 배열 또는 행렬을 임의 학습 및 테스트 하위 집합으로 분할# test_size=0.40 -&gt; float 인 경우 0.0에서 1.0 사이 여야하며 테스트 분할에 포함 할 데이터 세트의 비율, int 인 경우 테스트 샘플의 절대 수를 나타냄# random_state -&gt; 렌덤함수의 스피드(수치를 바꾸면 렌덤데이터를 추출하는 레코드값이 달라짐)indep_train, indep_test, dep_train, dep_test = train_test_split(indep, dep, test_size=0.40, random_state=47) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667### Model 1. Keras MLP### 23# 수동으로 디바이스 배치하기 CPU 0번with tf.device('/device:CPU:0'): # 선형 레이어 구성 모델 gs1 = Sequential() # Dense(45 -&gt; (*, 45) 형태의 배열을 출력 # activation='linear' -&gt; 활성화 함수지정('linear'= 계산된 값을 그대로 출력으로 보냄) # input_dim=45 -&gt; (*, 45) 형태의 배열을 인풋으로 받음 gs1.add(Dense(45 ,activation='linear', input_dim=45)) # 학습성능을 높이기 위함 정규화 gs1.add(BatchNormalization()) # Dense(9 -&gt; (*, 9) 형태의 배열을 출력 # activation='linear' -&gt; 활성화 함수지정('linear'= 계산된 값을 그대로 출력으로 보냄) gs1.add(Dense(9,activation='linear')) # 학습성능을 높이기 위함 정규화 gs1.add(BatchNormalization()) # Overfitting을 해소하기 위해사용 0.4 유닛을 드롭함 gs1.add(Dropout(0.4)) # Dense(5 -&gt; (*, 5) 형태의 배열을 출력 # activation='linear' -&gt; 활성화 함수지정('linear'= 계산된 값을 그대로 출력으로 보냄) gs1.add(Dense(5,activation='linear')) # 학습성능을 높이기 위함 정규화 gs1.add(BatchNormalization()) # Overfitting을 해소하기 위해사용 0.2 유닛을 드롭함 gs1.add(Dropout(0.2)) # Dense(5 -&gt; (*, 5) 형태의 배열을 출력 # activation='relu' -&gt; 활성화 함수 지정('relu'=정류 된 선형 단위 활성화 함수) gs1.add(Dense(1,activation='relu', )) # 학습을 위한 모델 생성 # optimizer=Adam -&gt; 컴파일을 위한 매개변수(Adam : 값을 예측할 경우 사용) # lr=0.001 -&gt; 0보다 크거나 같은 float 값. 학습률. # beta_1=0.9 -&gt; 0보다 크고 1보다 작은 float 값. 일반적으로 1에 가깝게 설정 # beta_2=0.999 -&gt; 0보다 크고 1보다 작은 float 값. 일반적으로 1에 가깝게 설정 # epsilon -&gt; 수치 식에 사용되는 fuzz factor의 값을 반환 # decay -&gt; 0보다 크거나 같은 float 값. 업데이트마다 적용되는 학습률의 감소율 # loss='binary_crossentropy' -&gt; 이진 교차 엔트로피 손실을 계산 # metrics=['accuracy'] -&gt; 딥 러닝 모델의 성능을 평가하는 데 사용되는 함수('accuracy'=예측이 레이블과 동일한 빈도를 계산) gs1.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0), loss='binary_crossentropy', metrics=['accuracy']) # fit -&gt; 데이터학습용 파라미터 입력 # epochs -&gt; 정수. 모델을 학습시킬 세대의 수. 한 세대는 제공된 모든 x와 y 데이터에 대한 반복 # batch_size -&gt; 정수 혹은 None. 몇 개의 샘플로 가중치를 갱신할 것인지 지정. 따로 지정하지 않으면 디폴트인 32가 적용 # validation_data -&gt; 성능 평가용 데이터 # verbose -&gt; 평가단계에서 인쇄여부 1=true 매번인쇄, 100=100회 반복마다 인쇄 gs1.fit(indep_train, dep_train, epochs=500, batch_size=30, validation_data=(indep_test,dep_test), verbose=False) # # res를 기반으로 gs1 데이터셋 예측 수행() g=gs1.predict_classes(res)[:,0] print(accuracy_score(dep_test, gs1.predict_classes(indep_test)), accuracy_score(dep_train, gs1.predict_classes(indep_train))) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102### 24cvscores = [] # cvscore 리스트변수 생성data = pd.DataFrame() # data 데이터프레임 변수 생성i=1# tensorflow에 사용할 디바이스 설정 CPU 0번 사용with tf.device('/device:CPU:0'): # 교차검증을 위한 설정 StratifiedKFold -&gt; target에 속성값의 개수를 동일하게 가져감으로써 kfold 같이 데이터가 한곳으로 몰리는것을 방지 # n_splits -&gt; 데이터 분할 수 / shuffle -&gt; 데이터를 분할할때마다 데이터를 섞을지 여부 # random_state -&gt; 렌덤함수의 스피드(수치를 바꾸면 렌덤데이터를 추출하는 레코드값이 달라짐) # split(indep, dep.iloc[:,0]) -&gt; indep과 dep.iloc[:,0]로 분할 # dep.iloc[:,0] -&gt; dep의 [:,0]행 선택 for train, test in StratifiedKFold(n_splits=5, shuffle=True, random_state=1).split(indep, dep.iloc[:,0]): # indep의 재 인덱싱작업(train열의 형태로 모든행 입력) X = indep.reindex().iloc[train,:] # dep의 재 인덱싱작업(train열의 형태로 첫번째행 입력) Y = dep.reindex().iloc[train,0] # dep의 재 인덱싱작업(test열의 형태로 모든행 입력) Xv = indep.reindex().iloc[test,:] # dep의 재 인덱싱작업(test열의 형태로 첫번째행 입력) Yv = dep.reindex().iloc[test,0] # 선형 레이어 구성 모델 gs1 = Sequential() # Dense(45 -&gt; (*, 45) 형태의 배열을 출력 # activation='linear' -&gt; 활성화 함수지정('linear'= 계산된 값을 그대로 출력으로 보냄) # input_dim=45 -&gt; (*, 45) 형태의 배열을 인풋으로 받음 gs1.add(Dense(45 ,activation='linear', input_dim=45)) # 학습성능을 높이기 위함 정규화 gs1.add(BatchNormalization()) # Dense(9 -&gt; (*, 9) 형태의 배열을 출력 # activation='linear' -&gt; 활성화 함수지정('linear'= 계산된 값을 그대로 출력으로 보냄) gs1.add(Dense(9,activation='linear')) # 학습성능을 높이기 위함 정규화 gs1.add(BatchNormalization()) # Overfitting을 해소하기 위해사용 0.4 유닛을 드롭함 gs1.add(Dropout(0.4)) # Dense(5 -&gt; (*, 5) 형태의 배열을 출력 # activation='linear' -&gt; 활성화 함수지정('linear'= 계산된 값을 그대로 출력으로 보냄) gs1.add(Dense(5,activation='linear')) # 학습성능을 높이기 위함 정규화 gs1.add(BatchNormalization()) # Overfitting을 해소하기 위해사용 0.2 유닛을 드롭함 gs1.add(Dropout(0.2)) # Dense(5 -&gt; (*, 5) 형태의 배열을 출력 # activation='relu' -&gt; 활성화 함수 지정('relu'=정류 된 선형 단위 활성화 함수) gs1.add(Dense(1,activation='relu', )) # 학습을 위한 모델 생성 # optimizer=Adam -&gt; 컴파일을 위한 매개변수(Adam : 값을 예측할 경우 사용) # lr=0.001 -&gt; 0보다 크거나 같은 float 값. 학습률. # beta_1=0.9 -&gt; 0보다 크고 1보다 작은 float 값. 일반적으로 1에 가깝게 설정 # beta_2=0.999 -&gt; 0보다 크고 1보다 작은 float 값. 일반적으로 1에 가깝게 설정 # epsilon -&gt; 수치 식에 사용되는 fuzz factor의 값을 반환 # decay -&gt; 0보다 크거나 같은 float 값. 업데이트마다 적용되는 학습률의 감소율 # loss='binary_crossentropy' -&gt; 이진 교차 엔트로피- 손실을 계산 # metrics=['accuracy'] -&gt; 딥 러닝 모델의 성능을 평가하는 데 사용되는 함수('accuracy'=예측이 레이블과 동일한 빈도를 계산) gs1.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0), loss='binary_crossentropy', metrics=['accuracy']) # fit -&gt; 데이터학습용 파라미터 입력 # epochs -&gt; 정수. 모델을 학습시킬 세대의 수. 한 세대는 제공된 모든 x와 y 데이터에 대한 반복 # batch_size -&gt; 정수 혹은 None. 몇 개의 샘플로 가중치를 갱신할 것인지 지정. 따로 지정하지 않으면 디폴트인 32가 적용 # validation_data -&gt; 성능 평가용 데이터 # verbose -&gt; 평가단계에서 인쇄여부 1=true 매번인쇄, 100=100회 반복마다 인쇄 gs1.fit(X, Y, epochs=500, batch_size=30, validation_data=(Xv, Yv), verbose=False) # res를 기반으로 gs1 데이터셋 예측 수행() data[i] = gs1.predict_classes(res)[:,0] # evaluate -&gt; 테스트 모드에서의 모델의 손실 값과 측정항목 값을 반환 # verbose -&gt; 평가단계에서 인쇄여부 1=true 매번인쇄, 100=100회 반복마다 인쇄 scores = gs1.evaluate(Xv, Yv, verbose=0) print(gs1.metrics_names[1], scores[1]) # cvscores에 scores[1] 데이터 추가 cvscores.append(scores[1]) i+=1# mlp_mean에 cvscores의 평균값 추가mlp_mean = np.mean(cvscores)# mlp_stdev에 cvscores의 표준편차값 추가mlp_stdev = np.std(cvscores)print(mlp_mean, mlp_stdev)# g에 data값의 행값들의 평균을 반올림한 값을 추가g = np.round(data.mean(axis=1)) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950### 25# Figure 객체를 생성하고 1*3 subplot에 대응하는 Figure 객체와 Axes 객체의 리스트를 리턴 # sharex='col' -&gt; 각 서브 플롯 열은 x 축 또는 y 축을 공유fig, ax = plt.subplots(2, 1, sharex='col', figsize=(20, 10))# 차트제목 설정ax[0].set_title('Model accuracy history')# 훈련 정확도 확인ax[0].plot(gs1.history.history['acc'])# 검증 정확도 확인ax[0].plot(gs1.history.history['val_acc'])# y축 레이블 설정ax[0].set_ylabel('Accuracy')# 차트의 범례 설정ax[0].legend(['train', 'test'], loc='right')# 차트에 그리드 설정여부 기본은 Falseax[0].grid()# 차트제목 설정ax[1].set_title('Model loss history')# 훈련 손실값 확인ax[1].plot(gs1.history.history['loss'])# 검증 손실값 확인ax[1].plot(gs1.history.history['val_loss'])# y축 라벨 설정ax[1].set_ylabel('Loss')# 차트의 범례 설정ax[1].legend(['train', 'test'], loc='right')# 차트에 그리드 설정여부 기본은 Falseax[1].grid()# x축 라벨 설정plt.xlabel('Epoch')plt.show() 12345678### 26#result = pd.DataFrame(res_index.astype(np.int), columns=[&quot;PassengerId&quot;])#result[&quot;Survived&quot;] = g.astype(np.int)#result.to_csv(r&quot;c:\\work\\dataset\\titanic\\mlp.csv&quot;, &quot;,&quot;, index=None)#result.to_csv(&quot;mlp.csv&quot;, &quot;,&quot;, index=None) 12345678910111213141516171819202122232425### Model 2. LightGBM### 27# Example of manual parameter tuning&quot;&quot;&quot;for i in range(1,10): params = {} params[&quot;max_depth&quot;] = i params[&quot;learning_rate&quot;] = 0.45 params[&quot;lambda_l1&quot;] = 0.1 params[&quot;lambda_l2&quot;] = 0.01 params[&quot;n_estimators&quot;] = 5000 params[&quot;n_jobs&quot;]=5 params[&quot;objective&quot;] = &quot;binary&quot; params[&quot;boosting&quot;] = &quot;dart&quot; params[&quot;colsample_bytree&quot;] = 0.9 params[&quot;subsample&quot;] =0.9 train_data = lgb.Dataset(data=indep_train, label=dep_train, free_raw_data=False, feature_name = list(indep_train)) cv_result = lgb.cv(params, train_data, nfold=5, stratified=False, metrics=['binary_error'], early_stopping_rounds=50) print(i, 1-np.mean(cv_result[&quot;binary_error-mean&quot;])) &quot;&quot;&quot;; 1234567891011121314151617181920212223242526272829303132333435363738### 28# train_test_split -&gt; 배열 또는 행렬을 임의 학습 및 테스트 하위 집합으로 분할# test_size=0.40 -&gt; float 인 경우 0.0에서 1.0 사이 여야하며 테스트 분할에 포함 할 데이터 세트의 비율, int 인 경우 테스트 샘플의 절대 수를 나타냄# random_state -&gt; 렌덤함수의 스피드(수치를 바꾸면 렌덤데이터를 추출하는 레코드값이 달라짐)indep_train, indep_test, dep_train, dep_test = train_test_split(indep, dep, test_size=0.40, random_state=47)# lgb.LGBMClassifier -&gt; 모델생성 # max_depth = 7 -&gt; 기본 학습자의 최대 트리 깊이 7# lambda_l1 / lambda_l2 -&gt; 정규화. 과적합을 방지할수 있지만, 정확도를 저하시킬수 있기때문에 일반적으로 디폴트값이 0을 사용# learning_rate = 0.01 -&gt; 학습 스텝의 크기 / n_estimators -&gt; 생성할 트리의 개수# reg_alpha -&gt; 가중치에 대한 L1 정규화 항. 이 값을 늘리면 모델이 더 보수적이 됨# colsample_bytree -&gt; 개별 의사결정나무 모형에 사용될 변수갯수를 지정. 보통 0.5 ~ 1 사용됨. 기본값 1# subsample -&gt; 개별 의사결정나무 모형에 사용되는 임의 표본수를 지정. 보통 0.5 ~ 1 사용됨# n_jobs -&gt; xgboost를 실행하는 데 사용되는 병렬 스레드 수. 그리드 검색과 같은 다른 Scikit-Learn 알고리즘과 함께 사용하는 경우 스레드를 병렬화하고 균형을 조정할 알고리즘을 선택할 수 있습니다. 스레드 경합을 만들면 두 알고리즘이 크게 느려집니다.gs1 = lgb.LGBMClassifier(max_depth = 7, lambda_l1 = 0.1, lambda_l2 = 0.01, learning_rate = 0.01, n_estimators = 500, reg_alpha = 1.1, colsample_bytree = 0.9, subsample = 0.9, n_jobs = 5)# eval_set -&gt; 검증용 세트 지정 / verbose -&gt; 평가단계에서 인쇄여부 1=true 매번인쇄, 100=100회 반복마다 인쇄# eval_metric='accuracy' -&gt; 딥 러닝 모델의 성능을 평가하는 데 사용되는 함수# ('accuracy'=예측이 레이블과 동일한 빈도를 계산) # early_stopping_rounds -&gt; 조기 중단을 위한 라운드를 설정gs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)], eval_metric='accuracy', verbose=False, early_stopping_rounds=50);# res를 기반으로 gs1 데이터셋 예측 수행g = gs1.predict(res)# accuracy_score -&gt; 부분 집합 정확도를 계산a = accuracy_score(dep_test, gs1.predict(indep_test))b = accuracy_score(dep_train, gs1.predict(indep_train))print(a, b) 12345678910111213141516171819202122232425262728### 29# zip -&gt; indep.columns과 gs1의 feature_importances_(변수중요도)를 묶어줌 attr2 = {k: v for k, v in zip(indep.columns, gs1.feature_importances_) if v&gt;0}# 중요도가 낮은순으로 정렬attr2 = sorted(attr2.items(), key=lambda x: x[1], reverse = False)# x1과 y1에 attr2의 컬럼을 각각 분리해서 넣음x1,y1 = zip(*attr2)i1=range(len(x1)) # range(0, 33) 컬럼의 갯수# 새로운 피규어 생성plt.figure(num=None, figsize=(9, 7), dpi=300, facecolor='w', edgecolor='k')# 가로 막대그래프 생성plt.barh(i1, y1)# 그래프 제목설정plt.title(&quot;LGBM&quot;)# y축 눈금값 설정plt.yticks(i1, x1)plt.show(); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768### 30model = [] # model 리스트 변수 생성cvscores = [] # cvscores 리스트 변수 생성for i in range(0,90): # 90회 반복하며 학습함 # train_test_split -&gt; 배열 또는 행렬을 임의 학습 및 테스트 하위 집합으로 분할 # test_size=0.40 -&gt; float 인 경우 0.0에서 1.0 사이 여야하며 테스트 분할에 포함 할 데이터 세트의 비율, int 인 경우 테스트 샘플의 절대 수를 나타냄 # random_state -&gt; 렌덤함수의 스피드(수치를 바꾸면 렌덤데이터를 추출하는 레코드값이 달라짐) indep_train, indep_test, dep_train, dep_test = train_test_split(indep, dep, test_size=0.40, random_state=i) # lgb.LGBMClassifier -&gt; 모델생성 # max_depth = 7 -&gt; 기본 학습자의 최대 트리 깊이 7 # lambda_l1 / lambda_l2 -&gt; 정규화. 과적합을 방지할수 있지만, 정확도를 저하시킬수 있기때문에 일반적으로 디폴트값이 0을 사용 # learning_rate = 0.01 -&gt; 학습 스텝의 크기 # num_iterations -&gt; 나무를 반복하며 부스팅 하는데 몇번을 반복할것인가 # n_estimators -&gt; 생성할 트리의 개수 # reg_alpha -&gt; 가중치에 대한 L1 정규화 항. 이 값을 늘리면 모델이 더 보수적이 됨 # colsample_bytree -&gt; 개별 의사결정나무 모형에 사용될 변수갯수를 지정. 보통 0.5 ~ 1 사용됨. 기본값 1 # subsample -&gt; 개별 의사결정나무 모형에 사용되는 임의 표본수를 지정. 보통 0.5 ~ 1 사용됨 # n_jobs -&gt; LightGBM을 실행하는 데 사용되는 병렬 스레드 수. # boosting='dart' -&gt; 부스팅방법 'dart'는 딥러닝 드랍아웃을 사용하여 정확도를 중요시하는 방식 gs1 = lgb.LGBMClassifier(max_depth = 7, lambda_l1 = 0.1, lambda_l2 = 0.01, learning_rate = 0.01, num_iterations=20000, n_estimators = 5000, reg_alpha = 1.1, colsample_bytree = 0.9, subsample = 0.9, n_jobs = 5, boosting='dart' ) # eval_set -&gt; 검증용 세트 지정 # eval_metric='accuracy' -&gt; 딥 러닝 모델의 성능을 평가하는 데 사용되는 함수 # ('accuracy'=예측이 레이블과 동일한 빈도를 계산) # verbose -&gt; 평가단계에서 인쇄여부 1=true 매번인쇄, 100=100회 반복마다 인쇄 # early_stopping_rounds -&gt; 조기 중단을 위한 라운드를 설정 gs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)], eval_metric='accuracy', verbose=False, early_stopping_rounds=50); # model에 gs1데이터를 추가 model.append(gs1) # cvscores에 accuracy_score -&gt; 부분 집합 정확도를 계산해서 추가 cvscores.append(accuracy_score(dep_test, gs1.predict(indep_test)))data = pd.DataFrame() # data 데이터프레임 생성te = pd.DataFrame() # te 데이터 프레임 생성for i in range(0,90): # res 데이터를 기반으로한 예측 data[i] = model[i].predict(res) # indep_test 데이터를 기반으로한 예측 te[i] = model[i].predict(indep_test)# data의 데이터중 행의 평균값을 구하고 반올림g = np.round(data.mean(axis=1))# te의 데이터중 행의 평균값을 구하고 반올림t = np.round(te.mean(axis=1))# cvscores 데이터의 평균값lgb_mean = np.mean(cvscores)# cvscores 데이터의 표준편차lgb_stdev = np.std(cvscores)print(lgb_mean, lgb_stdev) 1234567### 31result = pd.DataFrame(res_index.astype(np.int), columns=[&quot;PassengerId&quot;])result[&quot;Survived&quot;] = g.astype(np.int)result.to_csv(&quot;lgbm.csv&quot;, &quot;,&quot;, index=None) 123456789101112131415161718192021222324252627### Model 3. CatBoost### 32# cb.CatBoostClassifier -&gt; 모델생성 # depth -&gt; 기본 학습자의 최대 트리 깊이 # reg_lambda -&gt; L2 정규화. 과적합을 방지할수 있지만, 정확도를 저하시킬수 있기때문에 일반적으로 디폴트값이 0을 사용# learning_rate -&gt; 학습 스텝의 크기 # iterations -&gt; 나무를 반복하며 부스팅 하는데 몇번을 반복할것인가gs1 = cb.CatBoostClassifier(depth = 9, reg_lambda=0.1, learning_rate = 0.09, iterations = 500)# eval_set -&gt; 검증용 세트 지정 # verbose -&gt; 평가단계에서 인쇄여부 1=true 매번인쇄, 100=100회 반복마다 인쇄# early_stopping_rounds -&gt; 조기 중단을 위한 라운드를 설정 gs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)], verbose=False, early_stopping_rounds=50);# res를 기반으로 gs1 데이터셋 예측 수행g = gs1.predict(res)# cvscores에 accuracy_score -&gt; 부분 집합 정확도를 계산해서 추가a = accuracy_score(dep_test, gs1.predict(indep_test))b = accuracy_score(dep_train, gs1.predict(indep_train))#cv = cross_val_score(gs1, indep_train, dep_train, cv=5)print(a, b) 12345678910111213141516171819202122232425262728### 33# zip -&gt; indep.columns과 gs1의 feature_importances_(변수중요도)를 묶어줌 attr2 = {k: v for k, v in zip(indep.columns, gs1.feature_importances_) if v&gt;0}# 중요도가 낮은순으로 정렬attr2 = sorted(attr2.items(), key=lambda x: x[1], reverse = False)# x1과 y1에 attr2의 컬럼을 각각 분리해서 넣음x1,y1 = zip(*attr2)i1=range(len(x1)) # range(0, 42) 컬럼의 갯수# 새로운 피규어 생성plt.figure(num=None, figsize=(9, 8), dpi=300, facecolor='w', edgecolor='k')# 가로 막대그래프 생성plt.barh(i1, y1)# 그래프 제목설정plt.title(&quot;CatBoost&quot;)# y축 눈금값 설정plt.yticks(i1, x1)plt.show(); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657### 34 model = [] # model 리스트 변수 생성cvscores = [] # cvscores 리스트 변수 생성for i in range(0, 90): # 90회 반복하며 학습함 # train_test_split -&gt; 배열 또는 행렬을 임의 학습 및 테스트 하위 집합으로 분할 # test_size=0.40 -&gt; float 인 경우 0.0에서 1.0 사이 여야하며 테스트 분할에 포함 할 데이터 세트의 비율, int 인 경우 테스트 샘플의 절대 수를 나타냄 # random_state -&gt; 렌덤함수의 스피드(수치를 바꾸면 렌덤데이터를 추출하는 레코드값이 달라짐) indep_train, indep_test, dep_train, dep_test = train_test_split(indep, dep, test_size=0.40, random_state=i) # cb.CatBoostClassifier -&gt; 모델생성 # depth -&gt; 기본 학습자의 최대 트리 깊이 # reg_lambda -&gt; L2 정규화. 과적합을 방지할수 있지만, 정확도를 저하시킬수 있기때문에 일반적으로 디폴트값이 0을 사용 # learning_rate -&gt; 학습 스텝의 크기 # iterations -&gt; 나무를 반복하며 부스팅 하는데 몇번을 반복할것인가 gs1 = cb.CatBoostClassifier(depth = 9, reg_lambda=0.1, learning_rate = 0.09, iterations = 500) # 데이터학습용 파라미터 입력 # eval_set -&gt; 검증용 세트 지정 / verbose -&gt; 평가단계에서 인쇄여부 1=true 매번인쇄, 100=100회 반복마다 인쇄 # early_stopping_rounds -&gt; 조기 중단을 위한 라운드를 설정 gs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)], verbose=False, early_stopping_rounds=50); model.append(gs1) # gs1 데이터를 model에 삽입 # accuracy_score -&gt; 부분 집합 정확도를 계산 # 정확도 계산값을 cvscores에 삽입 cvscores.append(accuracy_score(dep_test, gs1.predict(indep_test))) data = pd.DataFrame() # data 데이타 프레임생성te = pd.DataFrame() # te 데이타 프레임생성for i in range(0, 90): # res 데이터를 기반으로한 예측 data[i] = model[i].predict(res) # indep_test 데이터를 기반으로한 예측 te[i] = model[i].predict(indep_test)# data의 데이터중 행의 평균값을 구하고 반올림g = np.round(data.mean(axis=1))# data의 데이터중 행의 평균값을 구하고 반올림t = np.round(te.mean(axis=1))# cvscores 데이터의 평균값cb_mean = np.mean(cvscores)# cvscores 데이터의 표준편차cb_stdev = np.std(cvscores)print(cb_mean, cb_stdev) 12345678### 35#result = pd.DataFrame(res_index.astype(np.int), columns=[&quot;PassengerId&quot;])#result[&quot;Survived&quot;] = g.astype(np.int)#result.to_csv(r&quot;c:\\work\\dataset\\titanic\\catboost.csv&quot;, &quot;,&quot;, index=None)#result.to_csv(&quot;catboost.csv&quot;, &quot;,&quot;, index=None) 1234567891011121314151617181920212223242526272829### Model 4. XGBoost### 36# xgb.XGBClassifier -&gt; 모델생성 # max_depth = 9 -&gt; 기본 학습자의 최대 트리 깊이 9# learning_rate = 0.01 -&gt; 학습 스텝의 크기 / n_estimators -&gt; 생성할 트리의 개수# reg_alpha -&gt; 가중치에 대한 L1 정규화 항. 이 값을 늘리면 모델이 더 보수적이 됨# colsample_bytree -&gt; 개별 의사결정나무 모형에 사용될 변수갯수를 지정. 보통 0.5 ~ 1 사용됨. 기본값 1# subsample -&gt; 개별 의사결정나무 모형에 사용되는 임의 표본수를 지정. 보통 0.5 ~ 1 사용됨# n_jobs -&gt; xgboost를 실행하는 데 사용되는 병렬 스레드 수. 그리드 검색과 같은 다른 Scikit-Learn 알고리즘과 함께 사용하는 경우 스레드를 병렬화하고 균형을 조정할 알고리즘을 선택할 수 있습니다. 스레드 경합을 만들면 두 알고리즘이 크게 느려집니다.gs1 = xgb.XGBClassifier(max_depth = 9, learning_rate = 0.01, n_estimators = 500, reg_alpha = 1.1, colsample_bytree = 0.9, subsample = 0.9, n_jobs = 5)# 데이터학습용 파라미터 입력# eval_set -&gt; 검증용 세트 지정 / verbose -&gt; 평가단계에서 인쇄여부 1=true 매번인쇄, 100=100회 반복마다 인쇄# early_stopping_rounds -&gt; 조기 중단을 위한 라운드를 설정gs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)], verbose=False, early_stopping_rounds=50);# res를 기반으로 gs1 데이터셋 예측 수행g = gs1.predict(res)# accuracy_score -&gt; 부분 집합 정확도를 계산a = accuracy_score(dep_test, gs1.predict(indep_test))b = accuracy_score(dep_train, gs1.predict(indep_train))print(a, b) 12345678910111213141516171819202122232425262728### 37# zip -&gt; indep.columns과 gs1의 feature_importances_(변수중요도)를 묶어줌 attr2 = {k: v for k, v in zip(indep.columns, gs1.feature_importances_) if v&gt;0}# 중요도가 낮은순으로 정렬attr2 = sorted(attr2.items(), key=lambda x: x[1], reverse = False)# x1과 y1에 attr2의 컬럼을 각각 분리해서 넣음x1,y1 = zip(*attr2)i1=range(len(x1)) # range(0, 35) 컬럼의 갯수# 새로운 피규어 생성plt.figure(num=None, figsize=(9, 7), dpi=300, facecolor='w', edgecolor='k')# 가로 막대그래프 생성plt.barh(i1, y1)# 그래프 제목설정plt.title(&quot;XGBoost&quot;)# y축 눈금값 설정plt.yticks(i1, x1)plt.show(); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061### 38model = [] # model 리스트 변수 생성cvscores = [] # cvscores 리스트 변수 생성for i in range(0, 90): # 90회 반복하며 학습함 # train_test_split -&gt; 배열 또는 행렬을 임의 학습 및 테스트 하위 집합으로 분할 # test_size=0.40 -&gt; float 인 경우 0.0에서 1.0 사이 여야하며 테스트 분할에 포함 할 데이터 세트의 비율, int 인 경우 테스트 샘플의 절대 수를 나타냄 # random_state -&gt; 렌덤함수의 스피드(수치를 바꾸면 렌덤데이터를 추출하는 레코드값이 달라짐) indep_train, indep_test, dep_train, dep_test = train_test_split(indep, dep, test_size=0.40, random_state=i) # xgb.XGBClassifier -&gt; 모델생성 # max_depth -&gt; 기본 학습자의 최대 트리 깊이 / reg_lambda -&gt; 가중치에 대한 L2 정규화 항. 이 값을 늘리면 모델이 더 보수적이 됨 # learning_rate -&gt; 학습 스텝의 크기 / n_estimators -&gt; 생성할 트리의 개수 # reg_alpha -&gt; 가중치에 대한 L1 정규화 항. 이 값을 늘리면 모델이 더 보수적이 됨 # colsample_bytree -&gt; 개별 의사결정나무 모형에 사용될 변수갯수를 지정. 보통 0.5 ~ 1 사용됨. 기본값 1 # subsample -&gt; 개별 의사결정나무 모형에 사용되는 임의 표본수를 지정. 보통 0.5 ~ 1 사용됨 # n_jobs -&gt; xgboost를 실행하는 데 사용되는 병렬 스레드 수. 그리드 검색과 같은 다른 Scikit-Learn 알고리즘과 함께 사용하는 경우 스레드를 병렬화하고 균형을 조정할 알고리즘을 선택할 수 있습니다. 스레드 경합을 만들면 두 알고리즘이 크게 느려집니다. gs1 = xgb.XGBClassifier(max_depth = 7, reg_lambda = 0.02, learning_rate = 0.01, n_estimators = 5000, reg_alpha = 1.1, colsample_bytree = 0.9, subsample = 0.9, n_jobs = 5) # 데이터학습용 파라미터 입력 # eval_set -&gt; 검증용 세트 지정 / verbose -&gt; 평가단계에서 인쇄여부 1=true 매번인쇄, 100=100회 반복마다 인쇄 # early_stopping_rounds -&gt; 조기 중단을 위한 라운드를 설정 gs1.fit(indep_train, dep_train, eval_set=[(indep_test, dep_test)], verbose=False, early_stopping_rounds=50); model.append(gs1) # gs1 데이터를 model에 삽입 # accuracy_score -&gt; 부분 집합 정확도를 계산 # 정확도 계산값을 cvscores에 삽입 cvscores.append(accuracy_score(dep_test, gs1.predict(indep_test)))data = pd.DataFrame() # data 데이타 프레임생성te = pd.DataFrame() # te 데이타 프레임생성for i in range(0, 90): # res 데이터를 기반으로한 예측 data[i] = model[i].predict(res) # indep_test 데이터를 기반으로한 예측 te[i] = model[i].predict(indep_test)# data의 데이터중 행의 평균값을 구하고 반올림g = np.round(data.mean(axis=1))# te의 데이터중 행의 평균값을 구하고 반올림t = np.round(te.mean(axis=1))# cvscores 데이터의 평균값xgb_mean = np.mean(cvscores)# cvscores 데이터의 표준편차xgb_stdev = np.std(cvscores)print(xgb_mean, xgb_stdev) 12345678### 39#result = pd.DataFrame(res_index.astype(np.int), columns=[&quot;PassengerId&quot;])#result[&quot;Survived&quot;] = g.astype(np.int)#result.to_csv(r&quot;c:\\work\\dataset\\titanic\\xgb.csv&quot;, &quot;,&quot;, index=None)#result.to_csv(&quot;lgbm.csv&quot;, &quot;,&quot;, index=None) 12345678910### 40d = {'Model':[&quot;Keras MLP&quot;, &quot;LightGBM&quot;, &quot;CatBoost&quot;, &quot;XGBoost&quot;], 'Mean accuracy': [mlp_mean, lgb_mean, cb_mean, xgb_mean], 'Std. Dev.': [mlp_stdev, lgb_stdev, cb_stdev, xgb_stdev], 'Leaderboard': [0.77033, 0.82296, 0.78947, 0.77511]}pd.DataFrame(data=d, columns=[&quot;Model&quot;, &quot;Mean accuracy&quot;, &quot;Std. Dev.&quot;, &quot;Leaderboard&quot;]).sort_values(&quot;Mean accuracy&quot;, ascending=False).head(10)","link":"/2021/07/09/project_md/%EC%BD%94%EB%93%9C/P001_%EC%BC%80%EA%B8%80_%ED%83%80%EC%9D%B4%ED%83%80%EB%8B%89_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_%EC%BD%94%EB%93%9C%ED%95%B4%EC%84%9D/"},{"title":"코로나전국현황_구글_크롤링_오라클연동_코드","text":"사용 버전 - Python 3.9.5 64-bit / 오라클 SQL 21.2.0.187 빌드 187.1842 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101from numpy.lib.function_base import appendimport requestsfrom bs4 import BeautifulSoupimport pandas as pdimport cx_Oraclefrom sqlalchemy import create_enginedef 코로나전세계_구글(): url = 'https://www.google.com/search?q=%EC%BD%94%EB%A1%9C%EB%82%98+%EB%B0%94%EC%9D%B4%EB%9F%AC%EC%8A%A4+%ED%86%B5%EA%B3%84&amp;sxsrf=ALeKk039rbc7RrUiD3uqu1MtY-ai2yNKSQ%3A1627884520659&amp;source=hp&amp;ei=6IsHYaHaJYbS-Qa11bjABg&amp;iflsig=AINFCbYAAAAAYQeZ-I1ac-vheRVEnaDHYl1UC3E_0GKU&amp;oq=%EC%BD%94%EB%A1%9C%EB%82%98+%EB%B0%94%EC%9D%B4%EB%9F%AC%EC%8A%A4+%ED%86%B5%EA%B3%84&amp;gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgUIABCABDIGCAAQBRAeMgYIABAFEB4yBggAEAUQHjIGCAAQBRAeMgYIABAFEB4yBggAEAUQHjIGCAAQBRAeMgYIABAFEB46BwgjEOoCECdQ0glY0glgohJoAXAAeACAAXGIAXGSAQMwLjGYAQCgAQKgAQGwAQo&amp;sclient=gws-wiz&amp;ved=0ahUKEwih7ti01pHyAhUGad4KHbUqDmgQ4dUDCAc&amp;uact=5' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} res = requests.get(url, headers=headers) res.raise_for_status() soup = BeautifulSoup(res.text, 'lxml') # 전세계 합계 데이터 추출 세계통계전체 = soup.find('div', attrs={'class':'o6Yscf iB3eO'}) 세계통계확진자 = 세계통계전체.find_all('td', attrs={'class':'dZdtsb Pmvw7b QmWbpe ZDeom'}) 세계통계확진자수0 = [] 전세계사망자수0 = [] for i in 세계통계확진자: a = i.get_text() if a.startswith('확진자'): b = a.replace(',','') # 숫자형으로 변환하기위해 ,제거 세계통계확진자수0.append(b) elif a.startswith('사망자'): b = a.replace(',','') 전세계사망자수0.append(b) # print(a) 세계통계확진자수1 = &quot;&quot;.join(세계통계확진자수0) # 리스트를 문자열로 변환 전세계확진자수 = 세계통계확진자수1[8:19] # 필요한 부분추출 전세계사망자수1 = &quot;&quot;.join(전세계사망자수0) 전세계사망자수 = 전세계사망자수1[7:16] # 전세계코로나 합산수치 전세계코로나현황 = pd.DataFrame({'국가':'전체', '확진자' : 전세계확진자수, '사망자' : 전세계사망자수}, index=[0]) 국가명 = [] 확진자수 = [] 사망자수 = [] 전체 = soup.find_all('tr', attrs={'class':'viwUIc'}) 국가명 = [] # print(전체) for index, i in enumerate(전체): if index == 0: pass elif 0 &lt; index &lt; 200: 국가명1 = i.find('div', attrs={'class':'OrdL9b'}) 확진자1 = i.find('div', attrs={'class':'ruktOc'}) for index, ii in enumerate(i): if index == 3 : 사망자1 = ii.find('div', attrs={'class':'ruktOc'}) 사망자2 = 사망자1.find('span').get_text() 사망자3 = 사망자2.replace(',', '') # 숫자형으로 변환하기위해 ,제거 사망자수.append(사망자3) 국가명2 = 국가명1.find('span').get_text() 확진자2 = 확진자1.find('span').get_text() 확진자3 = 확진자2.replace(',', '') 국가명.append(국가명2) 확진자수.append(확진자3) 국가명2 = pd.DataFrame(국가명) 확진자수2 = pd.DataFrame(확진자수) 사망자수2 = pd.DataFrame(사망자수) 전세계코로나현황1 = pd.concat([국가명2,확진자수2, 사망자수2], axis=1) 전세계코로나현황1.columns = ['국가', '확진자', '사망자'] 전세계코로나현황2 = pd.concat([전세계코로나현황, 전세계코로나현황1]) # 전세계 합계와 국가별 데이터 합치기 전세계코로나현황2.sort_values(by=['국가'], inplace=True) # 이름순으로 정렬 전세계코로나현황3 = 전세계코로나현황2.reset_index(drop=True) # 인덱스 리셋 좌표파일 = './source/xlsx/P008_세계코로나현황_구글_좌표.xlsx' 좌표 = pd.read_excel(좌표파일) 세계코로나현황 = pd.concat([전세계코로나현황3, 좌표], axis=1) 세계코로나현황['확진자'] = pd.to_numeric(세계코로나현황['확진자'], errors='ignore') 세계코로나현황['사망자'] = pd.to_numeric(세계코로나현황['사망자'], errors='ignore') 세계코로나현황['사망률'] = round(((세계코로나현황['사망자'] / 세계코로나현황['확진자']) * 100 ),3) # 사망률 추가하기 세계코로나현황['위도'] = 세계코로나현황['위도'].round(3) 세계코로나현황['경도'] = 세계코로나현황['경도'].round(3) 세계코로나현황.to_excel('./source/xlsx/P008_세계코로나현황_구글.xlsx') return 세계코로나현황 if __name__ == '__main__': 코로나전세계_구글()세계코로나현황 = 코로나전세계_구글()# 데이터 프레임을 오라클 데이터베이스에 넣기# PROTOCOL=TCP / HOST = 서버주소 / PORT = 서버포트 / SERVICE_NAME = 데이터베이스이름dsn_tns = &quot;(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=localhost)(PORT=1521))\\ (CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=orcl)))&quot;pwd = 'tiger' # 사용자 비밀번호 / 아래 c##scott은 사용자명engine = create_engine('oracle+cx_oracle://c##scott:' + pwd + '@%s' % dsn_tns)세계코로나현황.to_sql('P008_세계코로나현황_구글', engine.connect(), if_exists='replace', index=False)","link":"/2021/08/04/project_md/%EC%BD%94%EB%93%9C/P008_%EC%BD%94%EB%A1%9C%EB%82%98%EC%A0%84%EC%84%B8%EA%B3%84%ED%98%84%ED%99%A9_%EA%B5%AC%EA%B8%80_%ED%81%AC%EB%A1%A4%EB%A7%81_%EC%BD%94%EB%93%9C/"},{"title":"코로나전세계현황_네이버_크롤링_오라클연동_코드","text":"사용 버전 - Python 3.9.5 64-bit / 오라클 SQL 21.2.0.187 빌드 187.1842 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107from numpy.lib.function_base import appendfrom bs4 import BeautifulSoupimport pandas as pdimport cx_Oraclefrom sqlalchemy import create_enginefrom selenium import webdriverimport timeimport redef 코로나전세계_네이버(): 국가 = [] 누적확진자 = [] 신규확진자 = [] 사망자 = [] 사망율 = [] driver = webdriver.Chrome('./chromedriver') driver.get('https://search.naver.com/search.naver?where=nexearch&amp;sm=top_hty&amp;fbm=1&amp;ie=utf8&amp;query=%EC%BD%94%EB%A1%9C%EB%82%98%ED%98%84%ED%99%A9') time.sleep(2) driver.find_element_by_xpath('//*[@id=&quot;_cs_production_type&quot;]/div/div[2]/div/div/ul/li[3]/a/span').click() for i in range(0, 27): # 28까지 driver.find_element_by_xpath('//*[@id=&quot;_cs_production_type&quot;]/div/div[5]/div/div[3]/div[4]/div/div/a[2]').click() html = driver.page_source soup = BeautifulSoup(html, 'html.parser') # 신규확진자 for i in range(1, 9): 국가1 = soup.select('#_cs_production_type &gt; div &gt; div:nth-child(5) &gt; div &gt; div:nth-child(3) &gt; div.csp_table_area.overseas_table &gt; div &gt; table &gt; tbody &gt; tr:nth-child(%d) &gt; td.align_left &gt; span' %(i)) 누적확진자1 = soup.select('#_cs_production_type &gt; div &gt; div:nth-child(5) &gt; div &gt; div:nth-child(3) &gt; div.csp_table_area.overseas_table &gt; div &gt; table &gt; tbody &gt; tr:nth-child(%d) &gt; td:nth-child(2) &gt; span' %(i)) 신규확진자1 = soup.select('#_cs_production_type &gt; div &gt; div:nth-child(5) &gt; div &gt; div:nth-child(3) &gt; div.csp_table_area.overseas_table &gt; div &gt; table &gt; tbody &gt; tr:nth-child(%d) &gt; td:nth-child(3) &gt; span' %(i)) 사망자1 = soup.select('#_cs_production_type &gt; div &gt; div:nth-child(5) &gt; div &gt; div:nth-child(3) &gt; div.csp_table_area.overseas_table &gt; div &gt; table &gt; tbody &gt; tr:nth-child(%d) &gt; td.align_center &gt; span' %(i)) for ii in 국가1: aa = str(ii) aa1 = re.search('&quot;&gt;(.*)&lt;/', aa).group(1) # 문자열 추출 국가.append(aa1) for ii in 누적확진자1: aa = str(ii) aa1 = re.search('&quot;&gt;(.*)&lt;/', aa).group(1) 제거할문자 = ',' # 제거할문자목록 aa1 = ''.join( x for x in aa1 if x not in 제거할문자) # 불필요한 문자제거 aa2 = int(aa1) # 데이터 타입 변환 누적확진자.append(aa2) for ii in 신규확진자1: aa = str(ii) aa1 = re.search('&quot;&gt;(.*)&lt;/', aa).group(1) if aa1 == '-': aa1 = 0 # 정수 aa1 = str(aa1) # 정수가 나오면 밑에 join에서 에러가 나기때문에 문자로 변환 제거할문자 = ',' aa1 = ''.join( x for x in aa1 if x not in 제거할문자) aa2 = int(aa1) 신규확진자.append(aa2) for ii in 사망자1: aa = str(ii) aa1 = re.search('&quot;&gt;(.*)&lt;/', aa).group(1) aa2 = re.search('(.*)&lt;sp', aa1).group(1) aa3 = re.search('&gt;(.*)&lt;', aa1).group(1) 제거할문자 = '(),%' aa2 = ''.join( x for x in aa2 if x not in 제거할문자) aa3 = ''.join( x for x in aa3 if x not in 제거할문자) aa4 = float(aa2) # 사망율 aa5 = int(aa3) # 사망자 사망율.append(aa4) 사망자.append(aa5) 국가 = pd.DataFrame(국가) 누적확진자 = pd.DataFrame(누적확진자) 신규확진자 = pd.DataFrame(신규확진자) 사망자 = pd.DataFrame(사망자) 사망율 = pd.DataFrame(사망율) 좌표파일 = './source/xlsx/P009_세계코로나현황_네이버_좌표.xlsx' 좌표 = pd.read_excel(좌표파일) 좌표['위도'] = 좌표['위도'].round(3) 좌표['경도'] = 좌표['경도'].round(3) # print(좌표) 세계코로나현황 = pd.concat([국가, 누적확진자, 신규확진자, 사망자, 사망율], axis=1) 세계코로나현황.columns = ['국가', '누적확진자', '신규확진자', '사망자', '사망율'] 세계코로나현황.sort_values(by=['국가'], inplace=True) # 이름순으로 정렬 세계코로나현황.reset_index(drop=True, inplace=True) # 인덱스 리셋 세계코로나현황 = 세계코로나현황.drop(152).reset_index(drop=True) # 152행 일본크루즈 삭제 세계코로나현황 = 세계코로나현황.drop(206).reset_index(drop=True) # 206번행 레위니옹 삭제 세계코로나현황1 = pd.concat([세계코로나현황, 좌표], axis=1) 세계코로나현황1.to_excel('./source/xlsx/P009_세계코로나현황_네이버.xlsx') # 데이터 프레임을 오라클 데이터베이스에 넣기 # PROTOCOL=TCP / HOST = 서버주소 / PORT = 서버포트 / SERVICE_NAME = 데이터베이스이름 dsn_tns = &quot;(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=localhost)(PORT=1521))\\ (CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=orcl)))&quot; pwd = 'tiger' # 사용자 비밀번호 / 아래 c##scott은 사용자명 engine = create_engine('oracle+cx_oracle://c##scott:' + pwd + '@%s' % dsn_tns) 세계코로나현황1.to_sql('P009_세계코로나현황_네이버', engine.connect(), if_exists='replace', index=False)if __name__ == '__main__': 코로나전세계_네이버()","link":"/2021/08/06/project_md/%EC%BD%94%EB%93%9C/P009_%EC%BD%94%EB%A1%9C%EB%82%98%EC%A0%84%EC%84%B8%EA%B3%84%ED%98%84%ED%99%A9_%EB%84%A4%EC%9D%B4%EB%B2%84_%ED%81%AC%EB%A1%A4%EB%A7%81_%EC%BD%94%EB%93%9C/"},{"title":"코로나전세계현황_네이버_크롤링_오라클연동 PPT &#x2F; PDF 링크","text":"클릭 –&gt; 코로나 전국현황 크롤링 PPT 클릭 –&gt; 코로나 전국현황 크롤링 PDF","link":"/2021/08/12/project_md/PDF/P006_%EC%BD%94%EB%A1%9C%EB%82%98%EC%A0%84%EC%84%B8%EA%B3%84%ED%98%84%ED%99%A9_%EB%84%A4%EC%9D%B4%EB%B2%84_%ED%81%AC%EB%A1%A4%EB%A7%81_%EC%98%A4%EB%9D%BC%ED%81%B4%EC%97%B0%EB%8F%99_PDF/"},{"title":"T031_국내이동통신사_주식데이터비교","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629792551387'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T031_%EA%B5%AD%EB%82%B4%EC%9D%B4%EB%8F%99%ED%86%B5%EC%8B%A0%EC%82%AC_%EC%A3%BC%EC%8B%9D%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B9%84%EA%B5%90/"},{"title":"T032_수익과할인율_분산형차트","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629792584827'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T032_%EC%88%98%EC%9D%B5%EA%B3%BC%ED%95%A0%EC%9D%B8%EC%9C%A8_%EB%B6%84%EC%82%B0%ED%98%95%EC%B0%A8%ED%8A%B8/"},{"title":"T033_매개변수를활용한_매출단위변경","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629792611226'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T033_%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98%EB%A5%BC%ED%99%9C%EC%9A%A9%ED%95%9C_%EB%A7%A4%EC%B6%9C%EB%8B%A8%EC%9C%84%EB%B3%80%EA%B2%BD/"},{"title":"T035_대시보드액션_매개변수작업","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629792666213'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='750px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='750px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='727px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T035_%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C%EC%95%A1%EC%85%98_%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98%EC%9E%91%EC%97%85/"},{"title":"T034_다양한_날짜필터","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629792635651'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T034_%EB%8B%A4%EC%96%91%ED%95%9C_%EB%82%A0%EC%A7%9C%ED%95%84%ED%84%B0/"},{"title":"T036_캔들차트활용_주식차트만들기","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629878367981'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/25/project_md/Tableau/T036_%EC%BA%94%EB%93%A4%EC%B0%A8%ED%8A%B8%ED%99%9C%EC%9A%A9_%EC%A3%BC%EC%8B%9D%EC%B0%A8%ED%8A%B8%EB%A7%8C%EB%93%A4%EA%B8%B0/"},{"title":"T037_지역별매출_상위N명구하기","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629878396831'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/25/project_md/Tableau/T037_%EC%A7%80%EC%97%AD%EB%B3%84%EB%A7%A4%EC%B6%9C_%EC%83%81%EC%9C%84N%EB%AA%85%EA%B5%AC%ED%95%98%EA%B8%B0/"},{"title":"T039__다른시트끼리_조인연결","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629878482497'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/25/project_md/Tableau/T039__%EB%8B%A4%EB%A5%B8%EC%8B%9C%ED%8A%B8%EB%81%BC%EB%A6%AC_%EC%A1%B0%EC%9D%B8%EC%97%B0%EA%B2%B0/"},{"title":"T038_원하는레이블만_표시하는방법","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629878455985'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='1016px';vizElement.style.height='991px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/25/project_md/Tableau/T038_%EC%9B%90%ED%95%98%EB%8A%94%EB%A0%88%EC%9D%B4%EB%B8%94%EB%A7%8C_%ED%91%9C%EC%8B%9C%ED%95%98%EB%8A%94%EB%B0%A9%EB%B2%95/"},{"title":"T040_목표대비_매출비율구하기","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629878513345'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/25/project_md/Tableau/T040_%EB%AA%A9%ED%91%9C%EB%8C%80%EB%B9%84_%EB%A7%A4%EC%B6%9C%EB%B9%84%EC%9C%A8%EA%B5%AC%ED%95%98%EA%B8%B0/"},{"title":"T041_데이터설명","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629878534597'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/25/project_md/Tableau/T041_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%84%A4%EB%AA%85/"},{"title":"T002_날씨_크롤링_스토리보드","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1627020477868'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='1016px';vizElement.style.height='991px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/07/23/project_md/Tableau/T001_T030/T002_%EB%82%A0%EC%94%A8_%ED%81%AC%EB%A1%A4%EB%A7%81_%EC%8A%A4%ED%86%A0%EB%A6%AC%EB%B3%B4%EB%93%9C/"},{"title":"T003_코로나전국현황_크롤링_스토리보드","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1627619173331'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='1016px';vizElement.style.height='991px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/07/30/project_md/Tableau/T001_T030/T003_%EC%BD%94%EB%A1%9C%EB%82%98%EC%A0%84%EA%B5%AD%ED%98%84%ED%99%A9_%ED%81%AC%EB%A1%A4%EB%A7%81_%EC%8A%A4%ED%86%A0%EB%A6%AC%EB%B3%B4%EB%93%9C/"},{"title":"T004_코로나전세계현황_네이버_크롤링_오라클연동","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1628740011461'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='1016px';vizElement.style.height='991px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/12/project_md/Tableau/T001_T030/T004_%EC%BD%94%EB%A1%9C%EB%82%98%EC%A0%84%EC%84%B8%EA%B3%84%ED%98%84%ED%99%A9_%EB%84%A4%EC%9D%B4%EB%B2%84_%ED%81%AC%EB%A1%A4%EB%A7%81_%EC%98%A4%EB%9D%BC%ED%81%B4%EC%97%B0%EB%8F%99/"},{"title":"T006_지역별인구_3단계지도","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1628756541121'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='727px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/17/project_md/Tableau/T001_T030/T006_%EC%A7%80%EC%97%AD%EB%B3%84%EC%9D%B8%EA%B5%AC_3%EB%8B%A8%EA%B3%84%EC%A7%80%EB%8F%84/"},{"title":"T007_그룹대그룹_비교차트","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1628756628339'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='727px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/17/project_md/Tableau/T001_T030/T007_%EA%B7%B8%EB%A3%B9%EB%8C%80%EA%B7%B8%EB%A3%B9_%EB%B9%84%EA%B5%90%EC%B0%A8%ED%8A%B8/"},{"title":"T001_케글_Tabular_Playground_202107_스토리보드","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1626407398965'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='1016px';vizElement.style.height='991px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/07/14/project_md/Tableau/T001_T030/T001_%EC%BC%80%EA%B8%80_Tabular_Playground_202107_%EC%8A%A4%ED%86%A0%EB%A6%AC%EB%B3%B4%EB%93%9C/"},{"title":"T005_달력차트","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1628756448188'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='777px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/17/project_md/Tableau/T001_T030/T005_%EB%8B%AC%EB%A0%A5%EC%B0%A8%ED%8A%B8/"},{"title":"T008_7월의서울날씨_1990_2019","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1628756777997'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/17/project_md/Tableau/T001_T030/T008_7%EC%9B%94%EC%9D%98%EC%84%9C%EC%9A%B8%EB%82%A0%EC%94%A8_1990_2019/"},{"title":"T009_매개변수에따른_하이라이팅","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1628756853412'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/17/project_md/Tableau/T001_T030/T009_%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98%EC%97%90%EB%94%B0%EB%A5%B8_%ED%95%98%EC%9D%B4%EB%9D%BC%EC%9D%B4%ED%8C%85/"},{"title":"T010_제품중_분류별_매출이_가장많이_나온달과_적은달 copy 2","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629444345242'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/20/project_md/Tableau/T001_T030/T010_%EC%A0%9C%ED%92%88%EC%A4%91_%EB%B6%84%EB%A5%98%EB%B3%84_%EB%A7%A4%EC%B6%9C%EC%9D%B4_%EA%B0%80%EC%9E%A5%EB%A7%8E%EC%9D%B4_%EB%82%98%EC%98%A8%EB%8B%AC%EA%B3%BC_%EC%A0%81%EC%9D%80%EB%8B%AC%20copy%202/"},{"title":"T012_DATEDIFF_주문번호별_배송기간","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629444585268'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/20/project_md/Tableau/T001_T030/T012_DATEDIFF_%EC%A3%BC%EB%AC%B8%EB%B2%88%ED%98%B8%EB%B3%84_%EB%B0%B0%EC%86%A1%EA%B8%B0%EA%B0%84/"},{"title":"T014_매출기준_Top11_20위찾기","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629445252394'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/20/project_md/Tableau/T001_T030/T014_%EB%A7%A4%EC%B6%9C%EA%B8%B0%EC%A4%80_Top11_20%EC%9C%84%EC%B0%BE%EA%B8%B0%20copy%202/"},{"title":"T013_도구설명포함_맵차트","text":"클릭 -&gt; 비주얼리제이션 링크 비주얼리제이션 var divElement = document.getElementById('viz1629444646739'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/20/project_md/Tableau/T001_T030/T013_%EB%8F%84%EA%B5%AC%EC%84%A4%EB%AA%85%ED%8F%AC%ED%95%A8_%EB%A7%B5%EC%B0%A8%ED%8A%B8/"},{"title":"T016_도넛차트만들기","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629703302958'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/23/project_md/Tableau/T001_T030/T016_%EB%8F%84%EB%84%9B%EC%B0%A8%ED%8A%B8%EB%A7%8C%EB%93%A4%EA%B8%B0/"},{"title":"T015_대시보드에_URL링크걸기","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629703278110'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='450px';vizElement.style.width='100%';vizElement.style.height='387px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='450px';vizElement.style.width='100%';vizElement.style.height='387px';} else { vizElement.style.width='100%';vizElement.style.height='727px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/23/project_md/Tableau/T001_T030/T015_%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C%EC%97%90_URL%EB%A7%81%ED%81%AC%EA%B1%B8%EA%B8%B0/"},{"title":"T011_퀵테이블계산_순위와_구성비율","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629444532202'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/20/project_md/Tableau/T001_T030/T011_%ED%80%B5%ED%85%8C%EC%9D%B4%EB%B8%94%EA%B3%84%EC%82%B0_%EC%88%9C%EC%9C%84%EC%99%80_%EA%B5%AC%EC%84%B1%EB%B9%84%EC%9C%A8/"},{"title":"T019_매개변수_구_별_수익률_기준_컬러","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629703448399'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/23/project_md/Tableau/T001_T030/T019_%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98_%EA%B5%AC_%EB%B3%84_%EC%88%98%EC%9D%B5%EB%A5%A0_%EA%B8%B0%EC%A4%80_%EC%BB%AC%EB%9F%AC/"},{"title":"T017_이중축_고객별_연매출","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629703326472'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/23/project_md/Tableau/T001_T030/T017_%EC%9D%B4%EC%A4%91%EC%B6%95_%EA%B3%A0%EA%B0%9D%EB%B3%84_%EC%97%B0%EB%A7%A4%EC%B6%9C/"},{"title":"T018_퀵테이블계산_전월_전년대비_성장율","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629703395564'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/23/project_md/Tableau/T001_T030/T018_%ED%80%B5%ED%85%8C%EC%9D%B4%EB%B8%94%EA%B3%84%EC%82%B0_%EC%A0%84%EC%9B%94_%EC%A0%84%EB%85%84%EB%8C%80%EB%B9%84_%EC%84%B1%EC%9E%A5%EC%9C%A8/"},{"title":"T020_수익구간_차원만들기","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629703483622'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/23/project_md/Tableau/T001_T030/T020_%EC%88%98%EC%9D%B5%EA%B5%AC%EA%B0%84_%EC%B0%A8%EC%9B%90%EB%A7%8C%EB%93%A4%EA%B8%B0/"},{"title":"T021_복합매개변수_이전_N일동안의추이","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629703595934'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/23/project_md/Tableau/T001_T030/T021_%EB%B3%B5%ED%95%A9%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98_%EC%9D%B4%EC%A0%84_N%EC%9D%BC%EB%8F%99%EC%95%88%EC%9D%98%EC%B6%94%EC%9D%B4/"},{"title":"T023_순위맵차트만들기","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629703648607'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/23/project_md/Tableau/T001_T030/T023_%EC%88%9C%EC%9C%84%EB%A7%B5%EC%B0%A8%ED%8A%B8%EB%A7%8C%EB%93%A4%EA%B8%B0/"},{"title":"T024_측정값없이_리스트만들기","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629781715781'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='850px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='850px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='2227px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T001_T030/T024_%EC%B8%A1%EC%A0%95%EA%B0%92%EC%97%86%EC%9D%B4_%EB%A6%AC%EC%8A%A4%ED%8A%B8%EB%A7%8C%EB%93%A4%EA%B8%B0/"},{"title":"T022_누적막대차트_이중축활용","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629703618430'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/23/project_md/Tableau/T001_T030/T022_%EB%88%84%EC%A0%81%EB%A7%89%EB%8C%80%EC%B0%A8%ED%8A%B8_%EC%9D%B4%EC%A4%91%EC%B6%95%ED%99%9C%EC%9A%A9/"},{"title":"T028_rounded_bar_chart","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629781860632'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T001_T030/T028_rounded_bar_chart/"},{"title":"T029_WINDOW_AVERAGE를_기준으로_색상구분","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629781887376'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T001_T030/T029_WINDOW_AVERAGE%EB%A5%BC_%EA%B8%B0%EC%A4%80%EC%9C%BC%EB%A1%9C_%EC%83%89%EC%83%81%EA%B5%AC%EB%B6%84/"},{"title":"T025_Null값을_0으로변경처리","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629781761113'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='850px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='850px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='727px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T001_T030/T025_Null%EA%B0%92%EC%9D%84_0%EC%9C%BC%EB%A1%9C%EB%B3%80%EA%B2%BD%EC%B2%98%EB%A6%AC/"},{"title":"T030_원하는_결과값을_정확하게_Late_filtering","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629781913667'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='727px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T001_T030/T030_%EC%9B%90%ED%95%98%EB%8A%94_%EA%B2%B0%EA%B3%BC%EA%B0%92%EC%9D%84_%EC%A0%95%ED%99%95%ED%95%98%EA%B2%8C_Late_filtering/"},{"title":"T026_연속형과_불연속형","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629781802569'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='1227px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T001_T030/T026_%EC%97%B0%EC%86%8D%ED%98%95%EA%B3%BC_%EB%B6%88%EC%97%B0%EC%86%8D%ED%98%95/"},{"title":"T027_누적영역차트","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629781831255'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='650px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='1227px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/24/project_md/Tableau/T001_T030/T027_%EB%88%84%EC%A0%81%EC%98%81%EC%97%AD%EC%B0%A8%ED%8A%B8/"},{"title":"T043_대시보드레이아웃_필터","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629966369341'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='750px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='750px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='1027px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/26/project_md/Tableau/T043_%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C%EB%A0%88%EC%9D%B4%EC%95%84%EC%9B%83_%ED%95%84%ED%84%B0/"},{"title":"T042_구글스프레드시트_데이터연결","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1629966196883'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/26/project_md/Tableau/T042_%EA%B5%AC%EA%B8%80%EC%8A%A4%ED%94%84%EB%A0%88%EB%93%9C%EC%8B%9C%ED%8A%B8_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%B0%EA%B2%B0/"},{"title":"T044_맵위에_밀도마크적용","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1630044100257'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/26/project_md/Tableau/T044_%EB%A7%B5%EC%9C%84%EC%97%90_%EB%B0%80%EB%8F%84%EB%A7%88%ED%81%AC%EC%A0%81%EC%9A%A9/"},{"title":"T045_DATEPART함수활용_계절데이터","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1630044127375'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/26/project_md/Tableau/T045_DATEPART%ED%95%A8%EC%88%98%ED%99%9C%EC%9A%A9_%EA%B3%84%EC%A0%88%EB%8D%B0%EC%9D%B4%ED%84%B0/"},{"title":"T046_당월_전월_매출비교_대시보드","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1630044154134'); var vizElement = divElement.getElementsByTagName('object')[0]; if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='750px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='420px';vizElement.style.maxWidth='750px';vizElement.style.width='100%';vizElement.style.minHeight='587px';vizElement.style.maxHeight='887px';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='1077px';} var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/26/project_md/Tableau/T046_%EB%8B%B9%EC%9B%94_%EC%A0%84%EC%9B%94_%EB%A7%A4%EC%B6%9C%EB%B9%84%EA%B5%90_%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C/"},{"title":"T047_문자열을_날짜형식으로","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1630044176327'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/27/project_md/Tableau/T047_%EB%AC%B8%EC%9E%90%EC%97%B4%EC%9D%84_%EB%82%A0%EC%A7%9C%ED%98%95%EC%8B%9D%EC%9C%BC%EB%A1%9C/"},{"title":"T048_LAST함수활용_마지막날짜기준_성장율","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1630044203320'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/27/project_md/Tableau/T048_LAST%ED%95%A8%EC%88%98%ED%99%9C%EC%9A%A9_%EB%A7%88%EC%A7%80%EB%A7%89%EB%82%A0%EC%A7%9C%EA%B8%B0%EC%A4%80_%EC%84%B1%EC%9E%A5%EC%9C%A8/"},{"title":"T049_데이터연결_Union_이익과할인_상관관계","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1630044225927'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/27/project_md/Tableau/T049_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%B0%EA%B2%B0_Union_%EC%9D%B4%EC%9D%B5%EA%B3%BC%ED%95%A0%EC%9D%B8_%EC%83%81%EA%B4%80%EA%B4%80%EA%B3%84/"},{"title":"T050_LAST함수_최근N개월매출","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1630044255254'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/27/project_md/Tableau/T050_LAST%ED%95%A8%EC%88%98_%EC%B5%9C%EA%B7%BCN%EA%B0%9C%EC%9B%94%EB%A7%A4%EC%B6%9C/"},{"title":"T051_분산형차트_국가별연간흐름","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1630044280265'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/27/project_md/Tableau/T051_%EB%B6%84%EC%82%B0%ED%98%95%EC%B0%A8%ED%8A%B8_%EA%B5%AD%EA%B0%80%EB%B3%84%EC%97%B0%EA%B0%84%ED%9D%90%EB%A6%84/"},{"title":"T052_한국vs해외_남녀평균수명비교","text":"클릭 -&gt; 겔러리 링크 var divElement = document.getElementById('viz1630552962130'); var vizElement = divElement.getElementsByTagName('object')[0]; vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px'; var scriptElement = document.createElement('script'); scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js'; vizElement.parentNode.insertBefore(scriptElement, vizElement);","link":"/2021/08/31/project_md/Tableau/T052_%ED%95%9C%EA%B5%ADvs%ED%95%B4%EC%99%B8_%EB%82%A8%EB%85%80%ED%8F%89%EA%B7%A0%EC%88%98%EB%AA%85%EB%B9%84%EA%B5%90/"}],"tags":[{"name":"Tableau","slug":"Tableau","link":"/tags/Tableau/"},{"name":"info","slug":"info","link":"/tags/info/"},{"name":"md","slug":"md","link":"/tags/md/"},{"name":"titanic","slug":"titanic","link":"/tags/titanic/"},{"name":"class","slug":"class","link":"/tags/class/"},{"name":"inheritance","slug":"inheritance","link":"/tags/inheritance/"},{"name":"A","slug":"A","link":"/tags/A/"},{"name":"Module","slug":"Module","link":"/tags/Module/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","link":"/tags/BeautifulSoup/"},{"name":"크롤링","slug":"크롤링","link":"/tags/%ED%81%AC%EB%A1%A4%EB%A7%81/"},{"name":"re","slug":"re","link":"/tags/re/"},{"name":"exception","slug":"exception","link":"/tags/exception/"},{"name":"예외처리","slug":"예외처리","link":"/tags/%EC%98%88%EC%99%B8%EC%B2%98%EB%A6%AC/"},{"name":"selenium","slug":"selenium","link":"/tags/selenium/"},{"name":"translator","slug":"translator","link":"/tags/translator/"},{"name":"파일생성","slug":"파일생성","link":"/tags/%ED%8C%8C%EC%9D%BC%EC%83%9D%EC%84%B1/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"pycaret","slug":"pycaret","link":"/tags/pycaret/"},{"name":"error","slug":"error","link":"/tags/error/"},{"name":"E","slug":"E","link":"/tags/E/"},{"name":"time","slug":"time","link":"/tags/time/"},{"name":"webdriver","slug":"webdriver","link":"/tags/webdriver/"},{"name":"Numpy","slug":"Numpy","link":"/tags/Numpy/"},{"name":"B","slug":"B","link":"/tags/B/"},{"name":"배열생성","slug":"배열생성","link":"/tags/%EB%B0%B0%EC%97%B4%EC%83%9D%EC%84%B1/"},{"name":"배열인쇄","slug":"배열인쇄","link":"/tags/%EB%B0%B0%EC%97%B4%EC%9D%B8%EC%87%84/"},{"name":"kaggle","slug":"kaggle","link":"/tags/kaggle/"},{"name":"Keras","slug":"Keras","link":"/tags/Keras/"},{"name":"LightGBM","slug":"LightGBM","link":"/tags/LightGBM/"},{"name":"CatBoost","slug":"CatBoost","link":"/tags/CatBoost/"},{"name":"XGBoost","slug":"XGBoost","link":"/tags/XGBoost/"},{"name":"chart_studio","slug":"chart-studio","link":"/tags/chart-studio/"},{"name":"기능","slug":"기능","link":"/tags/%EA%B8%B0%EB%8A%A5/"},{"name":"인덱싱","slug":"인덱싱","link":"/tags/%EC%9D%B8%EB%8D%B1%EC%8B%B1/"},{"name":"슬라이싱","slug":"슬라이싱","link":"/tags/%EC%8A%AC%EB%9D%BC%EC%9D%B4%EC%8B%B1/"},{"name":"for","slug":"for","link":"/tags/for/"},{"name":"범용함수","slug":"범용함수","link":"/tags/%EB%B2%94%EC%9A%A9%ED%95%A8%EC%88%98/"},{"name":"PDF","slug":"PDF","link":"/tags/PDF/"},{"name":"오라클","slug":"오라클","link":"/tags/%EC%98%A4%EB%9D%BC%ED%81%B4/"},{"name":"PPT","slug":"PPT","link":"/tags/PPT/"}],"categories":[{"name":"Tableau","slug":"Tableau","link":"/categories/Tableau/"},{"name":"Markdown","slug":"Markdown","link":"/categories/Markdown/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Error","slug":"Error","link":"/categories/Error/"},{"name":"Plotly","slug":"Plotly","link":"/categories/Plotly/"},{"name":"HEXO","slug":"HEXO","link":"/categories/HEXO/"},{"name":"포트폴리오","slug":"포트폴리오","link":"/categories/%ED%8F%AC%ED%8A%B8%ED%8F%B4%EB%A6%AC%EC%98%A4/"}]}